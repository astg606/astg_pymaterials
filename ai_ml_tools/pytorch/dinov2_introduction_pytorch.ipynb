{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee93059e-f5a3-41c6-ade1-403dfadeea34",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Course Series</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Introduction to DINOv2 with PyTorch</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dde6eb-fba1-45ef-822a-19551972c42a",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Objective</font>\n",
    "\n",
    "- Provide an overview of DINOv2\n",
    "   - Need for foundation models capable of generating features that work out of the box on any task.\n",
    "   - How DINOv2, a self-supervized model, works to create vector embeddings for representing a large collection of images. DINOv2 can then be used to transfer the representations to AI models for better performance.\n",
    "- Describe how DINOv2 can be combined with PyTorch to create model:\n",
    "   - Show how to extract vector embeddings with DINOv2.\n",
    "   - Create various models to solve the MNIST handwritten digit classification problem.\n",
    "   - Show to use PyTorch Lightning.\n",
    "\n",
    "### <font color=\"green\">Concepts and key concepts</font>\n",
    "\n",
    "- __Foundation models__: AI models trained on vast, immense datasets and can fulfill a broad range of general tasks. They serve as the base or building blocks for crafting more specialized applications.\n",
    "- __Embeddings__: An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n",
    "- __PyTorch Lightning__: A PyTorch-based high-level Python framework that aims to simplify the training and deployment of models by providing a lightweight and standardized interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea72fa-0bfc-46ab-8924-4d0cd4701694",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Background </font>\n",
    "\n",
    "- Labeling images is one of the most time consuming parts of training a computer vision (CV) model:\n",
    "   - Each object you want to identify needs to be labeled precisely.\n",
    "   - It is computational intensive not only to gather and precisely label data when we deal with large datasets.\n",
    "   - This is not only a limitation for subject matter-specific models, but also large general models that strive for high performance across a wider range of classes.\n",
    "- The field of Natural Language Processing (NLP) has had rich featurization available in the form of vector embeddings.\n",
    "   - Vector embeddings are numerical representations (keeping the meaning of the original data) of data points that express different types of data, including non-mathematical data such as words or images, as an array of numbers that ML models can process.\n",
    "   - __They can be used as inputs to models that perform useful real-world tasks through mathematical operations that compare, transform, combine, sort or otherwise manipulate those numerical representations.__\n",
    "   - Expressing data points as vectors enables the interoperability of different types of data, acting as a _lingua franca_ of sorts between different data formats by representing them in the same embedding space.\n",
    "   - These foundational embeddings paved the way for a large range of applications.\n",
    "      - Vector embeddings underpin nearly all modern machine learning, powering models used in the fields of NLP and CV, and serving as the fundamental building blocks of generative AI.\n",
    "- We need CV foundation models capable of generating visual features that work out of the box on any task, both at the image level, e.g., image classification, and pixel level, e.g., segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94149b-3311-4c22-b469-523aeb01b167",
   "metadata": {},
   "source": [
    "# <font color=\"red\">What is DINOv2? </font>\n",
    "\n",
    "DINOv2 (self-__DIstillation of knowledge with NO labels v2__) is:\n",
    "\n",
    "- A cutting-edge self-supervised vision transformer developed by [Meta AI](https://arxiv.org/abs/2304.07193?ref=blog.roboflow.com). \n",
    "- A model that is trained to learn from the data itself, without the need for human-labeled annotations. It generates its own supervisory signals from the input data, making it a form of unsupervised learning.\n",
    "   - The model learns to understand the underlying structure and relationships within the data, effectively learning useful representations.\n",
    "   - Traditional deep learning models for CV often rely on massive amounts of labeled data, which can be expensive and time-consuming to acquire. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec8f21-bd77-43ad-b5ef-938947e08c03",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Self-supervised model</font>\n",
    "- DINOv2 is a self-supervised vision transformer model that consists of a __family of foundation models__ producing universal features suitable for image-level visual tasks (image classification, instance retrieval, video understanding) as well as pixel-level visual tasks (depth estimation, semantic segmentation).\n",
    "- It is an advanced self-supervised learning technique to train models, enhancing computer vision by accurately identifying individual objects within images and video frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9055d-7904-481d-a226-1ebc6d72a89d",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Self-distillation framework</font>\n",
    "\n",
    "- DINOv2 uses self-supervized learning (SSL) and knowledge (or model) distillation methods.\n",
    "   - SSL is a “self-supervision” technique that involves a two-step process of pretraining and fine-tuning, where models learn representations from unlabeled data through auxiliary tasks and adapt to specific tasks using smaller amounts of labeled data. \n",
    "   - Knowledge distillation is the process of training a smaller model to mimic the larger model. In this case, you transfer the knowledge from the larger model (often called the “teacher”) to the smaller model (often called the “student”).\n",
    "      - __Step 1__: Train the teacher model with labeled data; it produces an output, so you map the input and output from the teacher model and use the smaller model to copy the output, while being more efficient in terms of model size and computational requirements.\n",
    "      - __Step 2__: Use a large dataset of unlabeled data to train the student models to perform as well as or better than the teacher models. The idea here is to train the large models with your techniques and distill a set of smaller models. This technique is very good for saving computing costs, and DINOv2 is built with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39802c-057f-40b1-b992-3035030c2a80",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Power of data</font>\n",
    "- DINOv2 is trained on a colossal dataset comprising over 142 million images.\n",
    "- The dataset encompasses a wide variety of scenes, objects, and viewpoints, crucial for learning representations applicable across different tasks.\n",
    "- This massive scale training enables the model to learn richer, more generalizable visual representations that capture the intricate nuances of the visual world.\n",
    "- Training with massive batches allows the model to learn from a more diverse set of examples simultaneously, leading to better generalization and faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49ebff-7116-4e8f-b5b1-c55dac67944f",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Benefits</font>\n",
    "\n",
    "- Being self-supervised, DINOv2 eliminates the need for labeled input data, allowing models built on this framework to acquire more comprehensive insights into image content.\n",
    "- DINOv2 utilizes a self-supervised learning technique, enabling the model to be trained on unlabeled images, yielding two significant advantages:\n",
    "   - The approach eliminates the need for substantial time and resource investment in labeling data.\n",
    "   - The model gains more profound and meaningful representations of the image input since it is directly trained on the images themselves.\n",
    "- Pre-training models with self-supervised learning and then fine-tuning them on specific downstream tasks has become a successful approach for transfer learning, enabling models to perform well even with limited labeled data for the target task.\n",
    "   - DINOv2 can learn adaptable, high-quality, all-purpose visual features, enabling it to perform various computer vision tasks, such as classification, estimating depth, semantic segmentation, instance retrieval, and more, without fine-tuning specific tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad95b4-88c6-44a8-a732-20b3eba239c8",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Python packages used</font>\n",
    "\n",
    "- __Matplotlib__: Create visualization.\n",
    "- __Pandas__: Data (two-dimensional labelled array) manipulation and analysis.\n",
    "- __PyTorch__: Used to to build, train, and evaluate a deep machine learning algorithm based on Neural Networks.\n",
    "- __PyTorch Lightning__: A wrapper framework for PyTorch that makes it easy to develop and train deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243b338-220e-4f9a-82ea-2a73fc3d16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab\")\n",
    "else:\n",
    "    print(\"Installing modules in Google Colab\")\n",
    "    !pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
    "    !pip3 install torch torchaudio torchvision torchtext torchdata\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf69514-b2ee-4ddb-b2fb-8a73bc92739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a473f-5cde-41e6-8e94-a40d5ac67298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b6ef1-3d00-48fe-8188-2e314e6424cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccc5f-d964-4a47-8857-67e1ba4dd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c599f2-535e-4281-a5c4-5e113c608865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6506e1-6ecb-43ac-a197-9d1c06fe5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import torchvision.transforms as T\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms \n",
    "#from transformers import AutoImageProcessor, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b311f1c-5ba6-4b81-877d-5fa81b4e52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lightning version: \", pl.__version__)\n",
    "print(\"Torch version:     \", torch.__version__)\n",
    "print(\"CUDA is available: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1a9b4-b459-4fd3-b7f3-f4d6ebb20a75",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Obtaining embeddings with DINOv2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15cf4a-36e4-43dd-a831-f1439cb960a9",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Choose your device</font>\n",
    "\n",
    "Use CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c96753-645b-4e60-a2c7-d3f7f60a4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec2e93-73c5-4bdd-ab29-0511e307ea29",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Load the DINOv2 model</font>\n",
    "- DINOv2 is a family of self-supervised Vision Transformer (ViT) models.\n",
    "- The models vary in size and complexity, and include `ViT-S/14` and `ViT-L/14`:\n",
    "   - `ViT-S/14` (Small, `dinov2_vits14`): This is a relatively smaller model in the DINOv2 family. It offers a good balance between performance and computational efficiency, making it suitable for applications with limited resources. It's a good choice when you need solid performance without the extensive computational cost of larger models.\n",
    "   - `ViT-L/14` (Large, `dinov2_vitl14`): This is a larger model with potentially more Transformer layers and attention heads, which can lead to improved performance, especially on complex tasks.\n",
    "      - It's the second largest model in the DINOv2 series, with `ViT-G/14` being the largest.\n",
    "      - `ViT-L/14` excels in applications where accuracy is paramount, even with increased computational cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c772cc4-63b9-4c2e-9599-0c62ce261a8c",
   "metadata": {},
   "source": [
    "Here we use the small ViT: `dinov2_vits14`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27558ed6-5bd0-4036-9dd8-11d2a6685c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_name_small = \"dinov2_vits14\"\n",
    "vit_name_large = \"dinov2_vitl14\"\n",
    "vit_name_grande = \"dinov2_vitg14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cdb4e-ffbf-4e31-ae6a-aaebc7de842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", vit_name_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b8289-0f79-44a5-aca8-a3fe115835fa",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Bring up the model to the device</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdc54f-e7c9-4fc3-a7c6-02eab2a6382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275d74e-cc22-40af-85f5-07074c4c1309",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Set the model to evaluation mode</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e366e5b-1510-4f3e-8a67-d20f06cd98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4c003-de1c-4c79-8211-1627aba69dc6",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Get the image of interest</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca4d56-2fc2-4999-9b76-70ded2e27008",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275dacb-0394-4971-8e05-c45789a5e8d3",
   "metadata": {},
   "source": [
    "#### Access image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5e06a-8c36-47e1-94fc-f71a5adefbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff97cf4-1f4c-46d6-8e0c-88c3ac15588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceba7d2-bb5b-47b6-90f6-1ea736205c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0197c7-1bcc-4c55-8cfe-e3b32bbe6d01",
   "metadata": {},
   "source": [
    "#### Display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ffdca-a1a7-4816-9775-66f634af4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d3047-8028-4513-9c36-1fd5d3131761",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Create a image preprocessor and apply over input image</font>\n",
    "\n",
    "- Data transformation is an essential preprocessing step that prepares raw data for models.\n",
    "- Transformations like resizing, converting images to tensors, or normalizing pixel values are common for image data. These transformations help the model to see data in a consistent, well-scaled format.\n",
    "- We use the `torchvision.transforms` module to perform a series of manipulations on the image:\n",
    "   - `Resize()`: Resize the input to the given size expected by DINOv2.\n",
    "   - `ToTensor()`: Convert the image to a tensor. In PyTorch, models operate on tensors, so images (or any data) need to be converted into tensors before they can be fed into a model. \n",
    "      - A Tensor Image is  a tensor with (`C`, `H`, `W`) shape, where `C` is a number of channels, `H` and `W` are image height and width. \n",
    "   - `Normalize()`: Adjust the pixel values of an image so that they fall within a specific range. It standardizes the data, making it easier for the model to learn patterns. Here, we normalize the tensor image with mean (mean values for the three channels) and standard deviation (std values for the three channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1fc78-fe5a-4bad-af8c-5a81be544867",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),       \n",
    "    transforms.ToTensor(),              \n",
    "    transforms.Normalize(                \n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d64839-7516-429e-ba8a-1b2a80ac2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = transform(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4cbc9-dfb6-4646-a6ff-845096eb8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b048c64-0fe4-432e-9c3f-e03d42fd3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e9b3d-e933-4b58-b335-1e8fd553faf4",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Feed the image to the model to extract embeddings</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75e214-d3a0-43f4-91ba-3ade141fa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    embeddings = dinov2_vits14(input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2503fca-82ea-4eae-a4cc-bf5b8fd72f1b",
   "metadata": {},
   "source": [
    "Shape of the `embeddings`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abd99a-8884-4b4a-b38c-ca38a4fdff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d662d-5025-422d-accc-7406841876d3",
   "metadata": {},
   "source": [
    "Print the first few values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0babb03-35b7-4abe-831d-446a63235b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42161e89-b4fb-484c-a64d-9d3778ebed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8ba58-18bb-4a0a-932b-cc98fe4f6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b403d7-9cb2-4275-90bb-775dcc20b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adb41e-0d27-4c88-b7e0-75c2942f5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\")\n",
    "print(f\"Min value: {np_embeddings.min()}\")\n",
    "print(f\"Max value: {np_embeddings.max()}\")\n",
    "print(f\"STD value: {np_embeddings.std()}\")\n",
    "print(f\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9e902-e1df-4433-b541-6344803bca90",
   "metadata": {},
   "source": [
    "Plot the embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea18665-10b4-4e1b-b0b8-206438af144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np_embeddings.reshape(1, -1), aspect='auto')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title('Embedded Image')\n",
    "plt.xlabel('Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3498d6-5943-4167-92ce-dd5611e4f619",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Obtain and pre-process MNIST handwritten digit dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5963e208-7ef4-4435-a083-2198dfce3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2a1aa-88e3-4880-beb8-6a980d633167",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Load the data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035ba8b-3231-4ad8-ad7b-84361c20fa4d",
   "metadata": {},
   "source": [
    "#### Create custom PyTorch dataset\n",
    "\n",
    "- The `Dataset` class is the primary tool for handling data.\n",
    "- It acts as an interface that allows users to define how their data is accessed from files, APIs, or even generated from scratch.\n",
    "- It helps prepare data for training by abstracting the complexities of data loading.\n",
    "- The three primary methods users need to implement when creating a custom dataset are:\n",
    "   - `__init__()`: Load the data into memory.\n",
    "   - `__len__ ()`: Define the total number of samples in our dataset.\n",
    "   - `__getitem__()`: Retrieve a specific data sample by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5178fdb-d34d-41a2-b810-bda670378a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, sample_dataset, transform=None):\n",
    "        self.data = list()\n",
    "        self.labels = list()\n",
    "        for idx in range(len(sample_dataset)):   \n",
    "            self.data.append(np.array(sample_dataset[idx][0]))\n",
    "            self.labels.append(sample_dataset[idx][1])\n",
    "        self.transform = transform\n",
    "        print(f\"Dataset contains {len(sample_dataset)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Provide the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        # Select sample associated with the provided index\n",
    "        image = self.data[idx].reshape(28, 28).astype('uint8')\n",
    "        \n",
    "        # Convert to 3-channel RGB\n",
    "        image = Image.fromarray(image).convert(\"RGB\")  \n",
    "\n",
    "        # If necessary perform the data transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3902b7f-4912-4fb7-882c-3df2bd2ecbfb",
   "metadata": {},
   "source": [
    "#### Define transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b55f0-e3a7-49f4-bdf7-9af85417f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc534805-cfd7-4e71-b0d8-6a80d346fab2",
   "metadata": {},
   "source": [
    "#### Get the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c5418-2c5e-43ac-93d8-4676b50f7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_dataset = datasets.MNIST(root='data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2cc331-7cde-4283-b24c-fa2a1b836ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_dataset = datasets.MNIST(root='data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b7b4c-c215-45ad-ab02-c67eec75ffd9",
   "metadata": {},
   "source": [
    "#### Make the dataset ready for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe39b2-7a31-4dbb-9b9c-d26640d9ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DigitDataset(train_raw_dataset, transform=transform)\n",
    "test_dataset = DigitDataset(test_raw_dataset, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378a07e-07e7-423f-9eda-09bc185e1b70",
   "metadata": {},
   "source": [
    "#### Extract a subset of the dataset\n",
    "\n",
    "We do it to reduce the computational requirement. You may choose to skip this step if you want to use the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6b5cd-19e1-4694-82e1-77932dae043f",
   "metadata": {},
   "source": [
    "Set seeds for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f312e4e-41e3-4568-90c1-711d17087b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4fefd-2aca-45fb-beb6-164b5a378fc4",
   "metadata": {},
   "source": [
    "Create random indices for sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2bea7-7c14-494d-b0de-e42b644d83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain_data = 12800\n",
    "nval_data = 2000\n",
    "ntest_data = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58587e2c-787c-4ff8-8801-d02a27f5c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(train_dataset), size=ntrain_data, replace=False)\n",
    "test_indices = np.random.choice(len(test_dataset), size=ntest_data, replace=False)\n",
    "new_list = list(set(range(len(test_dataset))) - set(test_indices))\n",
    "val_indices = np.random.choice(new_list, size=nval_data, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20003dc-6c1e-4c1d-9776-b6f0dae90d70",
   "metadata": {},
   "source": [
    "Create subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b585e1-90e2-49ee-b4af-9ee280f7c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(test_dataset, val_indices)\n",
    "test_dataset = Subset(test_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9545d86-45c3-49f2-9ed4-918df97c2d34",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Test Case 1: Use a linear model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80c637-5ec4-4109-a4dc-bd7389d482f5",
   "metadata": {},
   "source": [
    "### Basic steps\n",
    "\n",
    "- Load a pre-trained DINOv2 model using PyTorch Hub.\n",
    "- Define a function to load an MNIST image and transform it into a format accepted by DINOv2.\n",
    "- Compute DINOv2 embeddings for each MNIST image in your dataset.\n",
    "- Train a classifier on the generated embeddings and their corresponding labels.\n",
    "   - We can use a lightweight classifier, such as a Linear Support Vector Classification (SVC) model, on these embeddings.\n",
    "   - The trained classifier will then be able to accurately classify the MNIST digits based on the visual features extracted by DINOv2.\n",
    "- Use the trained classifier to predict the digit for new MNIST images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043c21d-069f-4f9b-8a80-b8d244d5fd1b",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Define the model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91bfc4a-7ffc-48f8-b850-68d0afa2ccd2",
   "metadata": {},
   "source": [
    "#### Head model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31651ce-ef05-4841-8662-e3ea7ebad828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifierHead(nn.Module):\n",
    "    def __init__(self, embed_dim, num_hidden_nodes, num_classes):\n",
    "        super().__init__()\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb4244-449b-434c-9e9b-3bbd8e468cb5",
   "metadata": {},
   "source": [
    "#### Model that combines DINOv2 and the head model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ae991-92e8-4db9-a8bf-6e024d00b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, dinov2_model, num_hidden_nodes, num_classes):\n",
    "        super().__init__()\n",
    "        self.dinov2_vits14 = dinov2_model\n",
    "        try:\n",
    "            self.embed_dim = dinov2_model.embed_dim\n",
    "        except:\n",
    "            self.embed_dim = dinov2_model.config.hidden_size\n",
    "        # Replace the original head or add a new one\n",
    "        # DINOv2 typically outputs a feature vector, so a linear layer is suitable\n",
    "        self.image_classifier = LinearClassifierHead(self.embed_dim, num_hidden_nodes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.dinov2_vits14(x)\n",
    "        \n",
    "        logits = self.image_classifier(features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae6cd6-a237-4fc2-ab75-82c3aa0c0fc9",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Create and train the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3481771-1215-4c88-848c-05ed92da97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e5ff2-19ec-4017-8854-b8ac4955dd0f",
   "metadata": {},
   "source": [
    "## <font color=\"green\"> Set hyperparameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ff4fd-cfac-402b-b3fc-dfae5d13bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21c414-ccf1-45a4-bf43-cc7e5d884d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93942a36-0b7f-4e97-b395-1355d97a0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5562bcc-d5aa-4736-b152-ff5dbde65008",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_nodes=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c468a61-e4f3-4015-be8c-d9977e51366f",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Load a pre-trained DINOv2 model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbe9c2-8be2-4b17-aabe-ba302724f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_model = torch.hub.load('facebookresearch/dinov2', vit_name_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c492684-139e-482f-9e13-cfd497144015",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    embed_dim = dinov2_model.embed_dim\n",
    "except:\n",
    "    embed_dim = dinov2_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d7728-4d2e-4037-a657-f9fe2fc5cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DINOv2 embed_dim: \\n\\t {embed_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddb04a-4a7e-474b-a774-a5d730e3ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bce284-90dd-48e1-a836-3540278d0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b054f8e-4d43-420a-98e1-9ec27a517bea",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Freeze DINOv2 backbone parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f8d91-7ca6-4d21-bc67-4bbc9f91f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in dinov2_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c02832-f27b-401f-a09f-b90d9c1b5177",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Create model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0f96f-4b30-4ef1-a2ba-9e7bb1b2e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = CustomModel(\n",
    "    dinov2_model=dinov2_model, \n",
    "    num_hidden_nodes = num_hidden_nodes, \n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eec958-331d-42e2-9d0d-24e8927f00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1927b-0874-4b61-bc7d-beb337475280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t Model information: \\n')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03bbe1-ac59-4b80-87fb-0441fccda403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters_per_layer(model):\n",
    "    n = 20\n",
    "    m = 10\n",
    "    p = n+m+2\n",
    "    print(f\"{'-'*p}\")\n",
    "    print(f\"{'Modules':<{n}}  {'Parameters':{m}}\")\n",
    "    print(f\"{'-'*p}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:<{n}}  {param.numel():{m}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d6f3d-ba93-4d39-9bc8-3c507e44c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters_per_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef749e3e-8919-4d0f-9595-58b0b3828174",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.image_classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142849e-485d-4da9-b297-cfb1f36d0d41",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Define the loss function and optimzer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447697e2-f9a6-4806-8f6f-435d34559798",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8b4a2-846d-44e0-9d0b-701d987d43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a7444-12a6-4587-948e-2ea7f4d710e0",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Define DataLoader</font>\n",
    "\n",
    "- We pass the dataset to our dataloader, and our `batch_size` hyperparameter as initialization arguments.\n",
    "- This creates an iterable data loader, so we can easily iterate over each batch using a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43ad80-f3d1-4738-9088-918327c571b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01245699-d875-4989-8bee-170d6b5759ee",
   "metadata": {},
   "source": [
    "__Let us check some examples (by using `test_loader`):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be737df0-227f-4426-90d8-add0848f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723b5ad-33a4-455f-b0ae-dcc66c0c8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b0b3b-fbb7-4142-9213-20a91f4185cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffbf39-8f75-4071-b630-062759899a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(feature_data, label_data, ref_title=\"Ground Truth\"):\n",
    "    fig = plt.figure()\n",
    "    for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.imshow(feature_data[i][0], cmap='gray', interpolation='none')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"{ref_title}: {label_data[i]}\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7073e6f-f86f-40cd-b253-5edf84db4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_images(example_data, example_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be528492-2766-412d-9221-64ceee45e1a9",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Define functions to train per batch and per epoch </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52730f1e-df4b-4baf-bc93-3dddc8c63f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_per_batch(data, target, mymodel, \n",
    "                          myloss_function, myoptimizer) -> float:\n",
    "    #data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Zero the gradients\n",
    "    myoptimizer.zero_grad()\n",
    "\n",
    "    # Perform forward pass\n",
    "    feature = mymodel(data)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = myloss_function(feature, target)\n",
    "\n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform optimization\n",
    "    myoptimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd85926-aeb8-497e-8f52-4d1de2eb917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_per_epoch(epoch_idx, mymodel, myloss_function, myoptimizer, \n",
    "                          dataloader_train, train_losses, train_counter):\n",
    "    # Put model in training model\n",
    "    mymodel.train()\n",
    "    n_dataloader = len(dataloader_train.dataset)\n",
    "    n_data_per_batch = len(dataloader_train)\n",
    "    print(f\"Inside train_model_per_epoch - Number of item: {n_dataloader}\")\n",
    "    for batch_idx, (data, target) in enumerate(dataloader_train):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        n_data = len(data)\n",
    "        loss_val = train_model_per_batch(data, target, mymodel, loss_function, optimizer)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch_idx} [{batch_idx*n_data}/{n_dataloader}' \n",
    "                  f'({100.*batch_idx/n_data_per_batch:.0f}%)]\\tLoss: {loss_val:.6f}')\n",
    "            train_losses.append(loss_val)\n",
    "            train_counter.append((batch_idx*64) + ((epoch_idx-1)*n_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7da63-017a-486e-b48b-5628c250dd1b",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Define function to evaluate the model accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e777d1d-abe2-472a-8e9b-0a966798f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader, test_loss):\n",
    "    \"\"\"\n",
    "    Compute the percentage of correct classification.\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    n_items = len(dataloader.dataset)\n",
    "    print(f\"Inside compute_accuracy - Number of item: {n_items}\")\n",
    "    correct = 0.0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(logits, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            correct += (pred == target).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_losses.append(test_loss)    \n",
    "    perc = 100. * correct / n_items\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{n_items} ({perc:.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679141df-d9ef-4fe9-a9c1-f8329cd695d6",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Train and evaluate the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262bf2c-1aef-4761-8feb-32d732066a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = list()\n",
    "train_counter = list()\n",
    "test_losses = list()\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(max_epochs+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa94a5-1fcb-46b9-859d-9973dd60be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "compute_accuracy(model, test_loader, test_losses)\n",
    "for epoch_idx in range(1, max_epochs+1):\n",
    "    train_model_per_epoch(epoch_idx, model, loss_function, optimizer,\n",
    "                         train_loader, train_losses, train_counter)\n",
    "    compute_accuracy(model, test_loader, test_losses)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7225d9-41a7-490c-a4c5-38b324dfeeca",
   "metadata": {},
   "source": [
    "__Plot the losses__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb3482-0da8-42a5-bba8-842607bf9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_counter, train_losses, test_counter, test_losses):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_counter, train_losses, color='blue')\n",
    "    plt.scatter(test_counter, test_losses, color='red')\n",
    "    plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "    plt.xlabel('Number of training examples seen')\n",
    "    plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b19bc-6eaf-4288-a36f-6fb5ec2cee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_counter, train_losses, test_counter, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbeb7d8-fb22-4f05-9008-d2787b1d7701",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Test Case 2: PyTorch Lightning and a multi-layer NN model</font>\n",
    "\n",
    "- In the previous example, we combined DINOv2 with a simple linear model.\n",
    "- Here we use a multi-layer sequatial model.\n",
    "- We also use PyTorch Lightning that is designed to automate and simplify the training and deployment of deep learning models.\n",
    "   - It eliminates boilerplate code for training loops and complex setups, which is cumbersome for many developers, and allows users to focus on the core model and experiment logic.\n",
    "   - It automate the training loop: abstracts the codes related to to epoch and batch iteration, `optimizer.step()`, `loss.backward()`, `optimizer.zero_grad()`, and setting the model to `eval()` or `train()` mode.\n",
    "   - It simplifies complex setups like multi-GPU and distributed training (e.g., DDP) with minimal code changes, making it easy to scale training from a single device to multiple GPUs or TPUs.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b629a09-4c42-41ac-83f9-24399d8d1fbf",
   "metadata": {},
   "source": [
    "__Custom classifier head__\n",
    "\n",
    "- This classifier is created for illustration only.\n",
    "- We create a sequential network consisting of:\n",
    "   - A fully-connected (Linear) layer with `num_hidden_nodes` nodes, followed by the `Tanh` activation function.\n",
    "   - A Dropout layer with a `20%` dropout rate to prevent overfitting.\n",
    "   - A second Linear layer, with `num_hidden_nodes` nodes, followed by the `Sigmoid` activation function.\n",
    "   - Another Dropout layer, that removes `20%` of the nodes.\n",
    "   - A final Linear layer, with `num_classes` nodes (matching the number of classes in the dataset), followed by a Softmax activation function that outputs class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda15852-1f4f-460b-a3c5-7e280dace251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_nodes, num_classes):\n",
    "        super(ImageClassifierNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, num_hidden_nodes),\n",
    "            #nn.ReLU(),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(.2),\n",
    "            \n",
    "            nn.Linear(num_hidden_nodes, num_hidden_nodes),\n",
    "            #nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(.2),\n",
    "            \n",
    "            nn.Linear(num_hidden_nodes, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        output = self.net(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8b0e4-6712-4563-9611-71e422c3462c",
   "metadata": {},
   "source": [
    "__Lightning model with DINOv2 and custom head__\n",
    "\n",
    "When using `LightningModule`, the PyTorch code isn't abstracted; it’s organized into six sections:\n",
    "\n",
    "- Initialization (`__init__` and `setup()` methods).\n",
    "- Train loop (`training_step()` method).\n",
    "- Validation loop (`validation_step()` method).\n",
    "- Test loop (`test_step()` method).\n",
    "- Prediction loop (`prediction_step()` method).\n",
    "- Optimizers and LR schedulers (`configure_optimizers()`).\n",
    "\n",
    "Each the above methods needs to be included inside the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e21a0-7e35-4c51-9b7f-0b18c597c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLightningModule(pl.LightningModule):\n",
    "    def __init__(self, dinov2_model, num_hidden_nodes, num_classes, learning_rate):\n",
    "        super().__init__()\n",
    "        self.dinov2_model = dinov2_model\n",
    "        try:\n",
    "            self.embed_dim = dinov2_model.embed_dim\n",
    "        except:\n",
    "            self.embed_dim = dinov2_model.config.hidden_size\n",
    "        print(f\"embed_dim = {self.embed_dim}\")\n",
    "        self.image_classifier = ImageClassifierNetwork(self.embed_dim, num_hidden_nodes, \n",
    "                                                       num_classes)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.dinov2_model(x)\n",
    "        \n",
    "        logits = self.image_classifier(features)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)     \n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log(\"train_acc\", acc)\n",
    "        # Log the loss at each training step and epoch, create a progress bar\n",
    "        self.log(\"train_loss\", loss, \n",
    "                 on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        #self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        #preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        #acc = (preds == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.image_classifier.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c97caa-80ff-440d-add4-308c6767a99c",
   "metadata": {},
   "source": [
    "__Create the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1b576-5d53-48a5-b561-ae739be83f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = MyLightningModule(\n",
    "    dinov2_model=dinov2_model, \n",
    "    num_hidden_nodes=num_hidden_nodes,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f13199-75f4-4680-b2b6-19cf46462df4",
   "metadata": {},
   "source": [
    "__Set up the `Trainer`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3cea4-03ae-4f84-a560-5ca272f72547",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_device = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9d2a5-f3d6-446b-bb01-8726d4ebb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=which_device, \n",
    "    devices=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23efad-cc1d-4531-9508-d7993d83d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3827b4-edce-4f04-b769-9b14b87bf030",
   "metadata": {},
   "source": [
    "__Train and validate the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172d0f4-ace9-4c2e-a927-66846f9d003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(new_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377177c2-4c7f-47b5-9dcc-be8f483821b0",
   "metadata": {},
   "source": [
    "__Test the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf679468-5ae5-4e19-bc61-fe3864939d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.test(new_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08213446-2b1f-4208-abcd-89210ad45f64",
   "metadata": {},
   "source": [
    "- We write here the `predict` function to determine the prediction.\n",
    "- This is redundant and is meant for verification only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055dc62-3f46-4eb4-a167-39d065591222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mymodel, dataloader, device):\n",
    "    mymodel = mymodel.to(device)\n",
    "    mymodel.eval()\n",
    "    all_preds = list()\n",
    "    all_actuals = list()\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = mymodel(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # Move predictions back to CPU and convert to numpy\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_actuals.extend(target.cpu().numpy())  \n",
    "    return all_actuals, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501c6a3-be49-43df-b75e-42b6f39226d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate predictions\n",
    "actuals, predictions = predict(new_model, test_loader, device)\n",
    "\n",
    "# Create submission file\n",
    "df = pd.DataFrame(\n",
    "    {'ImageId': range(1, len(predictions) + 1), \n",
    "     'Actuals': actuals, \n",
    "     'Predictions': predictions}\n",
    ")\n",
    "df.to_csv('test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb25789-456f-4ddc-97df-9d1e7f262d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = len(df[df.Actuals == df.Predictions])/len(df)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac0d7a-6c5c-4aa1-8544-5ef0b58f2409",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Test Case 3: Use DINOv2 embeddings as inputs of a NN</font>\n",
    "\n",
    "- In the previous examples, the first `layer` of the created models was the DINOv2 model.\n",
    "- The DINOv2 model is used only in evaluation mode, which primary role is to compute the embeddings.\n",
    "- At each epoch, the embeddings of each image are calculated. This makes the training, validation and testing time consuming.\n",
    "- We want to compute the DINOv2 embeddings first and use them as inputs to any neural network of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d248410-f9da-47f6-a6bb-a5ee91f242cc",
   "metadata": {},
   "source": [
    "\n",
    "__Step 1__: Steps to Generate Embeddings\n",
    "\n",
    "- Load DINOv2: Load a pre-trained DINOv2 model.\n",
    "- Obtain the MNIST Data\n",
    "- Apply necessary transformations to the MNIST images, such as resizing and normalization, to match the input requirements of the DINOv2 model.\n",
    "- Extract Embeddings: Feed the preprocessed MNIST images into the DINOv2 model. The output of the model, specifically the class token embedding or a global average pooled representation of the patch embeddings, will be the feature vector (embedding) for each MNIST image.\n",
    "\n",
    "__Step 2__: Training the PyTorch Model\n",
    "\n",
    "- Use the DINOv2 embeddings for the MNIST images as input to train a PyTorch classification model.\n",
    "- The model can be: \n",
    "   - A Linear Classifier: A simple linear layer can be sufficient to classify the embeddings.\n",
    "   - A Neural Network: Build a  feedforward neural network on top of the embeddings for potentially better classification performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e62bda-49ec-4fb9-b0ce-0642e5cb143e",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Determine the embeddings</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115e72b-356d-42c6-a20a-bc6c358ea1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_labels(dataset, dino_model, device):\n",
    "    embeddings_obj = list()\n",
    "    labels_obj = list()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i][0].unsqueeze(0).to(device)\n",
    "            \n",
    "            output = dino_model(data)\n",
    "            embeddings_obj.append(output.cpu())\n",
    "            labels_obj.append(dataset[i][1])\n",
    "    embeddings_obj = torch.cat(embeddings_obj, dim=0)\n",
    "    labels_obj = torch.tensor(labels_obj)\n",
    "\n",
    "    return embeddings_obj, labels_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9760b-0b9a-4328-9b9f-b3ec3a30b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    embed_dim = dinov2_model.embed_dim\n",
    "except:\n",
    "    embed_dim = dinov2_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff65cc-4058-4edf-8664-c11dc5ef5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_train, labels_train = create_embeddings_labels(train_dataset, dinov2_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00168a0d-2fba-4e37-98a6-7400bc7b9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_test, labels_test = create_embeddings_labels(test_dataset, dinov2_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cd3f5-8700-4604-bf6a-bd79339fd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_val, labels_val = create_embeddings_labels(val_dataset, dinov2_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800914a-4d3f-4ae3-b0e7-90d57d07c793",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Create the dataloaders</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6e5c2-daf7-495b-b1fc-802ca6ac7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset():\n",
    "    '''\n",
    "    Custom 'Dataset' object for our regression data.\n",
    "    Must implement these functions: __init__, __len__, and __getitem__.\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb73b35-d1fb-41e3-91db-93932f133e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_data(Xdata, ydata, batch_size=64, shuffle=False):\n",
    "    dataset = MyDataset(Xdata, ydata)\n",
    "    dataloader = DataLoader(dataset=dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5520aa4-0693-4b55-9ccd-894a67163a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = instantiate_data(embeddings_train, labels_train, \n",
    "                                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = instantiate_data(embeddings_test, labels_test, \n",
    "                                   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataloader = instantiate_data(embeddings_val, labels_val, \n",
    "                                  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed99ed9-4758-4584-b93a-ef407b994726",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Model creation</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739efc8-34da-4e84-aefc-72ac842bc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericLightningModule(pl.LightningModule):\n",
    "    def __init__(self, nn_model, embed_dim, num_hidden_nodes, num_classes, learning_rate):\n",
    "        super().__init__()\n",
    "        self.image_classifier = nn_model(embed_dim, num_hidden_nodes, num_classes)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.image_classifier(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)     \n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log(\"train_acc\", acc)\n",
    "        self.log(\"train_loss\", loss, \n",
    "                 on_step=True, on_epoch=True, prog_bar=True, logger=True) \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.image_classifier.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde00766-688e-4cae-8d5e-5a5f89cbac37",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Option 1: Linear model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424b3be-dcc0-4af1-a9be-248c0cbad0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = GenericLightningModule(\n",
    "    nn_model=LinearClassifierHead, \n",
    "    embed_dim=embed_dim,\n",
    "    num_hidden_nodes=num_hidden_nodes,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a3923-50d7-45d0-a4e7-592e7e46b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_device = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda50140-cb78-4541-86d0-06a6d31bc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=which_device, \n",
    "    devices=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fab93f-bf55-4b1c-b286-94b1b83fc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = linear_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797384c5-1191-414a-a14c-622e6995540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_trainer.fit(linear_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff920c-7d83-4c31-9201-b2155a61ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_trainer.test(linear_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6f99e-fac9-4f6f-b6ef-cd3efe9c50cd",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Option 2: Multi-layer model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de2179-19fd-4926-ba7d-09c0d84691ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = GenericLightningModule(\n",
    "    nn_model=ImageClassifierNetwork, \n",
    "    embed_dim=embed_dim,\n",
    "    num_hidden_nodes=num_hidden_nodes,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf3e5f-21d1-4f4c-82de-cc927494a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=which_device, \n",
    "    devices=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577733bc-b5c5-4e64-be61-20123590193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = nn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24961241-2e20-48fb-8ce3-ca822588544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nn_trainer.fit(nn_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68bcbdf-aefb-46c1-bf3c-e40fb52938ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nn_trainer.test(nn_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa601f7f-9b5a-44cf-9975-607fb930d213",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> References</font>\n",
    "\n",
    "- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/pdf/2304.07193) by Maxime Oquab et al.\n",
    "- [DINOv2 by Meta: A Self-Supervised foundational vision model](https://learnopencv.com/dinov2-self-supervised-vision-transformer/) by Bhomik Sharma, April 2025.\n",
    "- [01.Meta-DinoV2-Getting Started](https://www.kaggle.com/code/shravankumar147/01-meta-dinov2-getting-started)\n",
    "- [DINOv2](https://huggingface.co/docs/transformers/en/model_doc/dinov2) from hugginface.co\n",
    "- [Building the DINO model from Scratch with PyTorch: Self-Supervised Vision Transformer](https://medium.com/thedeephub/self-supervised-vision-transformer-implementing-the-dino-model-from-scratch-with-pytorch-62203911bcc9) by Shubh Mishra\n",
    "- [How to Classify Images with DINOv2](https://blog.roboflow.com/how-to-classify-images-with-dinov2/) by James Gallagher (May 30, 2023\n",
    "- [Deploying DINOv2 to A Rest API Endpoint for Image Classification | Modelbit](https://colab.research.google.com/github/write-with-neurl/modelbit-09/blob/main/notebook/Deploying_DINOv2_for_Image_Classification_with_Modelbit.ipynb#scrollTo=q06RxQlCzQnG)\n",
    "- [DINOv2: Self-supervised Learning Model Explained](https://encord.com/blog/dinov2-self-supervised-learning-explained/) eNCORD Blog, November 2024.\n",
    "- [How to Classify Images with DINOv2](https://blog.roboflow.com/how-to-classify-images-with-dinov2/) by James Gallagher, May 2023.\n",
    "- [DinoV2 Fine-Tuning Tutorial: How to Maximize Accuracy for Computer Vision Tasks](https://kili-technology.com/data-labeling/computer-vision/dinov2-fine-tuning-tutorial-maximizing-accuracy-for-computer-vision-tasks) by Asmaa Mirkhan\n",
    "- [PyTorch Lightning: A Comprehensive Hands-On Tutorial](https://www.datacamp.com/tutorial/pytorch-lightning-tutorial) by Bex Tuychiev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
