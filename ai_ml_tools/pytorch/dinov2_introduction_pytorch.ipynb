{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee93059e-f5a3-41c6-ade1-403dfadeea34",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Course Series</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Introduction to DINOv2 with PyTorch</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dde6eb-fba1-45ef-822a-19551972c42a",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> References</font>\n",
    "\n",
    "- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/pdf/2304.07193) by Maxime Oquab et al.\n",
    "- [DINOv2 by Meta: A Self-Supervised foundational vision model](https://learnopencv.com/dinov2-self-supervised-vision-transformer/) by Bhomik Sharma, April 2025.\n",
    "- [01.Meta-DinoV2-Getting Started](https://www.kaggle.com/code/shravankumar147/01-meta-dinov2-getting-started)\n",
    "- [DINOv2](https://huggingface.co/docs/transformers/en/model_doc/dinov2) from hugginface.co\n",
    "- [Building the DINO model from Scratch with PyTorch: Self-Supervised Vision Transformer](https://medium.com/thedeephub/self-supervised-vision-transformer-implementing-the-dino-model-from-scratch-with-pytorch-62203911bcc9) by Shubh Mishra\n",
    "- [How to Classify Images with DINOv2](https://blog.roboflow.com/how-to-classify-images-with-dinov2/) by James Gallagher (May 30, 2023\n",
    "- [Deploying DINOv2 to A Rest API Endpoint for Image Classification | Modelbit](https://colab.research.google.com/github/write-with-neurl/modelbit-09/blob/main/notebook/Deploying_DINOv2_for_Image_Classification_with_Modelbit.ipynb#scrollTo=q06RxQlCzQnG)\n",
    "- [DINOv2: Self-supervised Learning Model Explained](https://encord.com/blog/dinov2-self-supervised-learning-explained/) eNCORD Blog, November 2024.\n",
    "- [How to Classify Images with DINOv2](https://blog.roboflow.com/how-to-classify-images-with-dinov2/) by James Gallagher, May 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea72fa-0bfc-46ab-8924-4d0cd4701694",
   "metadata": {},
   "source": [
    "\n",
    "- Need for computer vision foundation models that generate visual features that work out of the box on any task, both at the image level, e.g., image classification, and pixel level, e.g., segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94149b-3311-4c22-b469-523aeb01b167",
   "metadata": {},
   "source": [
    "# <font color=\"red\">What is DINOv2? </font>\n",
    "\n",
    "#### Self-supervised model\n",
    "- DINOv2 (self-__DIstillation of knowledge with NO labels v2__) is a self-supervised vision transformer model that consists of s family of foundation models producing universal features suitable for image-level visual tasks (image classification, instance retrieval, video understanding) as well as pixel-level visual tasks (depth estimation, semantic segmentation).\n",
    "- It is an advanced self-supervised learning technique to train models, enhancing computer vision by accurately identifying individual objects within images and video frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9055d-7904-481d-a226-1ebc6d72a89d",
   "metadata": {},
   "source": [
    "#### Self-distillation framework\n",
    "\n",
    "- DINOv2 uses self-supervized learning (SSL) and knowledge (or model) distillation methods.\n",
    "   - SSL is a “self-supervision” technique that involves a two-step process of pretraining and fine-tuning, where models learn representations from unlabeled data through auxiliary tasks and adapt to specific tasks using smaller amounts of labeled data. \n",
    "   - Knowledge distillation is the process of training a smaller model to mimic the larger model. In this case, you transfer the knowledge from the larger model (often called the “teacher”) to the smaller model (often called the “student”).\n",
    "      - __Step 1__: Train the teacher model with labeled data; it produces an output, so you map the input and output from the teacher model and use the smaller model to copy the output, while being more efficient in terms of model size and computational requirements.\n",
    "      - __Step 2__: Use a large dataset of unlabeled data to train the student models to perform as well as or better than the teacher models. The idea here is to train the large models with your techniques and distill a set of smaller models. This technique is very good for saving computing costs, and DINOv2 is built with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39802c-057f-40b1-b992-3035030c2a80",
   "metadata": {},
   "source": [
    "#### Power of data\n",
    "- DINOv2 is trained on a colossal dataset comprising over 142 million images.\n",
    "- The dataset encompasses a wide variety of scenes, objects, and viewpoints, crucial for learning representations applicable across different tasks.\n",
    "- This massive scale training enables the model to learn richer, more generalizable visual representations that capture the intricate nuances of the visual world.\n",
    "- Training with massive batches allows the model to learn from a more diverse set of examples simultaneously, leading to better generalization and faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e07c4-c99f-4870-9574-b75d4e61b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a473f-5cde-41e6-8e94-a40d5ac67298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b6ef1-3d00-48fe-8188-2e314e6424cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccc5f-d964-4a47-8857-67e1ba4dd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c599f2-535e-4281-a5c4-5e113c608865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee9121-22b9-49ec-95ef-daa5f6d64171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import torchvision.transforms as T\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1a9b4-b459-4fd3-b7f3-f4d6ebb20a75",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Sample workflow</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15cf4a-36e4-43dd-a831-f1439cb960a9",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Choose your device</font>\n",
    "\n",
    "Use CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c96753-645b-4e60-a2c7-d3f7f60a4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec2e93-73c5-4bdd-ab29-0511e307ea29",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Load the DINOv2 model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cdb4e-ffbf-4e31-ae6a-aaebc7de842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b8289-0f79-44a5-aca8-a3fe115835fa",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Bring up the model to the device</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdc54f-e7c9-4fc3-a7c6-02eab2a6382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275d74e-cc22-40af-85f5-07074c4c1309",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Set the model to evaluation mode</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e366e5b-1510-4f3e-8a67-d20f06cd98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4c003-de1c-4c79-8211-1627aba69dc6",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Get the image of interest</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca4d56-2fc2-4999-9b76-70ded2e27008",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275dacb-0394-4971-8e05-c45789a5e8d3",
   "metadata": {},
   "source": [
    "#### Access image properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5e06a-8c36-47e1-94fc-f71a5adefbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff97cf4-1f4c-46d6-8e0c-88c3ac15588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceba7d2-bb5b-47b6-90f6-1ea736205c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0197c7-1bcc-4c55-8cfe-e3b32bbe6d01",
   "metadata": {},
   "source": [
    "#### Display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ffdca-a1a7-4816-9775-66f634af4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d3047-8028-4513-9c36-1fd5d3131761",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Create a image preprocessor and apply over input image</font>\n",
    "\n",
    "- We preprocess the image to make it ready for the model.\n",
    "- We use the `torchvision.transforms` modeule to perform a series of manipulations on the image:\n",
    "   - `Resize()`: Resize the input to the given size.\n",
    "   - `ToTensor()`: Convert the image to a tensor.\n",
    "      - A Tensor Image is  a tensor with (`C`, `H`, `W`) shape, where `C` is a number of channels, `H` and `W` are image height and width. \n",
    "   - `Normalize()`: Normalize the tensor image with mean (mean values for the three channels) and standard deviation (std values for the three channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1fc78-fe5a-4bad-af8c-5a81be544867",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),       \n",
    "    transforms.ToTensor(),              \n",
    "    transforms.Normalize(                \n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d64839-7516-429e-ba8a-1b2a80ac2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = transform(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4cbc9-dfb6-4646-a6ff-845096eb8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b048c64-0fe4-432e-9c3f-e03d42fd3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e9b3d-e933-4b58-b335-1e8fd553faf4",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Feed the image to the model to extract features</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75e214-d3a0-43f4-91ba-3ade141fa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = dinov2_vits14(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abd99a-8884-4b4a-b38c-ca38a4fdff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features.shape)\n",
    "# Expected output: torch.Size([1, 384]) for dinov2_vits14 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc5fb3-36a0-425c-a42f-6b10e9834fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.array(features[0].cpu().numpy()).reshape(1, -1)\n",
    "#a.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bffcfb-d211-461c-8013-f2923640de65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0ecdb-d81b-4dba-b76c-8c380c3cd7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
