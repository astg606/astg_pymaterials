{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee93059e-f5a3-41c6-ade1-403dfadeea34",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Course Series</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Introduction to DINOv2 with PyTorch</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dde6eb-fba1-45ef-822a-19551972c42a",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Objective</font>\n",
    "\n",
    "- Provide an overview of DINOv2\n",
    "   - Need for foundation models capable of generating features that work out of the box on any task.\n",
    "   - How DINOv2, a self-supervized model, works to create vector embeddings for representing a large collection of images. DINOv2 can then be used to transfer the representations to AI models for better performance.\n",
    "- Describe how DINOv2 can be combined with PyTorch to create model:\n",
    "   - Show how to extract vector embeddings with DINOv2.\n",
    "   - Create various models to solve the MNIST handwritten digit classification problem.\n",
    "   - Show how to use PyTorch Lightning.\n",
    "\n",
    "### <font color=\"green\">Concepts and key concepts</font>\n",
    "\n",
    "- __Foundation models__: AI models trained on vast, immense datasets and can fulfill a broad range of general tasks. They serve as the base or building blocks for crafting more specialized applications.\n",
    "- __Embeddings__: An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n",
    "- __PyTorch Lightning__: A PyTorch-based high-level Python framework that aims to simplify the training and deployment of models by providing a lightweight and standardized interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea72fa-0bfc-46ab-8924-4d0cd4701694",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Background </font>\n",
    "\n",
    "- Labeling images is one of the most time consuming parts of training a computer vision (CV) model:\n",
    "   - Each object you want to identify needs to be labeled precisely.\n",
    "   - It is computational intensive not only to gather and precisely label data when we deal with large datasets.\n",
    "   - This is not only a limitation for subject matter-specific models, but also large general models that strive for high performance across a wider range of classes.\n",
    "- The field of Natural Language Processing (NLP) has had rich featurization available in the form of vector embeddings.\n",
    "- The NLP foundational embeddings paved the way for a large range of applications.\n",
    "  - Vector embeddings underpin nearly all modern machine learning, powering models used in the fields of NLP and CV, and serving as the fundamental building blocks of generative AI.\n",
    "- __We need CV foundation models capable of generating visual features that work out of the box on any task, both at the image level, e.g., image classification, and pixel level, e.g., segmentation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94149b-3311-4c22-b469-523aeb01b167",
   "metadata": {},
   "source": [
    "# <font color=\"red\">What is DINOv2? </font>\n",
    "\n",
    "DINOv2 (self-__DIstillation of knowledge with NO labels v2__) is:\n",
    "\n",
    "- A cutting-edge self-supervised vision transformer developed by [Meta AI](https://arxiv.org/abs/2304.07193?ref=blog.roboflow.com). \n",
    "- A model that is trained to learn from the data itself, without the need for human-labeled annotations. It generates its own supervisory signals from the input data, making it a form of unsupervised learning.\n",
    "   - The model learns to understand the underlying structure and relationships within the data, effectively learning useful representations.\n",
    "   - Traditional deep learning models for CV often rely on massive amounts of labeled data, which can be expensive and time-consuming to acquire.\n",
    "- DINOv2 generates rich image representation vectors, also known as embeddings, that capture visual structure, texture, and spatial detail of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec8f21-bd77-43ad-b5ef-938947e08c03",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Self-supervised model</font>\n",
    "- DINOv2 is a self-supervised vision transformer model that consists of a __family of foundation models__ producing universal features suitable for image-level visual tasks (image classification, instance retrieval, video understanding) as well as pixel-level visual tasks (depth estimation, semantic segmentation).\n",
    "- It is an advanced self-supervised learning technique to train models, enhancing computer vision by accurately identifying individual objects within images and video frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9055d-7904-481d-a226-1ebc6d72a89d",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Self-distillation framework</font>\n",
    "\n",
    "- DINOv2 uses self-supervized learning (SSL) and knowledge (or model) distillation methods.\n",
    "   - SSL is a “self-supervision” technique that involves a two-step process of pretraining and fine-tuning, where models learn representations from unlabeled data through auxiliary tasks and adapt to specific tasks using smaller amounts of labeled data. \n",
    "   - Knowledge distillation is the process of training a smaller model to mimic the larger model. In this case, you transfer the knowledge from the larger model (often called the “teacher”) to the smaller model (often called the “student”).\n",
    "      - __Step 1__: Train the teacher model with labeled data; it produces an output, so you map the input and output from the teacher model and use the smaller model to copy the output, while being more efficient in terms of model size and computational requirements.\n",
    "      - __Step 2__: Use a large dataset of unlabeled data to train the student models to perform as well as or better than the teacher models. The idea here is to train the large models with your techniques and distill a set of smaller models. This technique is very good for saving computing costs, and DINOv2 is built with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39802c-057f-40b1-b992-3035030c2a80",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Power of data</font>\n",
    "- DINOv2 is trained on a colossal dataset comprising over 142 million images.\n",
    "- The dataset encompasses a wide variety of scenes, objects, and viewpoints, crucial for learning representations applicable across different tasks.\n",
    "- This massive scale training enables the model to learn richer, more generalizable visual representations that capture the intricate nuances of the visual world.\n",
    "- Training with massive batches allows the model to learn from a more diverse set of examples simultaneously, leading to better generalization and faster convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba49ebff-7116-4e8f-b5b1-c55dac67944f",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Benefits</font>\n",
    "\n",
    "- Being self-supervised, DINOv2 eliminates the need for labeled input data, allowing models built on this framework to acquire more comprehensive insights into image content.\n",
    "- DINOv2 utilizes a self-supervised learning technique, enabling the model to be trained on unlabeled images, yielding two significant advantages:\n",
    "   - The approach eliminates the need for substantial time and resource investment in labeling data.\n",
    "   - The model gains more profound and meaningful representations of the image input since it is directly trained on the images themselves.\n",
    "- Pre-training models with self-supervised learning and then fine-tuning them on specific downstream tasks has become a successful approach for transfer learning, enabling models to perform well even with limited labeled data for the target task.\n",
    "   - Fine-tuning is the process of retraining the model on a very limited set of examples to improve its performance on a specific task. \n",
    "   - DINOv2 can learn adaptable, high-quality, all-purpose visual features, enabling it to perform various computer vision tasks, such as classification, estimating depth, semantic segmentation, instance retrieval, and more, without fine-tuning specific tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fad95b4-88c6-44a8-a732-20b3eba239c8",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Python packages used</font>\n",
    "\n",
    "- __Matplotlib__: Create visualization.\n",
    "- __Pandas__: Data (two-dimensional labelled array) manipulation and analysis.\n",
    "- __PyTorch__: Used to to build, train, and evaluate a deep machine learning algorithm based on Neural Networks.\n",
    "- __PyTorch Lightning__: A wrapper framework for PyTorch that makes it easy to develop and train deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243b338-220e-4f9a-82ea-2a73fc3d16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    print(\"Not running in Google Colab\")\n",
    "else:\n",
    "    print(\"Installing modules in Google Colab\")\n",
    "    !pip3 uninstall --yes torch torchaudio torchvision torchtext torchdata\n",
    "    !pip3 install torch torchaudio torchvision torchtext torchdata\n",
    "    !pip install pytorch_lightning\n",
    "    !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf69514-b2ee-4ddb-b2fb-8a73bc92739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a473f-5cde-41e6-8e94-a40d5ac67298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b6ef1-3d00-48fe-8188-2e314e6424cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ccc5f-d964-4a47-8857-67e1ba4dd225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c599f2-535e-4281-a5c4-5e113c608865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6506e1-6ecb-43ac-a197-9d1c06fe5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#import torchvision.transforms as T\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms \n",
    "#from transformers import AutoImageProcessor, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b311f1c-5ba6-4b81-877d-5fa81b4e52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lightning version: \", pl.__version__)\n",
    "print(\"Torch version:     \", torch.__version__)\n",
    "print(\"CUDA is available: \", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1a9b4-b459-4fd3-b7f3-f4d6ebb20a75",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Obtaining embeddings with DINOv2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec15cf4a-36e4-43dd-a831-f1439cb960a9",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Choose your device</font>\n",
    "\n",
    "Use CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c96753-645b-4e60-a2c7-d3f7f60a4fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcec2e93-73c5-4bdd-ab29-0511e307ea29",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Load the DINOv2 model</font>\n",
    "- DINOv2 is a family of self-supervised Vision Transformer (ViT) models.\n",
    "- The models vary in size and complexity, and include `ViT-S/14` and `ViT-L/14`:\n",
    "   - `ViT-S/14` (Small, `dinov2_vits14`): This is a relatively smaller model in the DINOv2 family.\n",
    "      - It offers a good balance between performance and computational efficiency, making it suitable for applications with limited resources. It's a good choice when you need solid performance without the extensive computational cost of larger models.\n",
    "      - It uses 22M parameters and generates feature vector size of 384. \n",
    "   - `ViT-L/14` (Large, `dinov2_vitl14`): This is a larger model with potentially more Transformer layers and attention heads, which can lead to improved performance, especially on complex tasks.\n",
    "      - It's the second largest model in the DINOv2 series, with `ViT-G/14` being the largest.\n",
    "      - `ViT-L/14` excels in applications where accuracy is paramount, even with increased computational cost.\n",
    "      - It uses 300M parameters and creates  a feature vector size of 1,024. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c772cc4-63b9-4c2e-9599-0c62ce261a8c",
   "metadata": {},
   "source": [
    "Here we use the small ViT: `dinov2_vits14`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27558ed6-5bd0-4036-9dd8-11d2a6685c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_name_small = \"dinov2_vits14\"\n",
    "vit_name_large = \"dinov2_vitl14\"\n",
    "vit_name_grande = \"dinov2_vitg14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397cdb4e-ffbf-4e31-ae6a-aaebc7de842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", vit_name_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b8289-0f79-44a5-aca8-a3fe115835fa",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Bring up the model to the device</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdc54f-e7c9-4fc3-a7c6-02eab2a6382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a275d74e-cc22-40af-85f5-07074c4c1309",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Set the model to evaluation mode</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e366e5b-1510-4f3e-8a67-d20f06cd98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_vits14.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4c003-de1c-4c79-8211-1627aba69dc6",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Get a sample image</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca4d56-2fc2-4999-9b76-70ded2e27008",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = 'http://images.cocodataset.org/val2017/000000039769.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483de56b-9e8f-4e11-be4b-908fe0a81fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(requests.get(image_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275dacb-0394-4971-8e05-c45789a5e8d3",
   "metadata": {},
   "source": [
    "__Access image properties__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5e06a-8c36-47e1-94fc-f71a5adefbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_image.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff97cf4-1f4c-46d6-8e0c-88c3ac15588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_image.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceba7d2-bb5b-47b6-90f6-1ea736205c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_image.mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0197c7-1bcc-4c55-8cfe-e3b32bbe6d01",
   "metadata": {},
   "source": [
    "__Display the image__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ffdca-a1a7-4816-9775-66f634af4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d3047-8028-4513-9c36-1fd5d3131761",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Create a image preprocessor and apply it over input image</font>\n",
    "\n",
    "- Data transformation is an essential preprocessing step that prepares raw data for models.\n",
    "- Transformations like resizing, converting images to tensors, or normalizing pixel values are common for image data. These transformations help a model to see data in a consistent, well-scaled format.\n",
    "- We use the `torchvision.transforms` module to perform a series of manipulations on the image:\n",
    "   - `Resize()`: Resize the input to the given size expected by DINOv2.\n",
    "      - DINOv2 was primarily pre-trained at a high resolution of $518\\times518$ pixels, but for fine-tuning and inference on standard datasets, it is commonly adapted for $224\\times224$ images\n",
    "   - `ToTensor()`: Convert the image to a tensor. In PyTorch, models operate on tensors, so images (or any data) need to be converted into tensors before they can be fed into a model. \n",
    "      - A Tensor Image is  a tensor with (`C`, `H`, `W`) shape, where `C` is a number of channels, `H` and `W` are image height and width. \n",
    "   - `Normalize()`: Adjust the pixel values of an image so that they fall within a specific range. It standardizes the data, making it easier for the model to learn patterns. Here, we normalize the tensor image with `mean` and `standard deviation` (`mean` and `std` values are set for the three channels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1fc78-fe5a-4bad-af8c-5a81be544867",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),       \n",
    "    transforms.ToTensor(),              \n",
    "    transforms.Normalize(                \n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2ca2b7-2b47-4b05-b3c7-7cfa6f353102",
   "metadata": {},
   "source": [
    "__Tranform the input image and feed it to the `device`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d64839-7516-429e-ba8a-1b2a80ac2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "transformed_image = transform(input_image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4cbc9-dfb6-4646-a6ff-845096eb8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b048c64-0fe4-432e-9c3f-e03d42fd3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733e9b3d-e933-4b58-b335-1e8fd553faf4",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Feed the transformed image to the model to extract embeddings</font>\n",
    "\n",
    "- The embedding is a vector that represents the image in the DINOv2 model latent space.\n",
    "- It contains a compressed representation of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0119c-4b3d-4631-addb-273544d638fe",
   "metadata": {},
   "source": [
    "__Compute the `embeddings`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75e214-d3a0-43f4-91ba-3ade141fa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    embeddings = dinov2_vits14(transformed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2503fca-82ea-4eae-a4cc-bf5b8fd72f1b",
   "metadata": {},
   "source": [
    "__Shape of the `embeddings`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abd99a-8884-4b4a-b38c-ca38a4fdff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d662d-5025-422d-accc-7406841876d3",
   "metadata": {},
   "source": [
    "__Extract the embeddings from the `device` and create a NumPy array__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0babb03-35b7-4abe-831d-446a63235b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42161e89-b4fb-484c-a64d-9d3778ebed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8ba58-18bb-4a0a-932b-cc98fe4f6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b403d7-9cb2-4275-90bb-775dcc20b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adb41e-0d27-4c88-b7e0-75c2942f5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\")\n",
    "print(f\"Min value: {np_embeddings.min()}\")\n",
    "print(f\"Max value: {np_embeddings.max()}\")\n",
    "print(f\"STD value: {np_embeddings.std()}\")\n",
    "print(f\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c9e902-e1df-4433-b541-6344803bca90",
   "metadata": {},
   "source": [
    "__Plot the `embeddings`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea18665-10b4-4e1b-b0b8-206438af144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np_embeddings.reshape(1, -1), aspect='auto')\n",
    "plt.colorbar(label='Intensity')\n",
    "plt.title('Embedded Image')\n",
    "plt.xlabel('Index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3498d6-5943-4167-92ce-dd5611e4f619",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Obtain and pre-process MNIST handwritten digit dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb2a1aa-88e3-4880-beb8-6a980d633167",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Load the data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035ba8b-3231-4ad8-ad7b-84361c20fa4d",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Create custom PyTorch dataset </font>\n",
    "\n",
    "- The `Dataset` class is the primary tool for handling data.\n",
    "- It acts as an interface that allows users to define how their data is accessed from files, APIs, or even generated from scratch.\n",
    "- It helps prepare data for training by abstracting the complexities of data loading.\n",
    "- The three primary methods users need to implement when creating a custom dataset are:\n",
    "   - `__init__()`: Load the data into memory.\n",
    "   - `__len__ ()`: Define the total number of samples in our dataset.\n",
    "   - `__getitem__()`: Retrieve a specific data sample by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5178fdb-d34d-41a2-b810-bda670378a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitDataset(Dataset):\n",
    "    def __init__(self, sample_dataset, transform=None):\n",
    "        self.data = list()\n",
    "        self.labels = list()\n",
    "        for idx in range(len(sample_dataset)):   \n",
    "            self.data.append(np.array(sample_dataset[idx][0]))\n",
    "            self.labels.append(sample_dataset[idx][1])\n",
    "        self.transform = transform\n",
    "        print(f\"Dataset contains {len(sample_dataset)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Provide the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generates one sample of data.\n",
    "        \"\"\"\n",
    "        # Select sample associated with the provided index\n",
    "        image = self.data[idx].reshape(28, 28).astype('uint8')\n",
    "        \n",
    "        # Convert to 3-channel RGB\n",
    "        image = Image.fromarray(image).convert(\"RGB\")  \n",
    "\n",
    "        # If necessary perform the data transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3902b7f-4912-4fb7-882c-3df2bd2ecbfb",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Define transformations</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b55f0-e3a7-49f4-bdf7-9af85417f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                         std=(0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc534805-cfd7-4e71-b0d8-6a80d346fab2",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Get the MNIST dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c5418-2c5e-43ac-93d8-4676b50f7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_raw_dataset = datasets.MNIST(root='data', train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2cc331-7cde-4283-b24c-fa2a1b836ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_raw_dataset = datasets.MNIST(root='data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b7b4c-c215-45ad-ab02-c67eec75ffd9",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Make the dataset ready for PyTorch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e6490-c96d-429b-bc3b-3bbb445842be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_dataset = DigitDataset(train_raw_dataset, transform=transform)\n",
    "test_dataset = DigitDataset(test_raw_dataset, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6378a07e-07e7-423f-9eda-09bc185e1b70",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Extract a subset of the dataset</font>\n",
    "\n",
    "- We want to reduce the computational requirement by taking a subset of the data.\n",
    "\n",
    "__You may choose to skip this step if you want to use the entire dataset.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6b5cd-19e1-4694-82e1-77932dae043f",
   "metadata": {},
   "source": [
    "__Set seeds for reproducibility:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f312e4e-41e3-4568-90c1-711d17087b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e4fefd-2aca-45fb-beb6-164b5a378fc4",
   "metadata": {},
   "source": [
    "__Create random indices for sampling:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2bea7-7c14-494d-b0de-e42b644d83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain_data = 12800\n",
    "nval_data = 2000\n",
    "ntest_data = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58587e2c-787c-4ff8-8801-d02a27f5c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(train_dataset), size=ntrain_data, replace=False)\n",
    "test_indices = np.random.choice(len(test_dataset), size=ntest_data, replace=False)\n",
    "new_list = list(set(range(len(test_dataset))) - set(test_indices))\n",
    "val_indices = np.random.choice(new_list, size=nval_data, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20003dc-6c1e-4c1d-9776-b6f0dae90d70",
   "metadata": {},
   "source": [
    "__Create subsets__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b585e1-90e2-49ee-b4af-9ee280f7c324",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "val_dataset = Subset(test_dataset, val_indices)\n",
    "test_dataset = Subset(test_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9545d86-45c3-49f2-9ed4-918df97c2d34",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Test Case 1: Use a linear model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a80c637-5ec4-4109-a4dc-bd7389d482f5",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Description of the basic steps<font>\n",
    "\n",
    "- Load a pre-trained DINOv2 model using PyTorch Hub.\n",
    "- Define a function to load an MNIST image and transform it into a format accepted by DINOv2.\n",
    "- Compute DINOv2 embeddings for each MNIST image in your dataset.\n",
    "- Train a classifier on the generated embeddings and their corresponding labels.\n",
    "   - We can use a lightweight classifier, such as a Linear Support Vector Classification (SVC) model, on these embeddings.\n",
    "   - The trained classifier will then be able to accurately classify the MNIST digits based on the visual features extracted by DINOv2.\n",
    "- Use the trained classifier to predict the digit for new MNIST images. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043c21d-069f-4f9b-8a80-b8d244d5fd1b",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Define the model</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91bfc4a-7ffc-48f8-b850-68d0afa2ccd2",
   "metadata": {},
   "source": [
    "#### Head model\n",
    "\n",
    "- Create a PyTorch model that will be the final layer(s) of the overall neural network.\n",
    "- It will produce the output for the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31651ce-ef05-4841-8662-e3ea7ebad828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifierHead(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_hidden_nodes: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb4244-449b-434c-9e9b-3bbd8e468cb5",
   "metadata": {},
   "source": [
    "#### Model that combines DINOv2 and the head model\n",
    "\n",
    "- Create a PyTorch model where:\n",
    "   - The images will first be fed into DINOv2 to create the embeddings.\n",
    "   - The embeddings are inputs of the head model that will classify the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ae991-92e8-4db9-a8bf-6e024d00b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, dinov2_model, num_hidden_nodes, num_classes):\n",
    "        super().__init__()\n",
    "        self.dinov2_vits14 = dinov2_model\n",
    "        try:\n",
    "            self.embed_dim = dinov2_model.embed_dim\n",
    "        except:\n",
    "            self.embed_dim = dinov2_model.config.hidden_size\n",
    "        # Replace the original head or add a new one\n",
    "        # DINOv2 typically outputs a feature vector, so a linear layer is suitable\n",
    "        self.image_classifier = LinearClassifierHead(self.embed_dim, num_hidden_nodes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.dinov2_vits14(x)\n",
    "        \n",
    "        logits = self.image_classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae6cd6-a237-4fc2-ab75-82c3aa0c0fc9",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Create and train the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3481771-1215-4c88-848c-05ed92da97f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e5ff2-19ec-4017-8854-b8ac4955dd0f",
   "metadata": {},
   "source": [
    "## <font color=\"green\"> Set hyperparameters</font>\n",
    "\n",
    "The following parameters can be adjusted as needed for model speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ff4fd-cfac-402b-b3fc-dfae5d13bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21c414-ccf1-45a4-bf43-cc7e5d884d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93942a36-0b7f-4e97-b395-1355d97a0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5562bcc-d5aa-4736-b152-ff5dbde65008",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_nodes=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c468a61-e4f3-4015-be8c-d9977e51366f",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Load a pre-trained DINOv2 model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cbe9c2-8be2-4b17-aabe-ba302724f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_model = torch.hub.load('facebookresearch/dinov2', vit_name_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c492684-139e-482f-9e13-cfd497144015",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    embed_dim = dinov2_model.embed_dim\n",
    "except:\n",
    "    embed_dim = dinov2_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0d7728-4d2e-4037-a657-f9fe2fc5cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"DINOv2 embed_dim: \\n\\t {embed_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ddb04a-4a7e-474b-a774-a5d730e3ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bce284-90dd-48e1-a836-3540278d0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b054f8e-4d43-420a-98e1-9ec27a517bea",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Freeze DINOv2 backbone parameters</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f8d91-7ca6-4d21-bc67-4bbc9f91f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in dinov2_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c02832-f27b-401f-a09f-b90d9c1b5177",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Create model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0f96f-4b30-4ef1-a2ba-9e7bb1b2e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = CustomModel(\n",
    "    dinov2_model=dinov2_model, \n",
    "    num_hidden_nodes = num_hidden_nodes, \n",
    "    num_classes=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eec958-331d-42e2-9d0d-24e8927f00b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae1927b-0874-4b61-bc7d-beb337475280",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t Model information: \\n')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e03bbe1-ac59-4b80-87fb-0441fccda403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters_per_layer(model):\n",
    "    n = 20\n",
    "    m = 10\n",
    "    p = n+m+2\n",
    "    print(f\"{'-'*p}\")\n",
    "    print(f\"{'Modules':<{n}}  {'Parameters':{m}}\")\n",
    "    print(f\"{'-'*p}\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:<{n}}  {param.numel():{m}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d6f3d-ba93-4d39-9bc8-3c507e44c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters_per_layer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef749e3e-8919-4d0f-9595-58b0b3828174",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.image_classifier.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142849e-485d-4da9-b297-cfb1f36d0d41",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Define the loss function and the optimzer</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447697e2-f9a6-4806-8f6f-435d34559798",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8b4a2-846d-44e0-9d0b-701d987d43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a7444-12a6-4587-948e-2ea7f4d710e0",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Define DataLoader</font>\n",
    "\n",
    "- We pass the dataset to our dataloader, and our `batch_size` hyperparameter as initialization arguments.\n",
    "- This creates an iterable data loader, so we can easily iterate over each batch using a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf43ad80-f3d1-4738-9088-918327c571b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01245699-d875-4989-8bee-170d6b5759ee",
   "metadata": {},
   "source": [
    "__Let us check some examples (by using `test_loader`):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be737df0-227f-4426-90d8-add0848f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8723b5ad-33a4-455f-b0ae-dcc66c0c8bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5b0b3b-fbb7-4142-9213-20a91f4185cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffbf39-8f75-4071-b630-062759899a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(feature_data, label_data, ref_title=\"Ground Truth\"):\n",
    "    fig = plt.figure()\n",
    "    for i in range(6):\n",
    "        plt.subplot(2,3,i+1)\n",
    "        plt.imshow(feature_data[i][0], cmap='gray', interpolation='none')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(f\"{ref_title}: {label_data[i]}\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7073e6f-f86f-40cd-b253-5edf84db4ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_images(example_data, example_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be528492-2766-412d-9221-64ceee45e1a9",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Define functions to train per batch and per epoch </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52730f1e-df4b-4baf-bc93-3dddc8c63f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_per_batch(data, target, mymodel, \n",
    "                          myloss_function, myoptimizer) -> float:\n",
    "    #data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Zero the gradients\n",
    "    myoptimizer.zero_grad()\n",
    "\n",
    "    # Perform forward pass\n",
    "    feature = mymodel(data)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = myloss_function(feature, target)\n",
    "\n",
    "    # Perform backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Perform optimization\n",
    "    myoptimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd85926-aeb8-497e-8f52-4d1de2eb917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_per_epoch(epoch_idx, mymodel, myloss_function, myoptimizer, \n",
    "                          dataloader_train, train_losses, train_counter):\n",
    "    # Put model in training model\n",
    "    mymodel.train()\n",
    "    n_dataloader = len(dataloader_train.dataset)\n",
    "    n_data_per_batch = len(dataloader_train)\n",
    "    print(f\"Inside train_model_per_epoch - Number of item: {n_dataloader}\")\n",
    "    for batch_idx, (data, target) in enumerate(dataloader_train):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        n_data = len(data)\n",
    "        loss_val = train_model_per_batch(data, target, mymodel, loss_function, optimizer)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch_idx} [{batch_idx*n_data}/{n_dataloader}' \n",
    "                  f'({100.*batch_idx/n_data_per_batch:.0f}%)]\\tLoss: {loss_val:.6f}')\n",
    "            train_losses.append(loss_val)\n",
    "            train_counter.append((batch_idx*64) + ((epoch_idx-1)*n_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7da63-017a-486e-b48b-5628c250dd1b",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Define function to evaluate the model accuracy</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e777d1d-abe2-472a-8e9b-0a966798f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader, test_loss):\n",
    "    \"\"\"\n",
    "    Compute the percentage of correct classification.\n",
    "    \"\"\"\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    n_items = len(dataloader.dataset)\n",
    "    print(f\"Inside compute_accuracy - Number of item: {n_items}\")\n",
    "    correct = 0.0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(logits, target).item()\n",
    "            # get the index of the max log-probability\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            correct += (pred == target).sum().item()\n",
    "\n",
    "    test_loss /= len(dataloader)\n",
    "    test_losses.append(test_loss)    \n",
    "    perc = 100. * correct / n_items\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{n_items} ({perc:.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679141df-d9ef-4fe9-a9c1-f8329cd695d6",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Train and evaluate the model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8262bf2c-1aef-4761-8feb-32d732066a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = list()\n",
    "train_counter = list()\n",
    "test_losses = list()\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(max_epochs+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa94a5-1fcb-46b9-859d-9973dd60be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "compute_accuracy(model, test_loader, test_losses)\n",
    "for epoch_idx in range(1, max_epochs+1):\n",
    "    train_model_per_epoch(epoch_idx, model, loss_function, optimizer,\n",
    "                         train_loader, train_losses, train_counter)\n",
    "    compute_accuracy(model, test_loader, test_losses)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7225d9-41a7-490c-a4c5-38b324dfeeca",
   "metadata": {},
   "source": [
    "__Plot the losses__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb3482-0da8-42a5-bba8-842607bf9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_counter, train_losses, test_counter, test_losses):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_counter, train_losses, color='blue')\n",
    "    plt.scatter(test_counter, test_losses, color='red')\n",
    "    plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "    plt.xlabel('Number of training examples seen')\n",
    "    plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b19bc-6eaf-4288-a36f-6fb5ec2cee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_counter, train_losses, test_counter, test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbeb7d8-fb22-4f05-9008-d2787b1d7701",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Test Case 2: PyTorch Lightning and a multi-layer NN model</font>\n",
    "\n",
    "- In the previous example, we combined DINOv2 with a simple linear model.\n",
    "- Here we use a multi-layer sequatial model.\n",
    "- We also use PyTorch Lightning that is designed to automate and simplify the training and deployment of deep learning models.\n",
    "   - It eliminates boilerplate code for training loops and complex setups, which is cumbersome for many developers, and allows users to focus on the core model and experiment logic.\n",
    "   - It automate the training loop: abstracts the codes related to to epoch and batch iteration, `optimizer.step()`, `loss.backward()`, `optimizer.zero_grad()`, and setting the model to `eval()` or `train()` mode.\n",
    "   - It simplifies complex setups like multi-GPU and distributed training (e.g., DDP) with minimal code changes, making it easy to scale training from a single device to multiple GPUs or TPUs.\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b629a09-4c42-41ac-83f9-24399d8d1fbf",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Custom classifier head</font>\n",
    "\n",
    "- This classifier is created for illustration only.\n",
    "- We create a sequential network consisting of:\n",
    "   - A fully-connected (Linear) layer with `num_hidden_nodes` nodes, followed by the `ReLU` activation function.\n",
    "   - A Dropout layer with a `20%` dropout rate to prevent overfitting.\n",
    "   - A second Linear layer, with `num_hidden_nodes` nodes, followed by the `ReLU` activation function.\n",
    "   - Another Dropout layer, that removes `20%` of the nodes.\n",
    "   - A final Linear layer, with `num_classes` nodes (matching the number of classes in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda15852-1f4f-460b-a3c5-7e280dace251",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden_nodes, num_classes):\n",
    "        super(ImageClassifierNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Classification head\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, num_hidden_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(num_hidden_nodes, num_hidden_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(num_hidden_nodes, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        output = self.net(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8b0e4-6712-4563-9611-71e422c3462c",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Lightning model with DINOv2 and custom head</font>\n",
    "\n",
    "When using `LightningModule`, the PyTorch code isn't abstracted; it’s organized into six sections:\n",
    "\n",
    "1. __Initialization__ (`__init__` and `setup()` methods).\n",
    "2. __Train loop__ (`training_step()` method).\n",
    "3. __Validation loop__ (`validation_step()` method).\n",
    "4. __Test loop__ (`test_step()` method).\n",
    "5. __Prediction loop__ (`prediction_step()` method).\n",
    "6. __Optimizers and LR schedulers__ (`configure_optimizers()`).\n",
    "\n",
    "Each the above methods needs to be included inside the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4e21a0-7e35-4c51-9b7f-0b18c597c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLightningModule(pl.LightningModule):\n",
    "    def __init__(self, dinov2_model, num_hidden_nodes, num_classes, learning_rate):\n",
    "        super().__init__()\n",
    "        self.dinov2_model = dinov2_model\n",
    "        try:\n",
    "            self.embed_dim = dinov2_model.embed_dim\n",
    "        except:\n",
    "            self.embed_dim = dinov2_model.config.hidden_size\n",
    "        print(f\"embed_dim = {self.embed_dim}\")\n",
    "        self.image_classifier = ImageClassifierNetwork(\n",
    "            self.embed_dim, \n",
    "            num_hidden_nodes, \n",
    "            num_classes\n",
    "        )\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            features = self.dinov2_model(x)\n",
    "        \n",
    "        logits = self.image_classifier(features)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)     \n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log(\"train_acc\", acc)\n",
    "        # Log the loss at each training step and epoch, create a progress bar\n",
    "        self.log(\"train_loss\", loss, \n",
    "                 on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        #self.log('train_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        #preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        #acc = (preds == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.image_classifier.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c97caa-80ff-440d-add4-308c6767a99c",
   "metadata": {},
   "source": [
    "__Create the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1b576-5d53-48a5-b561-ae739be83f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = MyLightningModule(\n",
    "    dinov2_model=dinov2_model, \n",
    "    num_hidden_nodes=num_hidden_nodes,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f13199-75f4-4680-b2b6-19cf46462df4",
   "metadata": {},
   "source": [
    "__Set up the `Trainer`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3cea4-03ae-4f84-a560-5ca272f72547",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_device = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d9d2a5-f3d6-446b-bb01-8726d4ebb037",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=which_device, \n",
    "    devices=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23efad-cc1d-4531-9508-d7993d83d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = new_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3827b4-edce-4f04-b769-9b14b87bf030",
   "metadata": {},
   "source": [
    "__Train and validate the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172d0f4-ace9-4c2e-a927-66846f9d003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.fit(new_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377177c2-4c7f-47b5-9dcc-be8f483821b0",
   "metadata": {},
   "source": [
    "__Test the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf679468-5ae5-4e19-bc61-fe3864939d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trainer.test(new_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08213446-2b1f-4208-abcd-89210ad45f64",
   "metadata": {},
   "source": [
    "- We write here the `predict` function to determine the prediction.\n",
    "- This is redundant and is meant for verification only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055dc62-3f46-4eb4-a167-39d065591222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(mymodel, dataloader, device):\n",
    "    mymodel = mymodel.to(device)\n",
    "    mymodel.eval()\n",
    "    all_preds = list()\n",
    "    all_actuals = list()\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = mymodel(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # Move predictions back to CPU and convert to numpy\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_actuals.extend(target.cpu().numpy())  \n",
    "    return all_actuals, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501c6a3-be49-43df-b75e-42b6f39226d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate predictions\n",
    "actuals, predictions = predict(new_model, test_loader, device)\n",
    "\n",
    "# Create submission file\n",
    "df = pd.DataFrame(\n",
    "    {'ImageId': range(1, len(predictions) + 1), \n",
    "     'Actuals': actuals, \n",
    "     'Predictions': predictions}\n",
    ")\n",
    "df.to_csv('test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb25789-456f-4ddc-97df-9d1e7f262d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = len(df[df.Actuals == df.Predictions])/len(df)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac0d7a-6c5c-4aa1-8544-5ef0b58f2409",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Test Case 3: Use DINOv2 embeddings as inputs of a NN</font>\n",
    "\n",
    "- In the previous examples, the first `layer` of the created models was the DINOv2 model.\n",
    "- The DINOv2 model is used only in evaluation mode, which primary role is to compute the embeddings.\n",
    "- At each epoch, the embeddings of each image are calculated. This makes the training, validation and testing time consuming.\n",
    "- We want to compute the DINOv2 embeddings first and use them as inputs to any neural network of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d248410-f9da-47f6-a6bb-a5ee91f242cc",
   "metadata": {},
   "source": [
    "\n",
    "__Step 1__: Steps to Generate Embeddings\n",
    "\n",
    "- Load DINOv2: Load a pre-trained DINOv2 model.\n",
    "- Obtain the MNIST Data\n",
    "- Apply necessary transformations to the MNIST images, such as resizing and normalization, to match the input requirements of the DINOv2 model.\n",
    "- Extract Embeddings: Feed the preprocessed MNIST images into the DINOv2 model. The output of the model, specifically the class token embedding or a global average pooled representation of the patch embeddings, will be the feature vector (embedding) for each MNIST image.\n",
    "\n",
    "__Step 2__: Training the PyTorch Model\n",
    "\n",
    "- Use the DINOv2 embeddings for the MNIST images as input to train a PyTorch classification model.\n",
    "- The model can be: \n",
    "   - A Linear Classifier: A simple linear layer can be sufficient to classify the embeddings.\n",
    "   - A Neural Network: Build a  feedforward neural network on top of the embeddings for potentially better classification performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e62bda-49ec-4fb9-b0ce-0642e5cb143e",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Compute the embeddings of all the images</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3115e72b-356d-42c6-a20a-bc6c358ea1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_labels(dataset, dino_model, device):\n",
    "    embeddings_obj = list()\n",
    "    labels_obj = list()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(dataset)):\n",
    "            data = dataset[i][0].unsqueeze(0).to(device)\n",
    "            \n",
    "            output = dino_model(data)\n",
    "            embeddings_obj.append(output.cpu())\n",
    "            labels_obj.append(dataset[i][1])\n",
    "    embeddings_obj = torch.cat(embeddings_obj, dim=0)\n",
    "    labels_obj = torch.tensor(labels_obj)\n",
    "\n",
    "    return embeddings_obj, labels_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9760b-0b9a-4328-9b9f-b3ec3a30b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    embed_dim = dinov2_model.embed_dim\n",
    "except:\n",
    "    embed_dim = dinov2_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad34220-9bd5-4bf2-adcb-f18b57995553",
   "metadata": {},
   "source": [
    "__Embeddings of the train set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff65cc-4058-4edf-8664-c11dc5ef5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_train, labels_train = create_embeddings_labels(train_dataset, dinov2_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7e35f-f27b-46c3-8bb6-957237d016b5",
   "metadata": {},
   "source": [
    "__Embeddings of the test set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00168a0d-2fba-4e37-98a6-7400bc7b9522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_test, labels_test = create_embeddings_labels(test_dataset, dinov2_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952084de-9f5e-4170-9ac0-ab7738838d54",
   "metadata": {},
   "source": [
    "__Embeddings of the validation set__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68cd3f5-8700-4604-bf6a-bd79339fd669",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_val, labels_val = create_embeddings_labels(val_dataset, dinov2_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800914a-4d3f-4ae3-b0e7-90d57d07c793",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Create the dataloaders for the train, test and validation sets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6e5c2-daf7-495b-b1fc-802ca6ac7d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset():\n",
    "    '''\n",
    "    Custom 'Dataset' object for our regression data.\n",
    "    Must implement these functions: __init__, __len__, and __getitem__.\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        self.features = X\n",
    "        self.labels = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.features[index]\n",
    "        y = self.labels[index]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb73b35-d1fb-41e3-91db-93932f133e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_data(Xdata, ydata, batch_size=64, shuffle=False):\n",
    "    dataset = MyDataset(Xdata, ydata)\n",
    "    dataloader = DataLoader(dataset=dataset, \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5520aa4-0693-4b55-9ccd-894a67163a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = instantiate_data(embeddings_train, labels_train, \n",
    "                                    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = instantiate_data(embeddings_test, labels_test, \n",
    "                                   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataloader = instantiate_data(embeddings_val, labels_val, \n",
    "                                  batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed99ed9-4758-4584-b93a-ef407b994726",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Model creation with PyTorch Lightning</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739efc8-34da-4e84-aefc-72ac842bc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericLightningModule(pl.LightningModule):\n",
    "    def __init__(self, nn_model, embed_dim, num_hidden_nodes, num_classes, learning_rate):\n",
    "        super().__init__()\n",
    "        self.image_classifier = nn_model(embed_dim, num_hidden_nodes, num_classes)\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.image_classifier(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)     \n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log(\"train_acc\", acc)\n",
    "        self.log(\"train_loss\", loss, \n",
    "                 on_step=True, on_epoch=True, prog_bar=True, logger=True) \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(1) == labels).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.image_classifier.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde00766-688e-4cae-8d5e-5a5f89cbac37",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Option 1: Linear model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424b3be-dcc0-4af1-a9be-248c0cbad0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = GenericLightningModule(\n",
    "    nn_model=LinearClassifierHead, \n",
    "    embed_dim=embed_dim,\n",
    "    num_hidden_nodes=num_hidden_nodes,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a3923-50d7-45d0-a4e7-592e7e46b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_device = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda50140-cb78-4541-86d0-06a6d31bc4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=which_device, \n",
    "    devices=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fab93f-bf55-4b1c-b286-94b1b83fc3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = linear_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797384c5-1191-414a-a14c-622e6995540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_trainer.fit(linear_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff920c-7d83-4c31-9201-b2155a61ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "linear_trainer.test(linear_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6f99e-fac9-4f6f-b6ef-cd3efe9c50cd",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Option 2: Multi-layer model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de2179-19fd-4926-ba7d-09c0d84691ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = GenericLightningModule(\n",
    "    nn_model=ImageClassifierNetwork, \n",
    "    embed_dim=embed_dim,\n",
    "    num_hidden_nodes=num_hidden_nodes,\n",
    "    num_classes=num_classes,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf3e5f-21d1-4f4c-82de-cc927494a6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    accelerator=which_device, \n",
    "    devices=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577733bc-b5c5-4e64-be61-20123590193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = nn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24961241-2e20-48fb-8ce3-ca822588544d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nn_trainer.fit(nn_model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68bcbdf-aefb-46c1-bf3c-e40fb52938ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nn_trainer.test(nn_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa601f7f-9b5a-44cf-9975-607fb930d213",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> References</font>\n",
    "\n",
    "- [DINOv2: Learning Robust Visual Features without Supervision](https://arxiv.org/pdf/2304.07193) by Maxime Oquab et al.\n",
    "- [DINOv2 by Meta: A Self-Supervised foundational vision model](https://learnopencv.com/dinov2-self-supervised-vision-transformer/) by Bhomik Sharma, April 2025.\n",
    "- [01.Meta-DinoV2-Getting Started](https://www.kaggle.com/code/shravankumar147/01-meta-dinov2-getting-started)\n",
    "- [DINOv2](https://huggingface.co/docs/transformers/en/model_doc/dinov2) from hugginface.co\n",
    "- [Building the DINO model from Scratch with PyTorch: Self-Supervised Vision Transformer](https://medium.com/thedeephub/self-supervised-vision-transformer-implementing-the-dino-model-from-scratch-with-pytorch-62203911bcc9) by Shubh Mishra\n",
    "- [How to Classify Images with DINOv2](https://blog.roboflow.com/how-to-classify-images-with-dinov2/) by James Gallagher (May 30, 2023\n",
    "- [Deploying DINOv2 to A Rest API Endpoint for Image Classification | Modelbit](https://colab.research.google.com/github/write-with-neurl/modelbit-09/blob/main/notebook/Deploying_DINOv2_for_Image_Classification_with_Modelbit.ipynb#scrollTo=q06RxQlCzQnG)\n",
    "- [DINOv2: Self-supervised Learning Model Explained](https://encord.com/blog/dinov2-self-supervised-learning-explained/) eNCORD Blog, November 2024.\n",
    "- [How to Classify Images with DINOv2](https://blog.roboflow.com/how-to-classify-images-with-dinov2/) by James Gallagher, May 2023.\n",
    "- [DinoV2 Fine-Tuning Tutorial: How to Maximize Accuracy for Computer Vision Tasks](https://kili-technology.com/data-labeling/computer-vision/dinov2-fine-tuning-tutorial-maximizing-accuracy-for-computer-vision-tasks) by Asmaa Mirkhan\n",
    "- [PyTorch Lightning: A Comprehensive Hands-On Tutorial](https://www.datacamp.com/tutorial/pytorch-lightning-tutorial) by Bex Tuychiev\n",
    "- [How to fine-tune DINOv2 for image classification](https://python-sdk-docs.kili-technology.com/latest/sdk/tutorials/finetuning_dinov2/) from Python SDK.\n",
    "- [A Beginner’s Guide to Vector Embeddings](https://www.tigerdata.com/blog/a-beginners-guide-to-vector-embeddings) from tigerdata.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
