{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Introduction to Scikit-Learn</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "- <a href=\"https://medium.com/towards-artificial-intelligence/calculating-simple-linear-regression-and-linear-best-fit-an-in-depth-tutorial-with-math-and-python-804a0cb23660\">Calculating Simple Linear Regression and Linear Best Fit an In-depth Tutorial with Math and Python</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/tutorial/index.html\">scikit-learn Tutorials</a>\n",
    "- <a href=\"https://medium.com/@amitg0161/sklearn-linear-regression-tutorial-with-boston-house-dataset-cde74afd460a\">Sklearn Linear Regression Tutorial with Boston House Dataset</a>\n",
    "- <a href=\"https://www.dataquest.io/blog/sci-kit-learn-tutorial/\">Scikit-learn Tutorial: Machine Learning in Python</a>\n",
    "- <a href=\"https://debuggercafe.com/image-classification-with-mnist-dataset/\">Image Classification with MNIST Dataset</a>\n",
    "- <a href=\"https://davidburn.github.io/notebooks/mnist-numbers/MNIST%20Handwrititten%20numbers/\">MNIST handwritten number identification</a>\n",
    "- [K-Fold Cross-Validation in Python Using SKLearn](https://www.askpython.com/python/examples/k-fold-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">What is Scikit-Learn?</font>\n",
    "\n",
    "![fig_sckl](https://ulhpc-tutorials.readthedocs.io/en/latest/python/advanced/scikit-learn/images/scikit.png)\n",
    "Image Source: ulhpc-tutorials.readthedocs.io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Scikit-Learn (`sklearn`) is a free Python Machine Learning library.\n",
    "- It is built on top on NumPy (Python library for numerical computing), SciPy,  and Matplotlib (Python library for data visualization).\n",
    "- Scikit-learn was developed with real-world problems in mind. It’s user-friendly with a simple and intuitive interface. \n",
    "- It offers a high degree of flexibility in fine-tuning models and can be used for both supervised and unsupervised machine learning algorithms. \n",
    "- It provides a selection of efficient tools for Machine Learning and statistical modeling including: \n",
    "     - **Classification:** Identifying which category an object belongs to. Example: Spam detection\n",
    "     - **Regression:** Predicting a continuous variable based on relevant independent variables. Example: Stock price predictions\n",
    "     - **Clustering:** Automatic grouping of similar objects into different clusters. Example: Customer segmentation \n",
    "     - **Dimensionality Reduction:** Seek to reduce the number of input variables in training data by preserving the salient relationships in the data.\n",
    "     - __Model Selection__: You can compare different ML algorithms and automatically tune their settings to find the best fit for your data.\n",
    "- Features various algorithms like support vector machine, random forests, and k-neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some popular groups of models provided by `sklearn` include:\n",
    "\n",
    "- **Clustering:** Group unlabeled data.\n",
    "- **Dimensionality Reduction:** Reduce the number of attributes in data for summarization, visualization and feature selection.\n",
    "- **Cross Validation:** Estimate the performance of supervised models on unseen data.\n",
    "- **Datasets:** for test datasets and for generating datasets with specific properties for investigating model behavior.\n",
    "- **Ensemble Methods:** Combine the predictions of multiple supervised models.\n",
    "- **Feature Extraction:** Define attributes in image and text data.\n",
    "- **Feature Selection:** Identify meaningful attributes from which to create supervised models.\n",
    "- **Parameter Tuning:** Get the most out of supervised models.\n",
    "- **Manifold Learning:** Summarize and depicting complex multi-dimensional data.\n",
    "- **Supervised Models:** A vast array not limited to generalized linear models, discriminate analysis, naive bayes, lazy methods, neural networks, support vector machines and decision trees.\n",
    "- **Unsupervised Learning Algorithms:** − They include clustering, factor analysis, PCA (Principal Component Analysis), unsupervised neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Scikit-Learn modules</font>\n",
    "\n",
    "We list a couple of `sklearn` modules that can be used to obtain datasets and/or select the appropriate estimator needed to solve a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Available datasets</font>\n",
    "\n",
    "The `sklearn.dataset` module  embeds some small toy datasets and provides helpers to fetch larger datasets commonly used by the machine learning community to benchmark algorithms on data that comes from the ‘real world’.\n",
    "\n",
    "The [dataset loading utilities](https://scikit-learn.org/stable/datasets.html) webpage provides a list of datasets can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Linear Regression</font>\n",
    "\n",
    "The `sklearn.linear_model` module has the following methods: \n",
    "\n",
    "- `LinearRegression()`: fits linear model to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "- `Ridge()`: implements the linear least squares with l2 regularization.\n",
    "- `RidgeClassifier()`: Classifier using Ridge regression.\n",
    "- `LogisticRegression()`: implements regularized logistic regression.\n",
    "- `Lasso()`: implements the linear model trained with L1 prior as regularizer (aka the Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Stochastic Gradient Descent</font>\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions.\n",
    "SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. \n",
    "\n",
    "The `sklearn.linear_model` module provides the methods:\n",
    "\n",
    "- `SGDClassifier()`: implements linear classifiers with SGD training. The gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n",
    "- `SGDRegressor()`: implements a linear model fitted by minimizing a regularized empirical loss with SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Nearest Neighbors</font>\n",
    "\n",
    "The `sklearn.neighbors` module provides functionality for unsupervised and supervised neighbors-based learning methods.\n",
    "\n",
    "- `NearestNeighbors()`: unsupervised learner for implementing neighbor searches.\n",
    "- `KNeighborsClassifier()`: classifier implementing the k-nearest neighbors vote.\n",
    "- `RadiusNeighborsClassifier()`: implements learning based on the number of neighbors within a fixed radius _r_ of each training point, where _r_ is a floating-point value specified by the user.\n",
    "- `KNeighborsRegressor()`: Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\n",
    "- `RadiusNeighborsRegressor()`: Regression based on neighbors within a fixed radius. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Gaussian Processes</font>\n",
    "\n",
    "Gaussian Processes (GP) are a nonparametric supervised learning method used to solve regression and probabilistic classification problems.\n",
    "\n",
    "Here are some of the functions in the  `sklearn.gaussian_process`:\n",
    "\n",
    "- `GaussianProcessRegressor()`: implements Gaussian processes (GP) for regression purposes.\n",
    "- `GaussianProcessClassifier()`: implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Decision Trees</font>\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "Here are functions available in the `sklearn.tree` module:\n",
    "\n",
    "- `DecisionTreeClassifier()`: performs multi-class classification on a dataset.\n",
    "- `DecisionTreeRegressor()`: performs a decision tree regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Ensembles: Gradient boosting, random forests</font>\n",
    "\n",
    "Ensemble methods combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability/robustness over a single estimator.\n",
    "The two main ensemble methods are gradient-boosted trees and random forests.\n",
    "\n",
    "The methods are available through the `sklearn.ensemble` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Gradient-boosted trees</font>\n",
    "\n",
    "Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an excellent model for both regression and classification, in particular for tabular data.\n",
    "\n",
    "- `GradientBoostingRegressor()`: Gradient Boosting for regression.\n",
    "- `GradientBoostingClassifier()`: Gradient Boosting for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Random forests</font>\n",
    "\n",
    "In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. \n",
    "\n",
    "- `RandomForestRegressor()`: random forest regressor.\n",
    "- `RandomForestClassifier()`: random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Clustering</font>\n",
    "\n",
    "Clustering of unlabeled data can be performed with the `sklearn.cluster` module.\n",
    "\n",
    "- `KMeans()`: clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares.\n",
    "- `BisectingKMeans()`: an iterative variant of `KMeans`, using divisive hierarchical clustering. Instead of creating all centroids at once, centroids are picked progressively based on a previous clustering: a cluster is split into two new clusters repeatedly until the target number of clusters is reached.\n",
    "- `AffinityPropagation()`: creates clusters by sending messages between pairs of samples until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Validation Tools</font>\n",
    "\n",
    "Knowing how to determine whether or not an estimator performs well is an essential part of machine\n",
    "learning. `sklearn` has validation tools for many situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Evaluation Metrics</font>\n",
    "\n",
    "The `sklearn.metrics` module implements functions for evaluating regression, non-binary classification, and clustering models. We can mention:\n",
    "\n",
    "- The `accuracy_score()` method returns the accuracy of the model, which is the percent of labels that are predicted correctly.\n",
    "- The `confusinon_matrix()` is used for classification problems where  the `(i, j)`th entry is the number of observations with actual label `i` but that are classified as label `j`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Cross Validation</font>\n",
    "The `sklearn.model_selection` module has utilities to streamline and improve model evaluation.\n",
    "\n",
    "- `train_test_split()`: randomly splits data into training and testing sets (we already used this).\n",
    "- `cross_val_score()`: randomly splits the data and trains and scores the model a set number of times. Each trial uses different training data and results in a different model. The function returns the score of each trial.\n",
    "- `cross_validate()`: does the same thing as `cross_val_score()`, but it also reports the time it took to fit, the time it took to score, and the scores for the test set as well as the training set.\n",
    "\n",
    "Doing multiple evaluations with different testing and training sets is extremely important. \n",
    "If the scores on a cross validation test vary wildly, the model is likely overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Model Selection Process</font>\n",
    "\n",
    "- Choosing the right algorithm is critical for solving a machine learning problem.\n",
    "- Different algorithms are better suited for different types of data and different problems.\n",
    "- The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.\n",
    "\n",
    "![fig_estimators](https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Scikit-Learn vs. TensorFlow vs. PyTorch</font>\n",
    "\n",
    "| __Feature__ |\t__Scikit-learn__ | \t__TensorFlow__ | __PyTorch__ |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| __Use Case__ |\tTraditional ML\t| Deep Learning\t| Deep Learning |\n",
    "| __Ease of Use__ |\tSimple API\t| Requires tuning\t| More flexible |\n",
    "| __Performance__ |\tEfficient for small datasets\t| Optimized for large datasets\t| GPU-accelerated |\n",
    "| __Scalability__ |\tLimited for big data\t| Distributed training\t| Dynamic computation graphs |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
