{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Introduction to Scikit-Learn</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Objectives</font>\n",
    "\n",
    "Scikit-learn (`sklearn`) is a free, open-source machine learning library for Python that provides simple and efficient tools for data analysis and modeling. In this presentation, we:\n",
    "\n",
    "- Describe the main components of `sklearn`.\n",
    "- Introduce the steps to select a `sklearn` built-in model.\n",
    "- Present the main steps of a `sklearn` workflow.\n",
    "- Briefly compare `sklearn` against the other Python frameworks.\n",
    "- Present examples (regression, classification) on how to create ML models with `sklearn`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">What is Scikit-Learn?</font>\n",
    "\n",
    "![fig_sckl](https://ulhpc-tutorials.readthedocs.io/en/latest/python/advanced/scikit-learn/images/scikit.png)\n",
    "Image Source: ulhpc-tutorials.readthedocs.io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Scikit-Learn (`sklearn`) is a free Python Machine Learning library.\n",
    "- It is built on top on NumPy (Python library for numerical computing), SciPy,  and Matplotlib (Python library for data visualization).\n",
    "- Scikit-learn was developed with real-world problems in mind. It’s user-friendly with a simple and intuitive interface. \n",
    "- It offers a high degree of flexibility in fine-tuning models and can be used for both supervised and unsupervised machine learning algorithms. \n",
    "- It provides a selection of efficient tools for Machine Learning and statistical modeling including: \n",
    "     - **Classification:** Identifying which category an object belongs to. Example: Spam detection\n",
    "     - **Regression:** Predicting a continuous variable based on relevant independent variables. Example: Stock price predictions\n",
    "     - **Clustering:** Automatic grouping of similar objects into different clusters. Example: Customer segmentation \n",
    "     - **Dimensionality Reduction:** Seek to reduce the number of input variables in training data by preserving the salient relationships in the data.\n",
    "     - __Model Selection__: You can compare different ML algorithms and automatically tune their settings to find the best fit for your data.\n",
    "- Features various algorithms like support vector machine, random forests, and k-neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some popular groups of models provided by `sklearn` include:\n",
    "\n",
    "- **Clustering:** Group unlabeled data.\n",
    "- **Dimensionality Reduction:** Reduce the number of attributes in data for summarization, visualization and feature selection.\n",
    "- **Cross Validation:** Estimate the performance of supervised models on unseen data.\n",
    "- **Datasets:** for test datasets and for generating datasets with specific properties for investigating model behavior.\n",
    "- **Ensemble Methods:** Combine the predictions of multiple supervised models.\n",
    "- **Feature Extraction:** Define attributes in image and text data.\n",
    "- **Feature Selection:** Identify meaningful attributes from which to create supervised models.\n",
    "- **Parameter Tuning:** Get the most out of supervised models.\n",
    "- **Manifold Learning:** Summarize and depicting complex multi-dimensional data.\n",
    "- **Supervised Models:** A vast array not limited to generalized linear models, discriminate analysis, naive bayes, lazy methods, neural networks, support vector machines and decision trees.\n",
    "- **Unsupervised Learning Algorithms:** − They include clustering, factor analysis, PCA (Principal Component Analysis), unsupervised neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Scikit-Learn modules</font>\n",
    "\n",
    "We list a couple of `sklearn` modules that can be used to obtain datasets and/or select the appropriate estimator needed to solve a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Available datasets</font>\n",
    "\n",
    "The `sklearn.dataset` module  embeds some small toy datasets and provides helpers to fetch larger datasets commonly used by the machine learning community to benchmark algorithms on data that comes from the ‘real world’.\n",
    "\n",
    "The [dataset loading utilities](https://scikit-learn.org/stable/datasets.html) webpage provides a list of datasets can be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Linear Regression</font>\n",
    "\n",
    "The `sklearn.linear_model` module has the following methods: \n",
    "\n",
    "- `LinearRegression()`: fits linear model to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n",
    "- `Ridge()`: implements the linear least squares with l2 regularization.\n",
    "- `RidgeClassifier()`: Classifier using Ridge regression.\n",
    "- `LogisticRegression()`: implements regularized logistic regression.\n",
    "- `Lasso()`: implements the linear model trained with L1 prior as regularizer (aka the Lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Stochastic Gradient Descent</font>\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions.\n",
    "SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. \n",
    "\n",
    "The `sklearn.linear_model` module provides the methods:\n",
    "\n",
    "- `SGDClassifier()`: implements linear classifiers with SGD training. The gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n",
    "- `SGDRegressor()`: implements a linear model fitted by minimizing a regularized empirical loss with SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Nearest Neighbors</font>\n",
    "\n",
    "The `sklearn.neighbors` module provides functionality for unsupervised and supervised neighbors-based learning methods.\n",
    "\n",
    "- `NearestNeighbors()`: unsupervised learner for implementing neighbor searches.\n",
    "- `KNeighborsClassifier()`: classifier implementing the k-nearest neighbors vote.\n",
    "- `RadiusNeighborsClassifier()`: implements learning based on the number of neighbors within a fixed radius _r_ of each training point, where _r_ is a floating-point value specified by the user.\n",
    "- `KNeighborsRegressor()`: Regression based on k-nearest neighbors. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.\n",
    "- `RadiusNeighborsRegressor()`: Regression based on neighbors within a fixed radius. The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Gaussian Processes</font>\n",
    "\n",
    "Gaussian Processes (GP) are a nonparametric supervised learning method used to solve regression and probabilistic classification problems.\n",
    "\n",
    "Here are some of the functions in the  `sklearn.gaussian_process`:\n",
    "\n",
    "- `GaussianProcessRegressor()`: implements Gaussian processes (GP) for regression purposes.\n",
    "- `GaussianProcessClassifier()`: implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Decision Trees</font>\n",
    "\n",
    "Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.\n",
    "\n",
    "Here are functions available in the `sklearn.tree` module:\n",
    "\n",
    "- `DecisionTreeClassifier()`: performs multi-class classification on a dataset.\n",
    "- `DecisionTreeRegressor()`: performs a decision tree regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Ensembles: Gradient boosting, random forests</font>\n",
    "\n",
    "Ensemble methods combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability/robustness over a single estimator.\n",
    "The two main ensemble methods are gradient-boosted trees and random forests.\n",
    "\n",
    "The methods are available through the `sklearn.ensemble` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Gradient-boosted trees</font>\n",
    "\n",
    "Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss functions. GBDT is an excellent model for both regression and classification, in particular for tabular data.\n",
    "\n",
    "- `GradientBoostingRegressor()`: Gradient Boosting for regression.\n",
    "- `GradientBoostingClassifier()`: Gradient Boosting for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Random forests</font>\n",
    "\n",
    "In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. \n",
    "\n",
    "- `RandomForestRegressor()`: random forest regressor.\n",
    "- `RandomForestClassifier()`: random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Clustering</font>\n",
    "\n",
    "Clustering of unlabeled data can be performed with the `sklearn.cluster` module.\n",
    "\n",
    "- `KMeans()`: clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares.\n",
    "- `BisectingKMeans()`: an iterative variant of `KMeans`, using divisive hierarchical clustering. Instead of creating all centroids at once, centroids are picked progressively based on a previous clustering: a cluster is split into two new clusters repeatedly until the target number of clusters is reached.\n",
    "- `AffinityPropagation()`: creates clusters by sending messages between pairs of samples until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Validation Tools</font>\n",
    "\n",
    "Knowing how to determine whether or not an estimator performs well is an essential part of machine\n",
    "learning. `sklearn` has validation tools for many situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Evaluation Metrics</font>\n",
    "\n",
    "The `sklearn.metrics` module implements functions for evaluating regression, non-binary classification, and clustering models. We can mention:\n",
    "\n",
    "- The `accuracy_score()` method returns the accuracy of the model, which is the percent of labels that are predicted correctly.\n",
    "- The `confusinon_matrix()` is used for classification problems where  the `(i, j)`th entry is the number of observations with actual label `i` but that are classified as label `j`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Cross Validation</font>\n",
    "The `sklearn.model_selection` module has utilities to streamline and improve model evaluation.\n",
    "\n",
    "- `train_test_split()`: randomly splits data into training and testing sets (we already used this).\n",
    "- `cross_val_score()`: randomly splits the data and trains and scores the model a set number of times. Each trial uses different training data and results in a different model. The function returns the score of each trial.\n",
    "- `cross_validate()`: does the same thing as `cross_val_score()`, but it also reports the time it took to fit, the time it took to score, and the scores for the test set as well as the training set.\n",
    "\n",
    "Doing multiple evaluations with different testing and training sets is extremely important. \n",
    "If the scores on a cross validation test vary wildly, the model is likely overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Hyperparameter tuning</font>\n",
    "\n",
    "- In any ML project, we train different models on the dataset and select the one with the best performance.\n",
    "   - There is always room for improvement as we cannot say for sure that a particular model is best for the problem at hand.\n",
    "   - Our goal is to improve the model in any way possible.\n",
    "   - One important factor in the performances of these models are their hyperparameters, once we set appropriate values for these hyperparameters, the performance of a model can improve significantly. \n",
    "- `GridSearchCV` is a powerful tool within the `sklearn.model_selection` module, used for hyperparameter tuning.\n",
    "   - It systematically searches through a predefined set of hyperparameter values for a given ML model to find the combination that yields the best performance.\n",
    "   - By performing an exhaustive search over a set of hyperparameters, the function evaluates each combination using cross-validation and returns the best hyperparameter combination according to the model performance target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Model selection process</font>\n",
    "\n",
    "- Choosing the right algorithm is critical for solving a ML problem.\n",
    "- Different algorithms are better suited for different types of data and different problems.\n",
    "- The flowchart below is designed to give users a bit of a rough guide on how to approach problems with regard to which estimators to try on your data.\n",
    "\n",
    "![fig_estimators](https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Basic pipeline</font>\n",
    "\n",
    "![fig_workflow](https://images.ctfassets.net/aq13lwl6616q/2hl4fGgNuufSJRVSOCeelI/127a8077a7edb70ccac6652c8b097840/scikit_learn_workflow.jpg?w=720&fm=webp)\n",
    "Image source: Daniel Bourke\n",
    "\n",
    "A typical Scikit-Learn workflow involves the following steps:\n",
    "\n",
    "- __Data preparation__: Load and preprocess your data. This often includes performing Exploratory Data Analysis (EDA) and splitting data into different sets (training, validation, testing).\n",
    "- __Model selection__: Choose an appropriate ML algorithm (e.g., `LinearRegression`, `KNeighborsClassifier`, `SVC`, etc.) together with relevant hyperparameters.\n",
    "- __Model training__: Train the model using the `.fit()` method, providing the training features and targets.\n",
    "- __Prediction__: Use the trained model's `.predict()` method to make predictions on new, unseen data.\n",
    "- __Evaluation__: Assess the model's performance using relevant metrics (e.g., accuracy, precision, recall, R-squared).\n",
    "- __Hyperparameter tuning__: Optimize model parameters (hyperparameters) to improve performance, often using techniques like `GridSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Scikit-Learn vs. TensorFlow vs. PyTorch</font>\n",
    "\n",
    "With `sklearn`, you:\n",
    "\n",
    "- Are learning ML fundamentals\n",
    "- Can quickly create prototypes with classical algorithms\n",
    "- Can only manipulate datasets that fit in memory\n",
    "- Use a consistent, simple API\n",
    "- Work with structured, tabular data.\n",
    "\n",
    "When choosing a ML framework, it’s important to understand how `sklearn` compares to TensorFlow and PyTorch.\n",
    "The table below summarizes the main features of each framework.\n",
    "\n",
    "| __Feature__ |\t__Scikit-learn__ | \t__TensorFlow__ | __PyTorch__ |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| __Use Case__ |\tTraditional ML\t| Deep Learning\t| Deep Learning |\n",
    "| __Ease of Use__ |\tSimple API\t| Requires tuning\t| More flexible |\n",
    "| __Performance with large data__ |\tLimited\t|\tExcellent\t|\tExcellent\t|\n",
    "| __Scalability__ |\tLimited for big data\t| Distributed training\t| Dynamic computation graphs |\n",
    "| __GPU acceleration__ |\tLimited\t|\tExtensive\t|\tExtensive\t|\n",
    "| __Neural network support__ |\tBasic\t|\tAdvanced\t|\tAdvanced\t|\n",
    "| __Deployment__ |\tSimple\t|\tProduction-ready\t|\tResearch-friendly\t|\n",
    "| __Community size__ |\tLarge\t|\tVery large\t|\tLarge and growing\t|\n",
    "| __Ideal use cases__ |\tClassical ML, prototyping, tabular data\t|\tProduction deep learning, deployment\t|\tResearch, experimentation, flexibility\t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Useful links</font>\n",
    "\n",
    "- <a href=\"https://medium.com/towards-artificial-intelligence/calculating-simple-linear-regression-and-linear-best-fit-an-in-depth-tutorial-with-math-and-python-804a0cb23660\">Calculating Simple Linear Regression and Linear Best Fit an In-depth Tutorial with Math and Python</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/tutorial/index.html\">scikit-learn Tutorials</a>\n",
    "- <a href=\"https://medium.com/@amitg0161/sklearn-linear-regression-tutorial-with-boston-house-dataset-cde74afd460a\">Sklearn Linear Regression Tutorial with Boston House Dataset</a>\n",
    "- <a href=\"https://www.dataquest.io/blog/sci-kit-learn-tutorial/\">Scikit-learn Tutorial: Machine Learning in Python</a>\n",
    "- <a href=\"https://debuggercafe.com/image-classification-with-mnist-dataset/\">Image Classification with MNIST Dataset</a>\n",
    "- <a href=\"https://davidburn.github.io/notebooks/mnist-numbers/MNIST%20Handwrititten%20numbers/\">MNIST handwritten number identification</a>\n",
    "- [K-Fold Cross-Validation in Python Using SKLearn](https://www.askpython.com/python/examples/k-fold-cross-validation)\n",
    "- [A Quick Machine Learning Modelling Tutorial with Python and Scikit-Learn](https://dev.mrdbourke.com/zero-to-mastery-ml/introduction-to-scikit-learn/) by Daniel Bourke.\n",
    "- [scikit-learn Tutorials](https://labex.io/tutorials/category/sklearn) from LabEx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
