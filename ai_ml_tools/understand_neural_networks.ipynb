{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Overview of Artificial Neural Networks</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Objective](#sec_obj)\n",
    "* [What are Neural Networks?](#sec_nn_what)\n",
    "* [Layers in Neural Networks](#sec_nn_layers)\n",
    "* [How Do Neural Network Works?](#sec_nn_how)\n",
    "* [Things to Consider](#sec_nn_things)\n",
    "* [Where are Neural Network Used?](#sec_nn_where)\n",
    "* [Limitations of  Neural Network](#sec_nn_limit)\n",
    "* [Useful documents](#sec_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Objective</font> <a class=\"anchor\" id=\"sec_obj\"></a>\n",
    "\n",
    "The objective of this presentation is to provide an overview of the fundamental concepts of neural networks and show how neural networks work. \n",
    "It will facilitate the understanding of the processes involved in creating machine learning models with TensorFlow and PyTorch.\n",
    "\n",
    "\n",
    "__Target audience:__\n",
    "\n",
    "This document is meant for people who:\n",
    "\n",
    "- Practice Data Science and want to start building their own AI/ML models.\n",
    "- Are involved in management and want to understand the principle of ML to lead their team.\n",
    "- Are decision makers and want to make sense of the data generated by ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> What are Neural Networks? </font> <a class=\"anchor\" id=\"sec_nn_what\"></a>\n",
    "\n",
    "> A neural network is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning (ML) process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain. It creates an adaptive system that computers use to learn from their mistakes and improve continuously.\n",
    "\n",
    "- Biological neural networks have interconnected `neurons` with `dendrites` that receive inputs, then based on these inputs they produce an output signal through an `axon` to another `neuron`.\n",
    "- Artificial Neural Networks (ANNs, also called Simulated Neural Networks (SNNs)) are computing systems with interconnected nodes that work much like neurons in the human brain.\n",
    "   - Their main goal is to map an input into a desire output.\n",
    "   - They are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. \n",
    "   - The task of every neuron (also known as a node, unit, or perceptron) is to process the information received and then transmit it to the neurons in the next layer.\n",
    "   - If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "- __The nodes in ANNs perform mathematical operations on input data, learning some underlying patterns in the data, before producing some output based on those patterns.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Layers in Neural Networks</font> <a class=\"anchor\" id=\"sec_nn_layers\"></a>\n",
    "\n",
    "ANNs can be viewed as weighted directed graphs (interconnected artificial neurons), that are commonly organized in three main layers:\n",
    "\n",
    "- **Input layer:** \n",
    "   - This is where the network starts. \n",
    "   - The number of units in this layer is fixed and corresponds exactly to the number of input features.\n",
    "- **Hidden layers:** \n",
    "   - The number of hidden layer and units per layer are the free parameters that the user has to fix. \n",
    "   - There is no rule to decide these two parameters but it depends strongly on the problem.\n",
    "- **Output layer:** \n",
    "   - This layer is where the network ends and the predictions are given. \n",
    "   - In the case of a classification problem, the number of units in the output layer corresponds exactly to the number of classes.\n",
    "\n",
    "<font color=\"blue\">There is not any precise rule, in general the number of hidden layers depends strongly on the problem, at odds to the input and output layers</font>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](https://www.xenonstack.com/hubfs/xenonstack-artificial-neural-network-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>How Do Neural Network Works?</font> <a class=\"anchor\" id=\"sec_nn_how\"></a>\n",
    "\n",
    "- The three layers (input, hidden, output) form the foundation for all new networks regardless of complexity.\n",
    "   - The number hidden layers in a neural network varies depending on the complexity of a problem it needs to solve\n",
    "- Once the architecture of a neural network has been set, we can proceed to the training of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Processes</font>\n",
    "- We initially assign random values to the weights and thresholds (for activation functions).\n",
    "- __Perform foward propagation__:\n",
    "   - Training data are fed to the input layer.\n",
    "   - Data passes through the succeeding layers, getting multiplied and added together in complex ways, until it finally arrives, radically transformed, at the output layer.\n",
    "      - Every neuron takes the sum of its inputs and then applies an activation layer to produce an output that gets processed to the next layer.\n",
    "      - Weighted connections represent the strength of the links between neurons.\n",
    "   - As data flows through the hidden layers, each progressively extracts key features until it reaches the output layer.\n",
    "      - The final output layer provides a prediction based on the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_neuron](https://www.quantamagazine.org/wp-content/uploads/2021/02/Simulating-a-Neuron.svg)\n",
    "Image Source: [Samuel Velasco/Quanta Magazine](https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Perform backward propagation__: Process of computing the gradients (from the output layer back to the input layer) to update the training weights.\n",
    "   - The predicted output is compared to the actual target value, and the difference (error) is calculated using a loss function.\n",
    "   - Calculate the gradient (rate of change) of the loss with respect to each parameter (weight and bias).\n",
    "   - The parameters are adjusted based on the calculated gradients and a learning rate (used to scale the gradients to update the weights), using an optimization algorithm like gradient descent to minimize the loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Iteration__\n",
    "   - This process of forward propagation, error calculation, and parameter adjustment is repeated iteratively over multiple training samples until the network converges to a satisfactory level of accuracy.\n",
    "      - During training, the weights and thresholds are continually adjusted until training data with the same labels consistently yield similar outputs.\n",
    "      - At each iteration, the network's weights are updated in a manner that improves the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_network](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ZXAOUqmlyECgfVa81Sr6Ew.png)\n",
    "Image Source: Rukshan Pramoditha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Things to consider</font> <a class=\"anchor\" id=\"sec_nn_things\"></a>\n",
    "\n",
    "When designing a neural network, it is important to have the following in mind:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(1) Input data</font>\n",
    "- Should be representive of the available dataset and should be selected to avoid overfitting.\n",
    "- When the input data from the training set is fed to the model, two parameters (tunable hyperparameters that can directly affect how well a model trains) need to be considered:\n",
    "   - __Epoch__: One complete pass of the entire training dataset through the learning algorithm, i.e., when all the training data has been exposed to the entire neural network.\n",
    "   - __Batch size__: Determines the portion of our training dataset that can be fed to the model during each cycle. It controls the number of training samples to work through before the model’s internal parameters are updated.\n",
    "- Too few epochs can result in an underfit model, whereas too many epochs can lead to overfitting.\n",
    "- An __iteration__ is the number of batches needed to complete one epoch.\n",
    "\n",
    "\n",
    "![fig_inp](https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png)\n",
    "Image Source: [baeldung.com](https://www.baeldung.com/cs/epoch-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(2) Number of neurons and number of hidden layers</font>\n",
    "\n",
    "The architecture of a ANN is completely specified by six parameters: the six cells in the interior grid in the table below: \n",
    "\n",
    "| Layer Type | Number of Layer Type | Number of Nodes per Layer |\n",
    "| --- | --- | --- |\n",
    "| _input layer_ | FIXED | FIXED |\n",
    "| _hidden layer_ | ?  | ? |\n",
    "| _output layer_ | FIXED | FIXED |\n",
    "\n",
    "Two of those (number of layer type for the input and output layers) are always one and one since ANNs have a single input layer and a single output layer. That leaves just two parameters that need to be set: the number of hidden layers and the number of nodes comprising each of those layers.\n",
    "\n",
    "- Proper numbers:\n",
    "   - too few (can lead to underfitting): the process becomes general. \n",
    "   - too many (can lead to overfitting): the network may not perform well on data that are not part of the training set.\n",
    "- To determine the correct number of neurons to use in the hidden layers, consider as starting point, the three rules:\n",
    "   - The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "   - The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "   - The number of hidden neurons should be less than twice the size of the input layer.\n",
    "- For most problems, one could start with the following configuration (and gradually uncrease until desired results):\n",
    "   1. The number of hidden layers equals one; and \n",
    "   2. The number of neurons in that layer is the mean of the neurons in the input and output layers.\n",
    "- Increasing the number of neurons and layers significantly increases the computational cost of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(3) Activation function</font>\n",
    "- Determines the active state of the neuron. \n",
    "   - It decides whether the information received by the neuron is relevant or not.\n",
    "- It transforms the input signal into the non-linear form and sent to the next layer as an input\n",
    "  - Two sets of learnable parameters are factored in the input signal:\n",
    "     - __Weights__: Control the strength of the connection between two neurons in two consecutive layers.\n",
    "     - __Biases__: Values which help determine whether or not the activation output from a neuron is going to be passed forward through the network.\n",
    "- __Start with the simplest and the use more complex ones if they improve the performance.__\n",
    "   - __Rectified linear unit__ (`ReLU`): Map any negative input to 0 and leaves any positive input unchanged.\n",
    "   - __Sigmoid__: Map any input to a value between 0 and 1.\n",
    "   - __Hyperbolic tangent__ (`tanh`): Map inputs to a value between -1 and 1.\n",
    "   - __Softmax__: Convert a vector of inputs to a vector whose elements range from 0 and 1 and collectively sum to 1. Take an output and turn it into a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(4) Optimizer</font>\n",
    "A mathematical algorithm that helps our loss function reach its convergence point with minimum delay.\n",
    "-  The difference in predicted value and the real outcome is known as error. This error is used to update network weights and internal model parameters.\n",
    "- Optimization is a process which tried to reduce the network error. \n",
    "- Examples include: Stochastic Gradient Descent, Nesterov accelerated gradient, Adagrad, RMSProp, AdaDelta, Adam, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_opt](https://miro.medium.com/v2/resize:fit:1156/format:webp/1*65Mxg_Yfq-L7AvaS0K5aGA.png)\n",
    "Image Source: Sanket Doshi/[towardsdatascience.com](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to consider the __learning rate__ ($\\eta$):\n",
    "\n",
    "$$ W_{new} = W_{old} - \\eta \\times \\frac{\\partial J(W)}{\\partial W}$$\n",
    "\n",
    "where $J(W)$ is the loss as function of the weights ($W$).\n",
    "\n",
    "- Regulates how much of the network's weights are updated every iteration of the optimization method. It determines the step size (a positive value less than 1.0) at which the optimizer makes updates to the weights of the network during training. \n",
    "- It helps ensure that a model learns enough from training to make meaningful adjustments to its parameters while also not overcorrecting.\n",
    "- Which value?\n",
    "   - Low: always step in the right direction, but the optimizer may take too long to converge or get stuck in a plateau or undesirable local minima.\n",
    "   - High: computationally efficient, but the algorithm can oscillate around or even jump over the minima.\n",
    "\n",
    "![fig_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
    "Image Source: Jeremy Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(5) Loss function</font> \n",
    "\n",
    "- A loss function is used to evaluate model performance by calculating the deviation of a model’s predictions from the correct, “ground truth” predictions. \n",
    "- Optimizing a ML model entails adjusting model parameters to minimize the the value of the loss function.\n",
    "   - If the model predictions are off, the loss function produces a \"large\"  number. \n",
    "   - If the model predictions are pretty good, the loss function outputs a \"small\" number. \n",
    "   - As the model parameters are modified, the loss function will tell how well the model is doing.\n",
    "- Examples of loss function:\n",
    "   - __Mean Squared Error (MSE)__: Calculate the average of the squared difference between predicted and target values. Used with regression tasks.\n",
    "   - __Mean Absolute Error (MAE)__: Calculate the average absolute differences between predicted and target values. Used with regression tasks.\n",
    "   - __Cross-Entropy__: Measure the difference between predicted class probabilities and true class labels. Used for classification tasks.\n",
    "   - __Binary Cross-Entropy (Log) Loss__: Measures the differences between predicted probabilities and actual binary labels. It is often used for binary classification tasks requiring an output of zero or one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(6) Dropout rate</font>\n",
    "\n",
    "- Dropout is a technique that involves randomly dropping, or eliminating, neurons from the network during training to prevent overfitting.\n",
    "- During training, dropout randomly “drops out” (i.e., temporarily deactivates) a fraction (dropout rate) of neurons in a layer during each forward and backward pass.\n",
    "   - This randomness forces the network to avoid relying too heavily on specific neurons, encouraging it to learn more robust and generalized features.\n",
    "   - We use it to make sure that the model does not become over relying on any given node so that we can generalize the training.\n",
    "   - We ensure that our model does not just memorize the training data but truly understand the underlying patterns.\n",
    "- The dropout rate typically ranges between 0.2 and 0.5, depending on the network architecture and the dataset.\n",
    "   - Higher dropout rates (e.g., 0.5) aggressively reduce overfitting but may slow training or cause underfitting if the network lacks capacity.\n",
    "   - Lower rates (e.g., 0.2) provide milder regularization.\n",
    "- Dropout is only active during training; during inference (testing), all neurons remain active.\n",
    "- A key advantage of dropout is its simplicity—it requires minimal code changes and computational overhead compared to techniques like data augmentation.\n",
    "-  Dropout is most effective in large networks with many parameters, where overfitting is likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_doupout](https://i.redd.it/gtz7f6j8rgm91.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">__You need to keep track of your results with different network designs to see which configurations work better for your problem domain and stop training when the model starts to overfit on the training data.__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Types of Neural Networks</font> <a class=\"anchor\" id=\"sec_nn_types\"></a>\n",
    "\n",
    "There are many types of neural networks that are classified depending on their:\n",
    "\n",
    "- Structure\n",
    "- Data flow\n",
    "- Neurons used and their density\n",
    "- Layers and their depth activation filters\n",
    "\n",
    "Here are some the most common types of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed-Forward Neural Networks (FNNs)\n",
    "\n",
    "- Convey information in one direction through input nodes; this information continues to be processed in this single direction until it reaches the output mode. There are no cycles/loops in the flow of data.\n",
    "- FNNs are good at solving problems with a clear relationship between the input and the output, epecially for regression and classification tasks requiring sequential data processing.\n",
    "- FNNs are the foundation for facial recognition, natural language processing, computer vision, and other neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- Take the output of a processing node and transmit the information back into the network. Each node stores historical processes, and these historical processes are reused in the future during processing.\n",
    "- In this model, each node behaves like a memory cell. These cells work to ensure intelligent computation and implementation by processing the data they receive.\n",
    "- RNNs are used for tasks involving data in a sequence, like words in a sentence. They are commonly used in text-to-speech applications and for sales forecasting and stock market predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "- Have several layers in which data is sorted into categories. These networks have an input layer, an output layer, and a hidden multitude of convolutional layers in between. The layers create feature maps that record areas of an image that are broken down further until they generate valuable outputs.\n",
    "- This model is designed for processing structured grid data, such as images. It uses principles from linear algebra, especially matrix multiplication, to detect and process patterns within images. \n",
    "- CNNs are beneficial for AI-powered image recognition, segmentation, and object detection applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deconvolutional Neural Networks (DNNs)\n",
    "\n",
    "- Work in reverse of convolutional neural networks. The application of the network is to detect items that might have been recognized as important under a convolutional neural network. These items would likely have been discarded during the convolutional neural network execution process. \n",
    "- DNNs are helpful for various applications, including image analysis and synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modular Neural Networks (MNNs)\n",
    "\n",
    "- Contain several networks that work independently from one another. \n",
    "- These networks do not interact with each other during an analysis process. Instead, these processes are done to allow complex, elaborate computing processes to be done more efficiently. \n",
    "- MNNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Adversarial Networks (GANs)\n",
    "\n",
    "- Operate on a unique principle of adversarial training, where two neural networks, the generator and the discriminator, engage in a competitive process to create realistic synthetic data.\n",
    "  - The generator creates realistic data, and the discriminator is responsible for distinguishing between real and synthetic data. The generator continually refines its output to fool the discriminator, while the discriminator improves its ability to differentiate between real and generated samples.\n",
    "- This model uses unsupervised learning to generate plausible conclusions from an original dataset.\n",
    "- GANs have been used to create realistic images and sounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Where are Neural Network Used?</font> <a class=\"anchor\" id=\"sec_nn_where\"></a>\n",
    "\n",
    "- Neural networks are ideally suited to help people solve complex problems in real-life situations.\n",
    "- One of the most popular uses of neural networks is building processes to locate and recognize patterns and relationships in data. \n",
    "- Neural networks: \n",
    "   - Can learn and model the relationships between inputs and outputs that are nonlinear and complex.\n",
    "   - Make generalizations and inferences.\n",
    "   - Reveal hidden relationships, patterns and predictions; and \n",
    "   - Model highly volatile data (such as time series data) and variances needed to predict rare events (such as fraud detection).\n",
    "\n",
    "\n",
    "### Few Applications\n",
    "\n",
    "- [New Deep Learning Method Adds 301 Planets to Kepler's Total Count](https://www.nasa.gov/feature/ames/new-deep-learning-method-adds-301-planets-to-keplers-total-count)\n",
    "- [Artificial Intelligence Helps Improve NASA’s Eyes on the Sun](https://www.nasa.gov/feature/goddard/2021/artificial-intelligence-helps-improve-nasa-s-eyes-on-the-sun)\n",
    "- [Multi-Objective Reinforcement Learning–based\n",
    "Deep Neural Networks for Cognitive Space\n",
    "Communications](https://ntrs.nasa.gov/api/citations/20170007958/downloads/20170007958.pdf)\n",
    "- [Neural networks meet space](https://www.symmetrymagazine.org/article/neural-networks-meet-space)\n",
    "\n",
    "- [AI in space exploration](https://www.aiacceleratorinstitute.com/ai-in-space-exploration/)\n",
    "- [Ten Ways to Apply Machine Learning in Earth and Space Sciences](https://eos.org/opinions/ten-ways-to-apply-machine-learning-in-earth-and-space-sciences)\n",
    "- [Artificial Intelligence and Machine Learning for Earth Science](https://ntrs.nasa.gov/api/citations/20210019474/downloads/2021-07-31_AIST-AI-ML_for_ES-Short_Revised.pdf)\n",
    "- [MACHINE LEARNING PROTOTYPING: ML and AI Research](https://impact.earthdata.nasa.gov/project/ml.html)\n",
    "- [Advancing AI for Earth Science: A Data Systems Perspective](https://eos.org/science-updates/advancing-ai-for-earth-science-a-data-systems-perspective)\n",
    "- [Carbon Nanotube Gas Sensor Using Neural Networks](https://www.nas.nasa.gov/hecc/assets/pdf/papers/HECC_Data_Science_CNT_Neural_Network.pdf)\n",
    "- [Predicting Composition of Photo Voltaic Cells Using Neural Networks](https://www.nas.nasa.gov/hecc/assets/pdf/papers/HECC_Data_Science_PV_Cells.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Limitations of  Neural Network</font> <a class=\"anchor\" id=\"sec_nn_limit\"></a>\n",
    "\n",
    "#### Need lots of data: \n",
    "- Unlike the human brain, which can learn to do things with very few examples, neural networks require a huge ampunt of data, at least thousands if not millions of labeled samples.\n",
    "- The efficiency of any neural network is directly proportional to the amount of data it receives to process. \n",
    "\n",
    "#### Not good for generalizing \n",
    "- A neural network will perform accurately at a task it has been trained for, but very poorly at anything else, even if it’s similar to the original problem.  \n",
    "- Unlike humans, neural networks don’t develop knowledge in terms of symbols (ears, eyes, whiskers, tail)—they process values. \n",
    "- They will not be able to learn about new objects in terms of high-level features and they need to be retrained from scratch.\n",
    "\n",
    "#### Network Structure\n",
    "- There is no specific rule for determining the structure of artificial neural networks.\n",
    "- The appropriate network structure is achieved through experience and trial and error.\n",
    "\n",
    "#### Computational Resources\n",
    "- Require significant computational power (parallel processing) to be trained.\n",
    "\n",
    "#### Seen as `Black Boxes`\n",
    "- Since neural networks express their behavior in terms of neuron weights and activations, it is very hard to determine the logic behind their decisions. \n",
    "- It is hard to find out if they’re making decisions based on the wrong factors. They do not provide a clue as to why and how a solution was obtained.\n",
    "   - Training an ANN results in internal weights and biases distributed throughtout the network making it difficult to understand why a solution is valid.\n",
    "- While neural networks can, in principle, perform accurate predictions, it’s unlikely that we’ll obtain insights on the structure of a dataset through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Useful Reference</font>\n",
    "\n",
    "- Kevin L. Priddy and Paul E. Keller, _Artificial Neural Network: And Introduction_, Spie Press, Bellingham, Washington USA, 2005.\n",
    "- Jared Wilber, _[Neural Networks](https://mlu-explain.github.io/neural-networks/)_, MLU-Explain, May 2023\n",
    "- _[Neural Networks: The basics of neural networks, and the math behind how they learn](https://www.3blue1brown.com/topics/neural-networks)_, 3blue1brown.com.\n",
    "- Jeff Heaton, _[The Number of Hidden Layers](https://www.heatonresearch.com/2017/06/01/hidden-layers.html)_, heatonresearch.com, 2017.\n",
    "- Chris V. Nicholson, _[A Beginner's Guide to Neural Networks and Deep Learning](https://wiki.pathmind.com/neural-network)_, wiki.pathmind.com.\n",
    "- _[Neural Networks: What they are & why they matter](https://www.sas.com/en_us/insights/analytics/neural-networks.html)_, sas.com.\n",
    "- _[Epoch in Neural Networks](https://www.baeldung.com/cs/epoch-neural-networks)_, Baeldung, March 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
