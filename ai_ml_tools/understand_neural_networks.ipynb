{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Overview of Artificial Neural Networks</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Objective](#sec_obj)\n",
    "* [What are Neural Networks?](#sec_nn_what)\n",
    "* [Layers in Neural Networks](#sec_nn_layers)\n",
    "* [How Do Neural Network Works?](#sec_nn_how)\n",
    "* [Things to Consider](#sec_nn_things)\n",
    "* [Where are Neural Network Used?](#sec_nn_where)\n",
    "* [Limitations of  Neural Network](#sec_nn_limit)\n",
    "* [Useful documents](#sec_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Objective</font> <a class=\"anchor\" id=\"sec_obj\"></a>\n",
    "\n",
    "The objective of this presentation is to provide an overview of the fundamental concepts of neural networks and show how neural networks work. \n",
    "It will facilitate the understanding of the processes involved in creating machine learning models with TensorFlow and PyTorch.\n",
    "\n",
    "\n",
    "__Target audience:__\n",
    "\n",
    "This document is meant for people who:\n",
    "\n",
    "- Practice Data Science and want to start building their own AI/ML models.\n",
    "- Are involved in management and want to understand the principle of ML to lead their team.\n",
    "- Are decision makers and want to make sense of the data generated by ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> What are Neural Networks? </font> <a class=\"anchor\" id=\"sec_nn_what\"></a>\n",
    "\n",
    "> A neural network is a method in artificial intelligence (AI) that teaches computers to process data in a way that is inspired by the human brain. It is a type of machine learning (ML) process, called deep learning, that uses interconnected nodes or neurons in a layered structure that resembles the human brain. It creates an adaptive system that computers use to learn from their mistakes and improve continuously.\n",
    "\n",
    "- Biological neural networks have interconnected `neurons` with `dendrites` that receive inputs, then based on these inputs they produce an output signal through an `axon` to another `neuron`.\n",
    "- Artificial Neural Networks (ANNs, also called Simulated Neural Networks (SNNs)) are computing systems with interconnected nodes that work much like neurons in the human brain.\n",
    "   - Their main goal is to map an input into a desire output.\n",
    "   - They are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. \n",
    "   - The task of every neuron (also known as a node, unit, or perceptron) is to process the information received and then transmit it to the neurons in the next layer.\n",
    "   - If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "- __The nodes in ANNs perform mathematical operations on input data, learning some underlying patterns in the data, before producing some output based on those patterns.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> Layers in Neural Networks</font> <a class=\"anchor\" id=\"sec_nn_layers\"></a>\n",
    "\n",
    "ANNs can be viewed as weighted directed graphs (interconnected artificial neurons), that are commonly organized in three main layers:\n",
    "\n",
    "- **Input layer:** \n",
    "   - This is where the network starts.\n",
    "   - The data that we feed to the model is loaded into the input layer.\n",
    "   - It is the only visible layer in the complete Neural Network architecture that passes the complete information from the outside world without any computation.\n",
    "   - The number of units (neurons) in this layer is fixed and corresponds exactly to the number of input features.\n",
    "- **Hidden layers:**\n",
    "   - They are intermediate layers that do all the computations and extract the features from the data.\n",
    "   - The number of hidden layer and units per layer are the free parameters that the user has to fix. \n",
    "   - There is no general rule to decide these two parameters but it depends strongly on the problem.\n",
    "   - Each hidden layer analyzes the output from the previous layer, processes it further, and passes it on to the next layer.\n",
    "- **Output layer:**\n",
    "   - The output layer is where the network ends. It takes input from preceding hidden layers and comes to a final prediction \n",
    "   - In the case of a classification problem for instance, the number of units in the output layer corresponds exactly to the number of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nn](https://www.xenonstack.com/hubfs/xenonstack-artificial-neural-network-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weights and biases__\n",
    "\n",
    "The neurons from one layer to another are connected to each other and each connection has an associated weight.\n",
    "- Weights help helps organize the variables by importance and impact of contribution.\n",
    "- They act like dials that control how strongly each input feature influences the decision.\n",
    "\n",
    "The network also includes biases that are built-in values that shift the decision threshold, allowing a neuron to activate even if the inputs themselves are weak. \n",
    "\n",
    "Together, these weights and biases determine how each neuron contributes to the overall computation. By adjusting these values during training, the network gradually learns to make accurate predictions.\n",
    "\n",
    "__Activation function__\n",
    "\n",
    "- The activation function, or transfer function, turns linear results into realistic or non-linear ones.\n",
    "- Each neuron applies the activation function to the weighted sum of its inputs to produce the output.\n",
    "- Activation functions help neurons make decisions and capture intricate relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_neuron](https://www.quantamagazine.org/wp-content/uploads/2021/02/Simulating-a-Neuron.svg)\n",
    "Image Source: [Samuel Velasco/Quanta Magazine](https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>How Do Neural Network Works?</font> <a class=\"anchor\" id=\"sec_nn_how\"></a>\n",
    "\n",
    "- The three layers (input, hidden, output) form the foundation for all new networks regardless of complexity.\n",
    "   - The number hidden layers in a neural network varies depending on the complexity of a problem it needs to solve\n",
    "- Once the architecture of a neural network has been set, we can proceed to the training of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Processes</font>\n",
    "- We initially assign random values to the weights and thresholds (for activation functions).\n",
    "- __Perform foward propagation__:\n",
    "   - Training data are fed to the input layer.\n",
    "   - Data passes through the succeeding layers, getting multiplied and added together in complex ways, until it finally arrives, radically transformed, at the output layer.\n",
    "      - Every neuron takes the sum of its inputs and then applies an activation layer to produce an output that gets processed to the next layer.\n",
    "      - Weighted connections represent the strength of the links between neurons.\n",
    "   - As data flows through the hidden layers, each progressively extracts key features until it reaches the output layer.\n",
    "      - The final output layer provides a prediction based on the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Perform backward propagation__: Process of computing the gradients (from the output layer back to the input layer) to update the training weights.\n",
    "   - The predicted output is compared to the actual target value, and the difference (error) is calculated using a loss function.\n",
    "   - Calculate the gradient (rate of change) of the loss with respect to each parameter (weight and bias).\n",
    "   - The parameters are adjusted based on the calculated gradients and a learning rate (used to scale the gradients to update the weights), using an optimization algorithm like gradient descent to minimize the loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Iteration__\n",
    "   - This process of forward propagation, error calculation, and parameter adjustment is repeated iteratively over multiple training samples until the network converges to a satisfactory level of accuracy.\n",
    "      - During training, the weights and thresholds are continually adjusted until training data with the same labels consistently yield similar outputs.\n",
    "      - At each iteration, the network's weights are updated in a manner that improves the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_network](https://miro.medium.com/v2/resize:fit:4800/format:webp/1*ZXAOUqmlyECgfVa81Sr6Ew.png)\n",
    "Image Source: Rukshan Pramoditha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Things to consider</font> <a class=\"anchor\" id=\"sec_nn_things\"></a>\n",
    "\n",
    "When designing a neural network, it is important to have the following in mind:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(1) Input data</font>\n",
    "- Should be representative of the available dataset and should be selected to avoid overfitting.\n",
    "- When the input data from the training set is fed to the model, two parameters (tunable hyperparameters that can directly affect how well a model trains) need to be considered:\n",
    "   - __Epoch__: One complete pass of the entire training dataset through the learning algorithm, i.e., when all the training data has been exposed to the entire neural network.\n",
    "   - __Batch size__: Determines the portion of our training dataset that can be fed to the model during each cycle.\n",
    "      - It controls the number of training samples to work through before the model’s internal parameters are updated.\n",
    "- Too few epochs can result in an underfit model, whereas too many epochs can lead to overfitting.\n",
    "- An __iteration__ is the number of batches needed to complete one epoch.\n",
    "\n",
    "\n",
    "![fig_inp](https://www.baeldung.com/wp-content/uploads/sites/4/2020/12/epoch-batch-size.png)\n",
    "Image Source: [baeldung.com](https://www.baeldung.com/cs/epoch-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(2) Number of neurons and number of hidden layers</font>\n",
    "\n",
    "The architecture of a ANN is completely specified by six parameters: the six cells in the interior grid in the table below: \n",
    "\n",
    "| Layer Type | Number of Layer Type | Number of Nodes per Layer |\n",
    "| --- | --- | --- |\n",
    "| _input layer_ | FIXED | FIXED |\n",
    "| _hidden layer_ | ?  | ? |\n",
    "| _output layer_ | FIXED | FIXED |\n",
    "\n",
    "Two of those (number of layer type for the input and output layers) are always one and one since ANNs have a single input layer and a single output layer. That leaves just two parameters that need to be set: the number of hidden layers and the number of nodes comprising each of those layers.\n",
    "\n",
    "- Proper numbers:\n",
    "   - too few (can lead to underfitting): the process becomes general. \n",
    "   - too many (can lead to overfitting): the network may not perform well on data that are not part of the training set.\n",
    "- To determine the correct number of neurons to use in the hidden layers, consider as starting point, the three rules:\n",
    "   - The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "   - The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "   - The number of hidden neurons should be less than twice the size of the input layer.\n",
    "- For most problems, one could start with the following configuration (and gradually uncrease until desired results):\n",
    "   1. The number of hidden layers equals one; and \n",
    "   2. The number of neurons in that layer is the mean of the neurons in the input and output layers.\n",
    "- Increasing the number of neurons and layers significantly increases the computational cost of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the number of trainable parameters in a neural network\n",
    "\n",
    "We assume here a a fully connected neural network.\n",
    "\n",
    "- For each layer, the number of trainable parameters can be computed by $(n + 1) \\times m$\n",
    "   - $n$ is the number of input neurons\n",
    "   - $m$ is the number of output neurons.\n",
    "   - The $+ 1$ term in the equation takes into account the bias terms.\n",
    "- If we have a network consisting of an input layers with 4 neurons, 3 hidden layers (with 16, 8, 4 neurons respectively) and an output layer with 2 neurons, then the network has 262 trainable parameters:\n",
    "\n",
    "$$ (4+1)\\times 16 + (16+1)\\times 8 + (8+1)\\times 4 + (4+1)\\times 2 = 262$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(3) Activation function</font>\n",
    "- Determines the active state of the neuron. \n",
    "   - It decides whether the information received by the neuron is relevant or not.\n",
    "- It transforms the input signal into the non-linear form and sent to the next layer as an input\n",
    "  - Two sets of learnable parameters are factored in the input signal:\n",
    "     - __Weights__: Control the strength of the connection between two neurons in two consecutive layers.\n",
    "     - __Biases__: Values which help determine whether or not the activation output from a neuron is going to be passed forward through the network.\n",
    "- __Start with the simplest and the use more complex ones if they improve the performance.__\n",
    "   - __Rectified linear unit__ (`ReLU`): Map any negative input to 0 and leaves any positive input unchanged.\n",
    "      - <font color=\"red\">The `ReLU` function is computationally inexpensive because it involves simple thresholding at zero. This allows networks to scale to many layers without a significant increase in computational burden, compared to more complex functions like `tanh` or `sigmoid`. </font>\n",
    "   - __Sigmoid__: Map any input to a value between 0 and 1.\n",
    "      - The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0.\n",
    "      - <font color=\"red\">The main use case of the `sigmoid` function is as the activation for the output layer of binary classification models. It squashes the output to a probability value between 0 and 1, which can be interpreted as the probability of the input belonging to a particular class. </font>\n",
    "   - __Hyperbolic tangent__ (`tanh`): Map inputs to a value between -1 and 1.\n",
    "      - The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\n",
    "      - <font color=\"red\">The `tanh` function is frequently used in the hidden layers of a neural network. Because of its zero-centered nature, when the data is also normalized to have mean zero, it can result in more efficient training.</font>\n",
    "   - __Softmax__: Convert a vector of inputs to a vector whose elements range from 0 and 1 and collectively sum to 1.\n",
    "      - Take an output and turn it into a probability distribution.\n",
    "      - <font color=\"red\">It is most commonly used as an activation function for the last layer of the neural network in the case of multi-class classification. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(4) Optimizer</font>\n",
    "A mathematical algorithm that helps our loss function reach its convergence point with minimum delay.\n",
    "-  The difference in predicted value and the real outcome is known as error. This error is used to update network weights and internal model parameters.\n",
    "- Optimization is a process which tried to reduce the network error.\n",
    "- The choice of the optimizer is critical in the process of building, testing, and deploying a ML model.\n",
    "   - Picking the wrong optimizer can have a substantial negative impact on the performance of a ML model.\n",
    "- Examples of optimizers include:\n",
    "   - Stochastic Gradient Descent (SGD): Estimates the direction of steepest descent based on a mini-batch and takes a step in this direction. Because the step size is fixed, SGD can quickly get stuck on plateaus or in local minima.\n",
    "   - SGD with momemtum: With momentum, SGD accelerates in directions of constant descent. This acceleration helps the model escape plateaus and makes it less susceptible to getting stuck in local minima.\n",
    "   - LARS: An extension of SGD with momentum which adapts a learning rate per layer.\n",
    "   - Adaptive Gradient Algorithm (AdaGrad): Dynamically scales the learning rate for each parameter based on the square root of the inverse of the sum of the squared gradients. It can converge faster in scenarios with sparse features.\n",
    "      - <font color=\"red\"> Used on sparse datasets like text and images where learning rate needs to adapt to feature frequency.</font>\n",
    "   - Root Mean Square Prop (RMSProp): Works by keeping an exponentially weighted average of the squares of past gradients. RMSProp then divides the learning rate by this average to speed up convergence.\n",
    "      - <font color=\"red\"> Used on non-convex optimization problems and on situations where AdaGrad’s learning rate diminishes too fast.</font>\n",
    "   - AdaDelta: Belongs to the family of stochastic gradient descent algorithms, that provide adaptive techniques for hyperparameter tuning.\n",
    "      - <font color=\"red\"> Used in similar applications as with RMSProp, but when you want to avoid manual learning rate setting.</font>\n",
    "   - Adaptive Moment Estimation (Adam): Combines the best properties of AdaGrad and RMSprop to provide an optimization algorithm that can handle sparse gradients on noisy problems. It computes adaptive learning rates for each parameter.\n",
    "      - <font color=\"red\"> Used for most deep learning applications, due to its efficiency and effectiveness.</font>\n",
    "- __Rule of thumb__:\n",
    "   - If you have the resources to find a good learning rate schedule, SGD with momentum is a solid choice.\n",
    "   - If you are in need of quick results without extensive hypertuning, tend towards adaptive gradient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_opt](https://miro.medium.com/v2/resize:fit:1156/format:webp/1*65Mxg_Yfq-L7AvaS0K5aGA.png)\n",
    "Image Source: Sanket Doshi/[towardsdatascience.com](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to consider the __learning rate__ ($\\eta$):\n",
    "\n",
    "$$ W_{new} = W_{old} - \\eta \\times \\frac{\\partial J(W)}{\\partial W}$$\n",
    "\n",
    "where $J(W)$ is the loss as function of the weights ($W$).\n",
    "\n",
    "- Regulates how much of the network's weights are updated every iteration of the optimization method. It determines the step size (a positive value less than 1.0) at which the optimizer makes updates to the weights of the network during training. \n",
    "- It helps ensure that a model learns enough from training to make meaningful adjustments to its parameters while also not overcorrecting.\n",
    "- Which value?\n",
    "   - Low: always step in the right direction, but the optimizer may take too long to converge or get stuck in a plateau or undesirable local minima.\n",
    "   - High: computationally efficient, but the algorithm can oscillate around or even jump over the minima.\n",
    "\n",
    "![fig_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
    "Image Source: Jeremy Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(5) Loss function</font> \n",
    "\n",
    "- A loss function is used to evaluate model performance by calculating the deviation of a model’s predictions from the correct, “ground truth” predictions. \n",
    "- Optimizing a ML model entails adjusting model parameters to minimize the the value of the loss function.\n",
    "   - If the model predictions are off, the loss function produces a \"large\"  number. \n",
    "   - If the model predictions are pretty good, the loss function outputs a \"small\" number. \n",
    "   - As the model parameters are modified, the loss function will tell how well the model is doing.\n",
    "- Examples of loss function:\n",
    "   - __Mean Squared Error (MSE)__: Calculate the average of the squared difference between predicted and target values. <font color=\"red\">Used with regression tasks.</font>\n",
    "   - __Mean Absolute Error (MAE)__: Calculate the average absolute differences between predicted and target values. <font color=\"red\">Used with regression tasks.</font>\n",
    "   - __Cross-Entropy__: Measure the difference between predicted class probabilities and true class labels. <font color=\"red\">Used for classification tasks.</font>\n",
    "   - __Binary Cross-Entropy (Log) Loss__: Measures the differences between predicted probabilities and actual binary labels. <font color=\"red\">It is often used for binary classification tasks requiring an output of zero or one.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">(6) Dropout rate</font>\n",
    "\n",
    "- Dropout is a technique that involves randomly dropping, or eliminating, neurons from the network during training to prevent overfitting.\n",
    "- During training, dropout randomly “drops out” (i.e., temporarily deactivates) a fraction (dropout rate) of neurons in a layer during each forward and backward pass.\n",
    "   - This randomness forces the network to avoid relying too heavily on specific neurons, encouraging it to learn more robust and generalized features.\n",
    "   - We use it to make sure that the model does not become over relying on any given node so that we can generalize the training.\n",
    "   - We ensure that our model does not just memorize the training data but truly understand the underlying patterns.\n",
    "- The dropout rate typically ranges between 0.2 and 0.5, depending on the network architecture and the dataset.\n",
    "   - Higher dropout rates (e.g., 0.5) aggressively reduce overfitting but may slow training or cause underfitting if the network lacks capacity.\n",
    "   - Lower rates (e.g., 0.2) provide milder regularization.\n",
    "- Dropout is only active during training; during inference (testing), all neurons remain active.\n",
    "- A key advantage of dropout is its simplicity—it requires minimal code changes and computational overhead compared to techniques like data augmentation.\n",
    "-  Dropout is most effective in large networks with many parameters, where overfitting is likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_doupout](https://i.redd.it/gtz7f6j8rgm91.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">__You need to keep track of your results with different network designs to see which configurations work better for your problem domain and stop training when the model starts to overfit on the training data.__</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Types of Neural Networks</font> <a class=\"anchor\" id=\"sec_nn_types\"></a>\n",
    "\n",
    "There are many types of neural networks that are classified depending on their:\n",
    "\n",
    "- Structure\n",
    "- Data flow\n",
    "- Neurons used and their density\n",
    "- Layers and their depth activation filters\n",
    "\n",
    "Here are some the most common types of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed-Forward Neural Networks (FNNs)\n",
    "\n",
    "- Convey information in one direction through input nodes; this information continues to be processed in this single direction until it reaches the output mode. There are no cycles/loops in the flow of data.\n",
    "- FNNs are good at solving problems with a clear relationship between the input and the output, epecially for regression and classification tasks requiring sequential data processing.\n",
    "\n",
    "![fig_FNNs](https://www.wikitechy.com/cdn/tutorial/deep-learning/application-of-feed-forward-neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- Take the output of a processing node and transmit the information back into the network. Each node stores historical processes, and these historical processes are reused in the future during processing.\n",
    "- In this model, each node behaves like a memory cell. These cells work to ensure intelligent computation and implementation by processing the data they receive.\n",
    "\n",
    "![fig_RNNs](https://www.wikitechy.com/cdn/tutorial/deep-learning/application-of-recurrent-neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks (CNNs)\n",
    "\n",
    "- Have several layers in which data is sorted into categories. These networks have an input layer, an output layer, and a hidden multitude of convolutional layers in between. The layers create feature maps that record areas of an image that are broken down further until they generate valuable outputs.\n",
    "- This model is designed for processing structured grid data, such as images. It uses principles from linear algebra, especially matrix multiplication, to detect and process patterns within images. \n",
    "\n",
    "![fig_CNNs](https://www.wikitechy.com/cdn/tutorial/deep-learning/application-of-convolutional-neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deconvolutional Neural Networks (DNNs)\n",
    "\n",
    "- Work in reverse of convolutional neural networks.\n",
    "- The application of the network is to detect items that might have been recognized as important under a convolutional neural network.\n",
    "   - These items would likely have been discarded during the convolutional neural network execution process.\n",
    "   - A signal may be lost due to having been convoluted with other signals. The deconvolution of signals can be used in both image synthesis and analysis.\n",
    "- DNNs are helpful for various applications, including:\n",
    "   - Reconstructing high-quality images from degraded or blurred versions.\n",
    "   - Enhancing the quality of satellite images.\n",
    "   - Restoring and enhancing digital images of paintings or cultural artifacts.\n",
    "   - Equalizing the effects of signal distortion during transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modular Neural Networks (MNNs)\n",
    "\n",
    "- Contain several networks that work independently from one another. \n",
    "- These networks do not interact with each other during an analysis process. Instead, these processes are done to allow complex, elaborate computing processes to be done more efficiently.\n",
    "    - The combination of multiple neural networks can lead to the development of a single neural network that is able to perform in a much more accurate and efficient manner than any of the previous networks had been able to on their own.\n",
    "\n",
    "![fig_MNNs](https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img,w_960,h_540/https://www.intellspot.com/wp-content/uploads/2024/03/Applications-of-Modular-Neural-Networks-MNNs-an-infographic.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generative Adversarial Networks (GANs)\n",
    "\n",
    "- Operate on a unique principle of adversarial training, where two neural networks, the generator and the discriminator, engage in a competitive process to create realistic synthetic data.\n",
    "  - The generator creates realistic data, and the discriminator is responsible for distinguishing between real and synthetic data. The generator continually refines its output to fool the discriminator, while the discriminator improves its ability to differentiate between real and generated samples.\n",
    "- This model uses unsupervised learning to generate plausible conclusions from an original dataset.\n",
    "- GANs have been used to <font color=\"red\">create realistic images and sounds.</font>\n",
    "\n",
    "![fig_GANs](https://www.intellspot.com/wp-content/uploads/2024/03/Applications-of-Generative-Adversarial-Networks-an-infographic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Where are Neural Network Used?</font> <a class=\"anchor\" id=\"sec_nn_where\"></a>\n",
    "\n",
    "- Neural networks are ideally suited to help people solve complex problems in real-life situations.\n",
    "- One of the most popular uses of neural networks is building processes to locate and recognize patterns and relationships in data. \n",
    "- Neural networks: \n",
    "   - Can learn and model the relationships between inputs and outputs that are nonlinear and complex.\n",
    "   - Make generalizations and inferences.\n",
    "   - Reveal hidden relationships, patterns and predictions; and \n",
    "   - Model highly volatile data (such as time series data) and variances needed to predict rare events (such as fraud detection).\n",
    "\n",
    "\n",
    "### Few Applications\n",
    "\n",
    "- [New Deep Learning Method Adds 301 Planets to Kepler's Total Count](https://www.nasa.gov/feature/ames/new-deep-learning-method-adds-301-planets-to-keplers-total-count)\n",
    "- [Artificial Intelligence Helps Improve NASA’s Eyes on the Sun](https://www.nasa.gov/feature/goddard/2021/artificial-intelligence-helps-improve-nasa-s-eyes-on-the-sun)\n",
    "- [Multi-Objective Reinforcement Learning–based\n",
    "Deep Neural Networks for Cognitive Space\n",
    "Communications](https://ntrs.nasa.gov/api/citations/20170007958/downloads/20170007958.pdf)\n",
    "- [Neural networks meet space](https://www.symmetrymagazine.org/article/neural-networks-meet-space)\n",
    "\n",
    "- [AI in space exploration](https://www.aiacceleratorinstitute.com/ai-in-space-exploration/)\n",
    "- [Ten Ways to Apply Machine Learning in Earth and Space Sciences](https://eos.org/opinions/ten-ways-to-apply-machine-learning-in-earth-and-space-sciences)\n",
    "- [Artificial Intelligence and Machine Learning for Earth Science](https://ntrs.nasa.gov/api/citations/20210019474/downloads/2021-07-31_AIST-AI-ML_for_ES-Short_Revised.pdf)\n",
    "- [MACHINE LEARNING PROTOTYPING: ML and AI Research](https://impact.earthdata.nasa.gov/project/ml.html)\n",
    "- [Advancing AI for Earth Science: A Data Systems Perspective](https://eos.org/science-updates/advancing-ai-for-earth-science-a-data-systems-perspective)\n",
    "- [Carbon Nanotube Gas Sensor Using Neural Networks](https://www.nas.nasa.gov/hecc/assets/pdf/papers/HECC_Data_Science_CNT_Neural_Network.pdf)\n",
    "- [Predicting Composition of Photo Voltaic Cells Using Neural Networks](https://www.nas.nasa.gov/hecc/assets/pdf/papers/HECC_Data_Science_PV_Cells.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Limitations of  neural networks</font> <a class=\"anchor\" id=\"sec_nn_limit\"></a>\n",
    "\n",
    "#### Need lots of data: \n",
    "- Unlike the human brain, which can learn to do things with very few examples, neural networks require a huge ampunt of data, at least thousands if not millions of labeled samples.\n",
    "- The efficiency of any neural network is directly proportional to the amount of data it receives to process. \n",
    "\n",
    "#### Not good for generalizing \n",
    "- A neural network will perform accurately at a task it has been trained for, but very poorly at anything else, even if it’s similar to the original problem.  \n",
    "- Unlike humans, neural networks don’t develop knowledge in terms of symbols (ears, eyes, whiskers, tail)—they process values. \n",
    "- They will not be able to learn about new objects in terms of high-level features and they need to be retrained from scratch.\n",
    "\n",
    "#### Network structure\n",
    "- There is no specific rule for determining the structure of artificial neural networks.\n",
    "- The appropriate network structure is achieved through experience and trial and error.\n",
    "\n",
    "#### Computational resources\n",
    "- Require significant computational power (parallel processing) to be trained.\n",
    "\n",
    "#### Seen as `Black Boxes`\n",
    "- Since neural networks express their behavior in terms of neuron weights and activations, it is very hard to determine the logic behind their decisions. \n",
    "- It is hard to find out if they’re making decisions based on the wrong factors. They do not provide a clue as to why and how a solution was obtained.\n",
    "   - Training an ANN results in internal weights and biases distributed throughtout the network making it difficult to understand why a solution is valid.\n",
    "- While neural networks can, in principle, perform accurate predictions, it’s unlikely that we’ll obtain insights on the structure of a dataset through them.\n",
    "\n",
    "#### Ethical considerations\n",
    "- Scientists and engineers, involved in the design of AI models,  are often unrealistically optimistic about the outcomes of their work, and the potential for harm is just as great.\n",
    "- It is important for them to be self-aware of the potential biases their works can lead to.\n",
    "  Creating fair systems and minimizing bias is critical. We need to scrutinize our training data and refine our models to prevent discrimination based on factors such as race, gender, and socioeconomic status.\n",
    "   - For instance, if we train a system to predict salary levels for individuals based\n",
    "on historical data, then this system will reproduce historical biases; for example, it will\n",
    "probably predict that women should be paid less than men.\n",
    "- AI is a powerful technology, if not used properly, can lead to conflicts between nations, even wars.\n",
    "- We do not know what effects the large-scale adoption of AI will have on society.\n",
    "   We have already seens a lot of disruptions.\n",
    "   As AI becomes more autonomous, the challenge is to maintai human control while ensuring that AI serves human values rather than undermining them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Useful Reference</font>\n",
    "\n",
    "- Kevin L. Priddy and Paul E. Keller, _Artificial Neural Network: And Introduction_, Spie Press, Bellingham, Washington USA, 2005.\n",
    "- Jared Wilber, _[Neural Networks](https://mlu-explain.github.io/neural-networks/)_, MLU-Explain, May 2023\n",
    "- _[Neural Networks: The basics of neural networks, and the math behind how they learn](https://www.3blue1brown.com/topics/neural-networks)_, 3blue1brown.com.\n",
    "- Jeff Heaton, _[The Number of Hidden Layers](https://www.heatonresearch.com/2017/06/01/hidden-layers.html)_, heatonresearch.com, 2017.\n",
    "- Chris V. Nicholson, _[A Beginner's Guide to Neural Networks and Deep Learning](https://wiki.pathmind.com/neural-network)_, wiki.pathmind.com.\n",
    "- _[Neural Networks: What they are & why they matter](https://www.sas.com/en_us/insights/analytics/neural-networks.html)_, sas.com.\n",
    "- _[Epoch in Neural Networks](https://www.baeldung.com/cs/epoch-neural-networks)_, Baeldung, March 2024.\n",
    "- Ron Schmelzer and Kathleen Walch, [Top 10 Ethical Considerations for AI Projects](https://www.pmi.org/blog/top-10-ethical-considerations-for-ai-projects), Project Management Insititute, January 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
