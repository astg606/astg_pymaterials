{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center><h1><font color=\"red\" size=\"+3\">Data Profiling with Pandas</font></h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Objectives</font>\n",
    "In this presentation, we will cover the following topics:\n",
    "\n",
    "- Define the concept of data profiling\n",
    "- Explain what data profiling involves\n",
    "- Introduce a couple of data profling tools\n",
    "- Show how data profiling principles are used with real datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Useful References</font>\n",
    "\n",
    "- [What is data profiling?](https://www.ibm.com/think/topics/data-profiling) from IBM\n",
    "- R. A. Ruddle, J. Cheshire and S. J. Fernstad, [Tasks and Visualizations Used for Data Profiling: A Survey and Interview Study](https://ieeexplore.ieee.org/document/10008084), in IEEE Transactions on Visualization and Computer Graphics, vol. 30, no. 7, pp. 3400-3412, July 2024, doi: 10.1109/TVCG.2023.3234337."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">What is Data Profiling?</font>\n",
    "\n",
    ">Data profiling is the practice of looking closely at a dataset to understand its overall structure and quality. This means reviewing things, like the types of data it contains, how the data is distributed, whether any information is missing, and if everything is consistent.\n",
    "\n",
    "- It involves reviewing, analyzing, and cleansing data to understand its structure, characteristics, integrity, and quality. \n",
    "- It helps you gain insights into your data, identify potential issues, and make informed decisions about how to use it.\n",
    "- The main purpose of data profiling is to make sure the data is correct, well-organized, and ready to be used for analysis or important decisions.\n",
    "   - It helps choosing the right algorithms by gaining an initial high-level understanding of the dataset.\n",
    "- It is a crucial preliminary step before using the data in a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Purpose of data profiling</font>\n",
    "\n",
    "The purpose of data profiling is to:\n",
    "\n",
    "- Identify and correct issues with the quality of the data\n",
    "- Explore and understand how the data was generated (i.e., from which source)\n",
    "- Identify missing values in the data\n",
    "- Identify duplicate records in the dataset\n",
    "- Identify how frequently each attribute occurs\n",
    "- Identify how unique the values on each attribute\n",
    "- Identify outliers in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data profiling serves as a data hygiene process resulting in a collection of information about data, also known as metadata and its overall health. This could include:\n",
    "\n",
    "- Data types: Are the values in a column numbers, text, dates, etc.?\n",
    "- Value ranges: What are the minimum and maximum values a field can hold?\n",
    "- Missing values: How many data points are missing in a specific column?\n",
    "- Data distributions: How are the values distributed across a column?\n",
    "- Data relationships: Are there any connections between different data points or columns?\n",
    "- Data quality issues: Are there any inconsistencies, anomalies, duplicates, or errors present?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Common techniques for data profiling</font>\n",
    "\n",
    "Data profiling relies on a set of activities, including discovery and analytical techniques to collect statistics or informative summaries about the data. \n",
    "\n",
    "The most common methods used are:\n",
    "\n",
    "- __Column profiling__: Analyze individual columns to understand their data type, distribution, unique values, and missing values.\n",
    "- __Data type profiling__: Identify the data types of each column and checking for inconsistencies.\n",
    "- __Pattern profiling__: Identifyi patterns and anomalies in the data.\n",
    "- __Relationship profiling__: Examine the relationships between different columns and tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Information generated by data profiling</font>\n",
    "\n",
    "The analysis performed by data profiling tools exposes:\n",
    "\n",
    "- data rule (for instance, identifies dependencies that represent business rules embedded within the data).\n",
    "- anomalies that exist within the data sets.\n",
    "\n",
    "Data profiling tools can generate information about: \n",
    "\n",
    "- Data patterns\n",
    "- Numeric statistics\n",
    "- Data domains\n",
    "- Dependencies\n",
    "- Relationships, and\n",
    "- Anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the column level, reports focus on statistical measures and metadata to provide insights into the distribution and quality of the data. These reports include information on:\n",
    "\n",
    "- Minimum and maximum values: indicate the range of the data. Limiting a value set range also helps to determine outliers. \n",
    "- Mean and mode: reveal the average and most common values.\n",
    "- Percentiles: show the data distribution across intervals.\n",
    "- Standard deviation: indicates data variability.\n",
    "- Frequency: highlights the repetition of specific values.\n",
    "- Variation: shows data diversity and the aggregated sum of values within a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Benefits of data profiling</font>\n",
    "\n",
    "- Identify inaccuracies and inconsistencies, resulting in cleaner datasets and reduced errors.\n",
    "- Improve data credibility and quality.\n",
    "- Minimize the risk of data errors or inaccurate results.\n",
    "- Make better sense of the relationships between different data sets and sources\n",
    "- Improve users' understanding of data.\n",
    "- Can help quickly identify and address problems, often before they arise.\n",
    "\n",
    "__The result of data profiling is a constructive process of information inference to prepare a data set for later integration, analysis and modeling__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Some data profiling tools</font>\n",
    "\n",
    "We will rely on two profiling tools in this presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">[Data Profiler](https://github.com/capitalone/DataProfiler)</font>\n",
    "\n",
    "- Created by the financial company Capital One.\n",
    "- Designed to make data analysis, monitoring and sensitive data detection easy.\n",
    "- It is mainly meant to analyze datasets and detect if any of the information contained within is sensitive data, such as bank account numbers, credit card information, or social security numbers.\n",
    "- It can be used to generate basic statistical reports on science related datasets too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">[ydata-profiling](https://docs.profiling.ydata.ai/)</font>\n",
    "\n",
    "- Provides a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. \n",
    "- Automates and standardizes the generation of detailed reports, complete with statistics and visualizations.\n",
    "- Delivers an extended analysis of a DataFrame while allowing the data analysis to be exported in different formats such as __html__ and __json__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Packages Used</font>\n",
    "\n",
    "We will use the followin packages:\n",
    "\n",
    "- `Matplolib`: for visualization\n",
    "- `Seaborn`: for visualization \n",
    "- `Plotly`: for interactive visualization\n",
    "- `NumPy`: for array creation.\n",
    "- `Pandas`: for creating and manipulating Series and DataFrames, and for visualization.\n",
    "- `Skimpy`: for descriptive statistics\n",
    "- `DataProfiler`: data profiling tool mainly for financial data\n",
    "- `ydata-profiling`: data profiling tool\n",
    "- `great_table`: for dispalying tables.\n",
    "\n",
    "In addition, we will use the `datetime` module to manipulate dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import skimpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataprofiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from great_tables import GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using Numpy version:  {np.__version__}')\n",
    "print(f'Using Pandas version: {pd.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 5 rows of data will be displayed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print floating point numbers using fixed point notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are five preset Seaborn themes: `darkgrid` (default), `whitegrid`, `dark`, `white`, and `ticks`.\n",
    "- They are each suited to different applications and personal preferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The four preset contexts, in order of relative size, are `paper`, `notebook` (default), `talk`, and `poster`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove spine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.despine();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Perform data profiling in real applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Arctic Oscillation and North Atlantic Oscillation  Datasets</font>\n",
    "\n",
    "__[Arctic oscillation](https://en.wikipedia.org/wiki/Arctic_oscillation)__\n",
    "\n",
    "- The Arctic oscillation (AO) or Northern Annular Mode/Northern Hemisphere Annular Mode (NAM) is a weather phenomenon at the Arctic poles north of 20 degrees latitude.\n",
    "- It describes how pressure patterns are distributed over the Arctic region and the middle latitudes of the Northern Hemisphere.\n",
    "- It is an important mode of climate variability for the Northern Hemisphere.\n",
    "- A negative AO index causes the jet stream to weaken and dip into the mid-latitudes, allowing Arctic air to spill out and creating cold-air outbreaks.\n",
    "- A positive AO index causes the jet stream to move farther north than normal.\n",
    "\n",
    "\n",
    "__[North Atlantic Oscillation](https://en.wikipedia.org/wiki/North_Atlantic_oscillation)__\n",
    "\n",
    "- The North Atlantic Oscillation (NAO) is a weather phenomenon in the North Atlantic Ocean of fluctuations in the difference of atmospheric pressure at sea level (SLP) between the Icelandic Low and the Azores High.\n",
    "- The NAO determines the speed and direction of the westerly winds across the North Atlantic, as well as winter sea surface temperature.\n",
    "   - A negative NAO leads to a weaker westerly flow into Western Europe; the more negative, the more this is true.\n",
    "   - When the NAO index is positive, enhanced westerly flow across the North Atlantic during winter moves relatively warm (and moist) maritime air over much of Europe and far downstream across Asia, while stronger northerlies over Greenland and northeastern Canada carry cold air southward and decrease land temperatures and SST over the northwest Atlantic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read the North Atlantic Oscillation (NAO) data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nao_url = \"http://www.cpc.ncep.noaa.gov/products/precip/CWlink/pna/norm.nao.monthly.b5001.current.ascii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nao_df = pd.read_table(nao_url, sep='\\s+', \n",
    "                       parse_dates={'dates':[0, 1]}, \n",
    "                       header=None)\n",
    "nao_df.columns = [\"dates\", \"NAO\"]\n",
    "nao_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read the Atlantic Oscillation (AO) data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_url = \"http://www.cpc.ncep.noaa.gov/products/precip/CWlink/daily_ao_index/monthly.ao.index.b50.current.ascii\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_df = pd.read_table(ao_url, sep='\\s+', \n",
    "                      parse_dates={'dates':[0, 1]}, \n",
    "                      header=None)\n",
    "ao_df.columns = [\"dates\", \"AO\"]\n",
    "ao_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create a Pandas DataFrame by combining the two Pandas objects__\n",
    "\n",
    "The `ao_df` and `nao_df` have the same number of rows and they have in common the column `dates`. We can use the `merge()` function to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_df = ao_df.merge(nao_df, how='outer')\n",
    "aonao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aonao_df = pd.concat([ao_df, nao_df[\"NAO\"]], axis=1)\n",
    "#aonao_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can use `great_tables` to better display the content of the DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT(aonao_df).tab_header(\n",
    "    title=\"Combined AO and NAO datasets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Obtain basic information on the columns</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quick observations__\n",
    "\n",
    "- There appears to be no missing values.\n",
    "- There is a consistency in the data type of each column.\n",
    "- We are dealing with time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Descriptive statistics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skimpy.skim(aonao_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Basic plots</font>\n",
    "\n",
    "__Time series plots ofAO and NAO__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "sns.lineplot(data=aonao_df, x=\"dates\", y=\"AO\", ax=ax)\n",
    "sns.lineplot(data=aonao_df, x=\"dates\", y=\"NAO\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(aonao_df, x=\"dates\", y=[\"AO\", \"NAO\"],\n",
    "              hover_data={\"dates\": \"|%b %Y\"},\n",
    "              title='AO and NAO')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Time series plot NAO only__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "sns.lineplot(data=aonao_df[aonao_df[\"NAO\"]>=0], x=\"dates\", y=\"NAO\", ax=ax)\n",
    "sns.lineplot(data=aonao_df[aonao_df[\"NAO\"]<0], x=\"dates\", y=\"NAO\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(aonao_df, x='dates', y=\"NAO\", \n",
    "              color=aonao_df[\"NAO\"]>=0, \n",
    "              color_discrete_map={True: \"red\", False: \"blue\"},\n",
    "              range_y=[-4, 4])\n",
    "# Set white background\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Change grid color and axis colors\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='lightgrey')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='lightgrey')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Time series plot AO only__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "sns.lineplot(data=aonao_df[aonao_df[\"AO\"]>=0], x=\"dates\", y=\"AO\", ax=ax)\n",
    "sns.lineplot(data=aonao_df[aonao_df[\"AO\"]<0], x=\"dates\", y=\"AO\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(aonao_df, x='dates', y=\"AO\", \n",
    "              color=aonao_df[\"AO\"]>=0, \n",
    "              color_discrete_map={True: \"red\", False: \"blue\"},\n",
    "              range_y=[-4, 4])\n",
    "# Set white background\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Change grid color and axis colors\n",
    "fig.update_xaxes(showline=True, linewidth=2, linecolor='black', gridcolor='lightgrey')\n",
    "fig.update_yaxes(showline=True, linewidth=2, linecolor='black', gridcolor='lightgrey')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Histograms</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=aonao_df, x='NAO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=aonao_df, x='AO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aonao_df, x=\"NAO\", color=aonao_df[\"NAO\"]>0)\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(aonao_df, x=\"AO\", color=aonao_df[\"AO\"]>0)\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Finding outliers</font>\n",
    "\n",
    "We use a __boxplot__:\n",
    "\n",
    "- Pictorial representation of distribution of data which shows extreme values, median and quartiles.\n",
    "- Shows robust measures of location and spread as well as providing information about symmetry and outliers.\n",
    "   - The range of the data provides us with a measure of spread and is equal to a value between the smallest data point (min) and the largest one (Max).\n",
    "   - The interquartile range (`IQR`), which is the range covered by the middle 50% of the data.\n",
    "   - `IQR=Q3-Q1`, the difference between the third and first quartiles.\n",
    "      - The first quartile (`Q1`) is the value such that one quarter (25%) of the data points fall below it, or the median of the bottom half of the data.\n",
    "      - The third quartile (`Q3`) is the value such that three quarters (75%) of the data points fall below it, or the median of the top half of the data.\n",
    "   - The `IQR` can be used to detect outliers using the 1.5(IQR) criteria. Outliers are observations that fall below `Q1-1.5(IQR)` or above `Q3+1.5(IQR)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boxplots(mydf):\n",
    "    \"\"\"\n",
    "    Create a boxplot for each column of the DataFrame.\n",
    "    \"\"\"\n",
    "    # Get the column names\n",
    "    column_names = mydf.columns\n",
    "    fig, axes = plt.subplots(ncols=len(column_names), figsize=(14,5))\n",
    "    \n",
    "    # Create the boxplots with Seaborn\n",
    "    for name, axis in zip(column_names, axes):\n",
    "        sns.boxplot(data=mydf[name], ax=axis) \n",
    "        axis.set_xlabel(name, rotation=45)\n",
    "        axis.set(xticklabels=[], xticks=[], ylabel='')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplots(aonao_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dealing with outliers using IQR__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = aonao_df.quantile(0.25)\n",
    "Q3 = aonao_df.quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outlier_IQR = aonao_df[~((aonao_df < (Q1-1.5*IQR)) | (aonao_df > (Q3+1.5*IQR))).any(axis=1)]\n",
    "df_outlier_IQR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplots(df_outlier_IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">violinplot</font>\n",
    "\n",
    "- Draw a combination of boxplot and kernel density estimate.\n",
    "- It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared.\n",
    "- This can be an effective and attractive way to show multiple distributions of data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  ax = plt.subplots(1, 2, figsize=(8,6))\n",
    "\n",
    "for i, name in enumerate(['AO', 'NAO']):\n",
    "    sns.violinplot(aonao_df[name], ax=ax[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  ax = plt.subplots(1, 2, figsize=(8,6))\n",
    "\n",
    "for i, name in enumerate(['AO', 'NAO']):\n",
    "    sns.violinplot(df_outlier_IQR[name], ax=ax[i]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Heatmap</font>\n",
    "\n",
    "- Represent the individual values that are contained in a matrix as colors.\n",
    "- Create a correlation matrix that measures the linear relationships between the variables.\n",
    "- The pairs which are highly correlated represent the same variance of the dataset thus we can further analyze them to understand which attribute among the pairs are most significant for building the model.\n",
    "- A number on the map indicates a strong inverse relationship, no relationship, and a strong direct relationship, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = aonao_df[['AO', 'NAO']].corr()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(correlation_matrix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap by year/month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_df[\"Year\"] = aonao_df.dates.apply(lambda x: x.year)\n",
    "aonao_df[\"Month\"] = aonao_df.dates.apply(lambda x: x.strftime(\"%b\"))\n",
    "aonao_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nao_pt = aonao_df.pivot_table(index=\"Year\", columns=\"Month\", values=\"NAO\")\n",
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(nao_pt, \n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": 6},\n",
    "            linewidths=.5,\n",
    "            ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_pt = aonao_df.pivot_table(index=\"Year\", columns=\"Month\", values=\"AO\")\n",
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "sns.heatmap(ao_pt, \n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": 6},\n",
    "            linewidths=.5,\n",
    "            ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Using `Data Profiler`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_profile1 = dataprofiler.Profiler(aonao_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the report using json to prettify.\n",
    "aonao_report = aonao_profile1.report(report_options={\"output_format\":\"pretty\"})\n",
    "print(json.dumps(aonao_report, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a specified column, in this case it is labeled 0:\n",
    "print(json.dumps(aonao_report[\"data_stats\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a specified column, in this case it is labeled 1:\n",
    "print(json.dumps(aonao_report[\"data_stats\"][1], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Using `ydata-profiling`</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_profile2 = ProfileReport(aonao_df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aonao_profile2.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">AERONET Observations at Goddard</font>\n",
    "\n",
    "- [AERONET](https://aeronet.gsfc.nasa.gov/) (AErosol RObotic NETwork) is a globally distributed network of identical robotically controlled ground-based sun/sky scanning radiometers. \n",
    "- Each instrument measures the intensity of sun and sky light throughout daylight hours from the ultraviolet through the near-infrared. \n",
    "- The program provides a longterm, continuous, and accessible public domain database of aerosol optical, microphysical, and radiative properties for aerosol research including, aerosol characterization, validation of satellite retrievals and model predictions, and synergism with other databases.\n",
    "- Here are some Science benefits of AERONET:\n",
    "     - AERONET measurements are used to validate and advance algorithm development of satellite retrievals of aerosols.\n",
    "     - Aerosol transport models use aerosol data from AERONET to validate and improve model algorithms.\n",
    "     - Aerosol assimilation models as well as weather prediction models use real time AERONET data to improve predictions.\n",
    "     - Long-term commitment to AERONET sites worldwide provides assessment of the regional climatological impact of aerosols (e.g., aerosol amount, size, and heating or cooling effects).\n",
    "- Over 840 stations worldwide.\n",
    "- Here, we analyze the measurements (Aerosol Optical Depth (AOD)) at the [NASA GSFC](https://aeronet.gsfc.nasa.gov/new_web/photo_db_v3/GSFC.html) site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://portal.nccs.nasa.gov/datashare/astg/training/python/pandas/aeronet/\"\n",
    "\n",
    "filename = url+\"19930101_20210102_GSFC.lev20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparse = lambda x: datetime.datetime.strptime(x, '%d:%m:%Y %H:%M:%S')\n",
    "aeronet_df = pd.read_csv(filename, skiprows=6, na_values=-999,\n",
    "                 parse_dates={'datetime': [0, 1]}, \n",
    "                 #date_parser=dateparse, # date_parser=dateparse\n",
    "                 index_col=0) \n",
    "                 #squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basic information on each column__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = aeronet_df.nunique()\n",
    "print(unique_counts.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quick observations__\n",
    "\n",
    "- There are 6835 data points.\n",
    "- Many columns only have `NaN` and need to be deleted.\n",
    "- Many columns only have one value and need to be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Identify and delete columns with all `NaN` values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = aeronet_df.isnull().sum()\n",
    "print(var.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_columns = aeronet_df.columns[aeronet_df.isna().all()].tolist()\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df.dropna(axis=1, how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Identify and delete columns with only one unique value__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value_columns = [col for col in aeronet_df.columns if aeronet_df[col].nunique() == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "unique_value_columns = list()\n",
    "for col in aeronet_df.columns:\n",
    "    if aeronet_df[col].nunique() == 1:\n",
    "        unique_value_columns.append(col)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_value_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df.drop(unique_value_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can use `great_tables` to better display the content of the DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT(aeronet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quick observations__\n",
    "\n",
    "- We initially started with 79 columns but we now have 30.\n",
    "- We still have missing values in some of the columns.\n",
    "- More than half of the values of the column `AOD_1640nm` are missing values. We can remove the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df.drop(['AOD_1640nm'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skimpy.skim(aeronet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplots(aeronet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Scatterplot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(aeronet_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_corr = aeronet_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(aeronet_corr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = px.imshow(aeronet_corr, x=aeronet_df.columns, \n",
    "                 y=aeronet_df.columns, title=\"Correlation matrix\", width=900, height=900)\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quick observations__\n",
    "\n",
    "- Based on the scatterplot matrix and the heatmap, there appears to be three groups of fields:\n",
    "  1. Group 1 that has `AOD_1640nm` to `AOD_340nm`. The fields are strongly correlated to each other. They all have float as data type.\n",
    "  2. Group 2 that has `N[AOD_1640nm]` to `N[340-440_Angstrom_Exponent]`. The fields are strongly correlated to each other. They all have int as data type.\n",
    "  3. Group 3 with the remaining fields not including `Day_of_Year`, `Precipitable_Water(cm)`, and `AERONET_Instrument_Number`. The fields here have moderate to strong correlation with each other.\n",
    "- `Precipitable_Water(cm)` has a moderate positive correlation with fields in Group 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `Data Profiler`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_profile1 = dataprofiler.Profiler(aeronet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the report using json to prettify.\n",
    "aeronet_report = aeronet_profile1.report(report_options={\"output_format\":\"pretty\"})\n",
    "print(json.dumps(aeronet_report, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a specified column, in this case it is labeled 1:\n",
    "print(json.dumps(aeronet_report[\"data_stats\"][1], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `ydata-profiling`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_profile2 = ProfileReport(aeronet_df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeronet_profile2.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Analyzing CME activities</font>\n",
    "\n",
    "We read a file (in JSON format) that contains a collection of CME activities. For each recorded CME, we want to extract the following parameters:\n",
    "\n",
    "- `start_time`\n",
    "- `speed`\n",
    "- `longitude`\n",
    "- `latitude`\n",
    "- `halfAngle`\n",
    "- The list of instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the remote CME json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_filename = \"all_cmes.json\"\n",
    "cme_url = f\"https://raw.githubusercontent.com/barbarajthompson/TCMM_Maps/refs/heads/main/{cme_filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(cme_url, cme_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cme_filename, \"r\") as fid: \n",
    "     cme_activities = json.load(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the file content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cme_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cme_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample record with CME activity__\n",
    "\n",
    "- Each CME comes as a dictionary.\n",
    "- There is actual activity if the key `'activeRegionNum'` has a value (different than `None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_activities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample record without CME activity__\n",
    "\n",
    "- There was no CME activity because the value associated with the key `'activeRegionNum'` is `None`.\n",
    "- In our analysis, we exclude such a record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_activities[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write functions to read all records and create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CME_parameters(cme_activity: dict):\n",
    "    \"\"\"\n",
    "    From a dictionary containing CME activity data on a specific date/time,\n",
    "    extract the following parameters:\n",
    "\n",
    "    - starting date/time\n",
    "    - latitude\n",
    "    - longitude\n",
    "    - halfAngle\n",
    "    - speed\n",
    "    \"\"\"\n",
    "\n",
    "    latitude = cme_activity['cmeAnalyses'][0]['latitude']\n",
    "    longitude = cme_activity['cmeAnalyses'][0]['longitude']\n",
    "    half_angle = cme_activity['cmeAnalyses'][0]['halfAngle']\n",
    "    speed = cme_activity['cmeAnalyses'][0]['speed']\n",
    "    start_time = cme_activity['startTime']\n",
    "    list_instruments = list()\n",
    "    for item in cme_activity['instruments']:\n",
    "        list_instruments.append(item['displayName'])\n",
    "    return start_time, speed, latitude, longitude, half_angle, tuple(sorted(list_instruments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(cme_activities: list):\n",
    "    \"\"\"\n",
    "    Using a list of CME activities, create a Pandas DataFrame\n",
    "    with columns:\n",
    "    \n",
    "    - star_time\n",
    "    - latitude\n",
    "    - longitude\n",
    "    - half_angle\n",
    "    - speed\n",
    "    - instruments\n",
    "    \"\"\"\n",
    "    columns = [\"start_time\", \"speed\", \"latitude\", \"longitude\", \"half_angle\", \"instruments\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Loop over the CME event\n",
    "    for cme_activity in cme_activities:\n",
    "        # Only process when a CME activity was recorded.\n",
    "        if cme_activity['cmeAnalyses']:   # This mean that an activity was recorded\n",
    "            df.loc[len(df)] = get_CME_parameters(cme_activity)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a DataFrame of CME activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_df = create_df(cme_activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_df['start_time'] = pd.to_datetime(cme_df['start_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__We can use `great_tables` to better display the content of the DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT(cme_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cme_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skimpy.skim(cme_df[[\"speed\", \"latitude\", \"longitude\", \"half_angle\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(cme_df[[\"speed\", \"latitude\", \"longitude\", \"half_angle\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cme_df = cme_df[[\"speed\", \"latitude\", \"longitude\", \"half_angle\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxplots(new_cme_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = px.imshow(new_cme_df.corr(), x=new_cme_df.columns, \n",
    "                y=new_cme_df.columns, \n",
    "                title=\"Correlation matrix\", \n",
    "                width=500, height=500)\n",
    "mat.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
