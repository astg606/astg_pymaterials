{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"white\">.</font> | <font color=\"white\">.</font> | <font color=\"white\">.</font>\n",
    "-- | -- | --\n",
    "![NASA](http://www.nasa.gov/sites/all/themes/custom/nasatwo/images/nasa-logo.svg) | <h1><font size=\"+3\">ASTG Python Courses</font></h1> | ![NASA](https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png)\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1 style=\"color:red\">\n",
    "Accelerating (Numba) and Scaling (Dask) Python Codes on GPUs\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "\n",
    "- [An introduction to CUDA in Python (Part 1)](https://www.vincent-lunot.com/post/an-introduction-to-cuda-in-python-part-1/)\n",
    "- [Supported Python features in CUDA Python](http://numba.pydata.org/numba-doc/latest/cuda/cudapysupported.html)\n",
    "- <a href=\"https://nyu-cds.github.io/python-numba/05-cuda/\">Introduction to Numba: CUDA Programming</a>\n",
    "- <a href=\"https://people.duke.edu/~ccc14/sta-663/CUDAPython.html\">Massively parallel programming with GPUs</a>\n",
    "- <a href=\"https://www.kdnuggets.com/2019/07/accelerate-data-science-on-gpu.html\">Here’s how you can accelerate your Data Science on GPU</a>\n",
    "- [Numba for CUDA GPUs](http://numba.pydata.org/numba-doc/0.30.0/cuda/index.html)\n",
    "- <a href=\"https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb\">Introduction to GPU programming with Numba</a>\n",
    "- <a href=\"https://thedatafrog.com/en/articles/make-python-fast-numba/\">Make python fast with numba</a>\n",
    "- <a href=\"https://www.deeplearningwizard.com/deep_learning/production_pytorch/speed_optimization_basics_numba/\">Speed Optimization Basics: Numba</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>What will be Covered?</font>\n",
    "\n",
    "* Quick Overview of GPUs\n",
    "* Numba on GPUs\n",
    "   * Quick Overview of Numba\n",
    "   * Basic Use of GPUs with Numba Decorators\n",
    "   * Writing Cuda Kernel\n",
    "   * Examples\n",
    "* Dask-CuDF through Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Initial Setup</font>\n",
    "\n",
    "## Accessing the GPU on Google Colab\n",
    "\n",
    "In order to access GPUs for free:\n",
    "\n",
    "1. Go to the `Runtime` menu,\n",
    "2. Click on `Change runtime type`, and \n",
    "3. In the pop-up box, under `Hardware accelerator`, select `GPU` and click on `SAVE`.\n",
    "\n",
    "## Environment Sanity Check ##\n",
    "\n",
    "- <font color='red'>Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.</font>\n",
    "- Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that you were allocated the GPU compatible with RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "gpu_name = pynvml.nvmlDeviceGetName(handle).decode('UTF-8')\n",
    "\n",
    "if('K80' not in gpu_name):\n",
    "   print('***********************************************************************')\n",
    "   print('Woo! Your instance has the right kind of GPU, a '+ str(gpu_name)+'!')\n",
    "   print('***********************************************************************')\n",
    "   print()\n",
    "else:\n",
    "   raise Exception(\"\"\"\n",
    "                  Unfortunately Colab didn't give you a RAPIDS compatible GPU (P4, P100, T4, or V100), but gave you a \"\"\"+ gpu_name +\"\"\".\n",
    "                  Make sure you've configured Colab to request a GPU Instance Type.                \n",
    "                  If you get an incompatible GPU (i.e., a K80), use 'Runtime -> Factory Reset Runtimes...' to try again\"\"\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing CUDA Libraries\n",
    "\n",
    "Even though the CUDA libararies, they might not be accessible. We need to find out where they are and include them in our Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dev_lib_path = !find / -iname 'libdevice'\n",
    "nvvm_lib_path = !find / -iname 'libnvvm.so'\n",
    "\n",
    "assert len(dev_lib_path)>0, \"Device Lib Missing\"\n",
    "assert len(nvvm_lib_path)>0, \"NVVM Missing\"\n",
    "\n",
    "os.environ['NUMBAPRO_LIBDEVICE'] = dev_lib_path[0]\n",
    "os.environ['NUMBAPRO_NVVM'] = nvvm_lib_path[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> GPUs</font>\n",
    "\n",
    "![GPUs](http://www.nvidia.com/docs/IO/143716/how-gpu-acceleration-works.png)\n",
    "Image Source: NVIDIA\n",
    "\n",
    "- Graphics Processing Units (GPUs) are custom designed to be very efficient at handling computer graphics and image processing.\n",
    "- Central Processing Units (CPUs) handle computations serially, meaning the logic in handled in one stream: the next task will complete when the subsequent task has finished. CPUs can execute tasks in parallel across cores. For example, most computer CPUs tend to have either two, four or six cores.\n",
    "- In comparison, GPUs have hundreds of 'cores'. This massively parallel architecture is what gives the GPU its high compute performance.\n",
    "\n",
    "**Useful Terminology**\n",
    "\n",
    "| Term | Meaning |\n",
    "| ---  | --- |\n",
    "| `host` | the CPU |\n",
    "| `device` | the GPU |\n",
    "| `host memory` | the system main memory |\n",
    "| `device memory` | onboard memory on a GPU card |\n",
    "| `kernels` | a GPU function launched by the host and executed on the device |\n",
    "| `device function` | a GPU function executed on the device which can only be called from the device  |\n",
    "\n",
    "\n",
    "### Numba and GPUs\n",
    "\n",
    "- Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. \n",
    "- Kernels written in Numba appear to have direct access to NumPy arrays. \n",
    "- NumPy arrays are transferred between the CPU and the GPU automatically.\n",
    "\n",
    "### Dask and GPUs\n",
    "\n",
    "- Dask can help to scale out large array and dataframe computations by combining the Dask Array and DataFrame collections with a GPU-accelerated array or dataframe library.\n",
    "- The RAPIDS libraries provide a GPU accelerated Pandas-like library, `cuDF`, which interoperates well and is tested against Dask DataFrame.\n",
    "- You can convert a Pandas-backed Dask DataFrame to a cuDF-backed Dask DataFrame.\n",
    "\n",
    "![rapids](https://pbs.twimg.com/media/D2CeyaYVAAAe3kM.jpg)\n",
    "Image Source: NVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\"> CUDA Programming</font>\n",
    "\n",
    "- A GPU program comprises two parts:\n",
    "   - A host part that runs on the CPU, and\n",
    "   - One or more kernels that run on the GPU.\n",
    "- Typically, the CPU portion of the program is used to set up the parameters and data for the computation, while the kernel portion performs the actual computation.\n",
    "- In some cases the CPU portion may comprise a parallel program that performs message passing operations using MPI.\n",
    "- CUDA Python maps directly to the single-instruction multiple-thread execution (SIMT) model of CUDA. \n",
    "- Each instruction is implicitly executed by multiple threads in parallel. \n",
    "- Array expressions are less useful because we don’t want multiple threads to perform the same task. Instead, we want threads to perform a task in a cooperative fashion.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Numba</font>\n",
    "\n",
    "> Numba is an open-source JIT compiler that translates a subset of Python and NumPy into fast machine code using `LLVM` (low-level virtual machine), via the llvmlite Python package. It offers a range of options for parallelising Python code for CPUs and GPUs, often with only minor code changes. \n",
    ">\n",
    ">Wikipedia\n",
    "\n",
    "The diagram below, shows all the steps carried out by Numba to execute `do_math`. \n",
    "\n",
    "![fig_numba](https://miro.medium.com/max/1400/1*S0S4QUjR-BsdTICtT9797Q.png)\n",
    "Image Source: Continuum Analytics\n",
    "\n",
    "- **IR**: Intermediate Representations\n",
    "- **Bytecode Analysis**: Intermediate code more abstract than machine code\n",
    "- **LLVM**: Low Level Virtual Machine, infrastructure to develop compilers\n",
    "- **NVVM**: It is an IR compiler based on LLVM, it is designed to represent GPU kernels\n",
    "\n",
    "\n",
    "    \n",
    "**Usage**:\n",
    "\n",
    "- Numba provides several utilities for code generation.\n",
    "- Its central feature is the `numba.jit()` decorator. \n",
    "- Using this decorator, you can mark a function for optimization by Numba’s JIT compiler. - - - Various invocation modes trigger differing compilation options and behaviours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "from numba import guvectorize\n",
    "from numba import vectorize\n",
    "from numba import njit\n",
    "from numba import prange\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking your System**\n",
    "\n",
    "The `numba -s` or `numba --sysinfo` command prints a lot of information about your system and your Numba installation and relevant dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!numba -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Basic Usage of Numba on GPUs</font>\n",
    "\n",
    "###  <font color=\"green\">Example:</font> Average of Square Root of Entries of an Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def average_sqrt_1D(array1D):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of \n",
    "        an array using loops.\n",
    "    \"\"\"\n",
    "    avg = 0.\n",
    "    for x in array1D:\n",
    "        avg += math.sqrt(x)\n",
    "    return avg/len(array1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def average_sqrt_1D_numba(array1D):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of \n",
    "        an array using loops.\n",
    "    \"\"\"\n",
    "    avg = 0.\n",
    "    for x in array1D:\n",
    "        avg += math.sqrt(x)\n",
    "    return avg/len(array1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other ways of doing the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sqrt_1D_2(array1D):\n",
    "    return np.sum(np.sqrt(array1D))/array1D.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guvectorize(['(float64[:], float64[:])'], '(m)->()')\n",
    "def average_sqrt_1D_numba_2(array1D, avg):\n",
    "    tmp = 0.\n",
    "    for x in array1D:\n",
    "        tmp += math.sqrt(x)\n",
    "    avg = tmp/len(array1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First use of GPUs\n",
    "\n",
    "There are two ways to use Numba on GPUs:\n",
    "- The first way uses ufuncs or gufuncs (GPU universal functions). \n",
    "- In the second way, you define CUDA Python kernels. \n",
    "\n",
    "To use the ufuncs/gufuncs approach:\n",
    "- We need to use the Numba `guvectorize` decorator:\n",
    "    - The functions it decorates, work with an arbitrary number of elements of input arrays, and take and return arrays of differing dimensions. \n",
    "    - The function do not return their result value: they take it as an array argument, which must be filled in by the function. \n",
    "    - Requires a signature for declaring an input and output layouts, in symbolic form. For instance `(n),()->(n)` tells NumPy that the function takes a n-element one-dimension array, a scalar (symbolically denoted by the empty tuple `()`) and returns a n-element one-dimension array\n",
    "- We specify the `cuda` target as argument of the decorator. \n",
    "\n",
    "\n",
    "Numba will automatically:\n",
    "\n",
    "- Compile a Cuda kernel to execute the operations.\n",
    "- Allocate GPU memory for the input.\n",
    "- Execute the CUDA kernel with the correct kernel dimensions given the input sizes.\n",
    "- Copy the result back from the GPU to the CPU.\n",
    "- Return the result on the host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guvectorize(['(float64[:], float64[:])'], '(n)->()', target =\"cuda\")\n",
    "def average_sqrt_1D_cuda(array1D, avg):\n",
    "    \"\"\"\n",
    "        Average of the square root of all the entries of \n",
    "        an array using loops.\n",
    "    \"\"\"\n",
    "    tmp = 0.\n",
    "    for x in array1D:\n",
    "        tmp += math.sqrt(x)\n",
    "    avg[0] = tmp/len(array1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10000000\n",
    "array1D = np.random.rand(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = average_sqrt_1D(array1D)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = average_sqrt_1D_numba(array1D)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = average_sqrt_1D_cuda(array1D)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_reg = %timeit -o average_sqrt_1D(array1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_numba = %timeit -o average_sqrt_1D_numba(array1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cuda1 = %timeit -o average_sqrt_1D_cuda(array1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regular Speedup:     {}\".format(time_reg.best/time_reg.best))\n",
    "print(\"Numba   Speedup:     {}\".format(time_reg.best/time_numba.best))\n",
    "print(\"Cuda    Speedup:     {}\".format(time_reg.best/time_cuda1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Writing Cuda Kernels</font>\n",
    "\n",
    "- In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). \n",
    "- The CUDA programming model allows you to abstract the GPU hardware into a software model defined by a thread hierachy that is composed of a **grid** containing **blocks** of **threads**. \n",
    "- These **threads** are the smallest individual unit in the programming model, and they execute together in groups (traditionally called **warps**, consisting of 32 threads each). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"violet\">Kernel Declaration</font>\n",
    "\n",
    "A kernel function is a GPU function that is meant to be called from CPU code. It has two fundamental characteristics:\n",
    "\n",
    "- **Kernels cannot explicitly return a value**; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array).\n",
    "- **Kernels explicitly declare their thread hierarchy when called**: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n",
    "- To tell Python that a function is a CUDA kernel, simply add `@cuda.jit` before the definition. \n",
    "\n",
    "```python\n",
    "@cuda.jit\n",
    "def average_sqrt_1D_cuda_kernel(array1D, avg):\n",
    "    \"\"\"\n",
    "    Code for kernel.\n",
    "    \"\"\"\n",
    "    # code here\n",
    "```\n",
    "\n",
    "\n",
    "### <font color=\"violet\">Kernel Invocation</font>\n",
    "\n",
    "There are two main steps:\n",
    "\n",
    "- Instantiate the kernel properly, by specifying a **number of blocks per grid** and a **number of threads per block**. \n",
    "    - The product of the two will give the total number of threads launched. \n",
    "    - Kernel instantiation is done by taking the compiled kernel function (here `average_sqrt_1D_cuda_kernel`) and indexing it with a tuple of integers.\n",
    "- Running the kernel, by passing it the input array (and any separate output arrays if necessary). \n",
    "    - By default, running a kernel is synchronous: the function returns when the kernel has finished executing and the data is synchronized back.\n",
    "\n",
    "A kernel is typically launched in the following way:\n",
    "\n",
    "```python\n",
    "# Create the data array - usually initialized some other way\n",
    "array1D = numpy.ones(256)\n",
    "\n",
    "avg = 0.0\n",
    "\n",
    "# Set the number of threads in a block\n",
    "threads_per_block = 32 \n",
    "\n",
    "# Calculate the number of thread blocks in the grid\n",
    "blocks_per_grid = (array1D.size + (threads_per_block - 1)) // threads_per_block\n",
    "\n",
    "# Now start the kernel\n",
    "average_sqrt_1D_cuda_kernel[blocks_per_grid, threads_per_block](array1D, avg)\n",
    "\n",
    "# Print the result\n",
    "print(avg)\n",
    "```\n",
    "\n",
    "\n",
    "### <font color=\"violet\">Choosing the Block Size</font>\n",
    "\n",
    "The two-level thread hierarchy is important for the following reasons:\n",
    "\n",
    "- On the software side, the block size determines how many threads share a given area of shared memory.\n",
    "- On the hardware side, the block size must be large enough for full occupation of execution units; recommendations can be found in the [CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/).\n",
    "\n",
    "The block size you choose depends on a range of factors, including:\n",
    "\n",
    "- The size of the data array\n",
    "- The size of the shared memory per block (e.g. 64KB)\n",
    "- The maximum number of threads per block supported by the hardware (e.g. 512 or 1024)\n",
    "- The maximum number of threads per multiprocessor (MP) (e.g. 2048)\n",
    "- The maximum number of blocks per MP (e.g. 32)\n",
    "- The number of threads that can be executed concurrently (a “warp” i.e. 32).\n",
    "\n",
    "\n",
    "Determiming the best size for your grid of thread blocks is a complicated problem that often depends on the specific algorithm and hardware you're using. Here a few good rules of thumb:\n",
    "\n",
    "- The size of a block should be a multiple of 32 threads, with typical block sizes between 128 and 512 threads per block.\n",
    "- **The size of the grid should ensure the full GPU is utilized where possible**. Launching a grid where the number of blocks is 2x-4x the number of **streaming multiprocessors** on the GPU is a good starting place. (The Tesla K80 GPUs provided by Colaboratory have 15 SMs - more modern GPUs like the P100s on TigerGPU have 60+.)\n",
    "- The CUDA kernel launch overhead does depend on the number of blocks, so it may not be best to launch a grid where the number of threads equals the number of input elements when the input size is very big. \n",
    "\n",
    "\n",
    "### <font color=\"violet\">Thread Positioning</font>\n",
    "- When running a kernel, the kernel function’s code is executed by every thread once. \n",
    "- It therefore has to know which thread it is in, in order to know which array element(s) it is responsible for (complex algorithms may define more complex responsibilities, but the underlying principle is the same).\n",
    "- One way is for the thread to determines its position in the grid and block and manually compute the corresponding array position.\n",
    "- Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def average_sqrt_1D_cuda_kernel(array1D, avg):\n",
    "    # this is the unique thread ID within a 1D block\n",
    "    tidx = cuda.threadIdx.x \n",
    "    # Similarly, this is the unique block ID within the 1D grid\n",
    "    bidx = cuda.blockIdx.x  \n",
    "\n",
    "    # number of threads per block\n",
    "    block_dimx = cuda.blockDim.x \n",
    "    # number of blocks in the grid\n",
    "    grid_dimx = cuda.gridDim.x    \n",
    "    \n",
    "    start = tidx + bidx * block_dimx\n",
    "    stride = block_dimx * grid_dimx\n",
    "    \n",
    "    tmp = 0.\n",
    "    for i in range(start, array1D.shape[0], stride):\n",
    "        tmp += math.sqrt(array1D[i])\n",
    "    avg = tmp/array1D.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `threadIdx`, `blockIdx`, `blockDim` and `gridDim` are special objects provided by the CUDA backend for the sole purpose of knowing the geometry of the thread hierarchy and the position of the current thread within that geometry.\n",
    "- These objects can be 1D, 2D or 3D, depending on how the kernel was invoked. To access the value at each dimension, use the x, y and z attributes of these objects, respectively.\n",
    "   - `numba.cuda.threadIdx`: The thread indices in the current thread block. For 1D blocks, the index (given by the x attribute) is an integer spanning the range from 0 inclusive to numba.cuda.blockDim exclusive. A similar rule exists for each dimension when more than one dimension is used.\n",
    "   - `numba.cuda.blockDim`: The shape of the block of threads, as declared when instantiating the kernel. This value is the same for all threads in a given kernel, even if they belong to different blocks (i.e. each block is “full”).\n",
    "   - `numba.cuda.blockIdx`: The block indices in the grid of threads launched a kernel. For a 1D grid, the index (given by the x attribute) is an integer spanning the range from 0 inclusive to numba.cuda.gridDim exclusive. A similar rule exists for each dimension when more than one dimension is used.\n",
    "   - `numba.cuda.gridDim`: The shape of the grid of blocks, i.e. the total number of blocks launched by this kernel invocation, as declared when instantiating the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = 32\n",
    "blocks_per_grid = (array1D.size + (threads_per_block - 1)) // threads_per_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sqrt_1D_cuda_kernel[blocks_per_grid, threads_per_block](array1D, avg)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cuda2 = %timeit -o average_sqrt_1D_cuda_kernel[blocks_per_grid, threads_per_block](array1D, avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The execution time includes the transfer of the data from the host memory to the device memory before the computation and the transfer from the device memory back to the host memory at the end of the computation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regular Speedup:     {}\".format(time_reg.best/time_reg.best))\n",
    "print(\"Numba   Speedup:     {}\".format(time_reg.best/time_numba.best))\n",
    "print(\"Cuda    Speedup:     {}\".format(time_reg.best/time_cuda1.best))\n",
    "print(\"Cuda Kernel Speedup: {}\".format(time_reg.best/time_cuda2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='violet'>Absolute Positions</font>\n",
    "\n",
    "- Simple algorithms will tend to always use thread indices in the same way as shown in the example above. \n",
    "- Numba provides additional facilities to automate such calculations:\n",
    "  - `numba.cuda.grid`(ndim): Return the absolute position of the current thread in the entire grid of blocks. `ndim` should correspond to the number of dimensions declared when instantiating the kernel. If `ndim` is 1, a single integer is returned. If `ndim` is 2 or 3, a tuple of the given number of integers is returned.\n",
    "  - `numba.cuda.gridsize`(ndim): Return the absolute size (or shape) in threads of the entire grid of blocks. `ndim` has the same meaning as in grid() above.\n",
    "  \n",
    "`cuda.grid()` is a convenience function provided by Numba. In CUDA, blocks and grids are actually three dimensional. \n",
    "- Each block has dimensions: `cuda.blockDim.x`, `cuda.blockDim.y`, and `cuda.blockDim.z`.\n",
    "- The grid has dimensions: `cuda.gridDim.x`, `cuda.gridDim.y`, and `cuda.gridDim.z`.\n",
    "\n",
    "This means that each block has:\n",
    "\n",
    "$$num\\_of\\_threads\\_per\\_block = cuda.blockDim.x \\times cuda.blockDim.y \\times cuda.blockDim.z$$\n",
    "\n",
    "And the grid has:\n",
    "\n",
    "$$number\\_of\\_blocks = cuda.gridDim.x \\times cuda.gridDim.y \\times cuda.gridDim.z$$\n",
    "\n",
    "\n",
    "The generic way to define the number of threads while launching a kernel is actually:\n",
    "\n",
    "```python\n",
    "kernel_name[(griddimx, griddimy, griddimz), (blockdimx, blockdimy, blockdimz)](arguments)\n",
    "```\n",
    "Launching a kernel specifying only two integers like, `cudakernel1[1024, 1024](array)`, is equivalent to launching a kernel with y and z dimensions equal to 1, e.g. `cudakernel1[(1024, 1, 1), (1024, 1, 1)](array)`.\n",
    "\n",
    "CUDA provides the following values to identify each thread:\n",
    "\n",
    "- `cuda.threadIdx.x`, `cuda.threadIdx.y`, `cuda.threadIdx.z` that give the (x, y, z) positions of the current thread inside the current block,\n",
    "- `cuda.blockIdx.x`, `cuda.blockIdx.y`, `cuda.blockIdx.z` that give the (x, y, z) positions of the current block inside the grid.\n",
    "\n",
    "Therefore, the absolute position of a thread inside the grid is given by:\n",
    "\n",
    "```python\n",
    "(cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x, \n",
    " cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y,\n",
    " cuda.blockIdx.z * cuda.blockDim.z + cuda.threadIdx.z) \n",
    "```\n",
    "\n",
    "The convenience function:\n",
    "- `cuda.grid(3`) returns this whole tuple\n",
    "- `cuda.grid(2)` returns the tuple (`cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x`, `cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y`) \n",
    "- `cuda.grid(1)` returns the integer `cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x`.\n",
    "\n",
    "![GPU_grid](https://www.researchgate.net/profile/Omar-Bouattane/publication/321666991/figure/fig2/AS:572931245260800@1513608861931/Figure-2-Execution-model-of-a-CUDA-program-on-NVidias-GPU-Hierarchy-grid-blocks-and_W640.jpg)\n",
    "Image Source: Omar Bouttane\n",
    "\n",
    "With these functions, the incrementation example can become:\n",
    "\n",
    "![grid_1d](https://www.programmersought.com/images/863/2c49da674a6e9cc10d349b3e492428e7.png)\n",
    "Image Source: programmersought.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit   \n",
    "def average_sqrt_1D_cuda_kernel2(array1D, avg):\n",
    "    # number of threads per block\n",
    "    block_dimx = cuda.blockDim.x \n",
    "    # number of blocks in the grid\n",
    "    grid_dimx = cuda.gridDim.x    \n",
    "    \n",
    "    start = cuda.grid(1)\n",
    "    stride = block_dimx * grid_dimx\n",
    "    \n",
    "    tmp = 0.\n",
    "    for i in range(start, array1D.shape[0], stride):\n",
    "        tmp += math.sqrt(array1D[i])\n",
    "    avg[0] = tmp/array1D.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "threads_per_block = 32\n",
    "blocks_per_grid = (array1D.size + (threads_per_block - 1)) // threads_per_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We first transfer all the data to the device using the function `cuda.to_device` and we next execute the kernel with the device data as arguments:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_array1D = cuda.to_device(array1D)\n",
    "d_avg = cuda.device_array((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_sqrt_1D_cuda_kernel2[blocks_per_grid, threads_per_block](d_array1D, d_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = d_avg.copy_to_host()\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_cuda3 = %timeit -o average_sqrt_1D_cuda_kernel2[blocks_per_grid, threads_per_block](d_array1D, d_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Regular Speedup:       {}\".format(time_reg.best/time_reg.best))\n",
    "print(\"Numba   Speedup:       {}\".format(time_reg.best/time_numba.best))\n",
    "print(\"Cuda    Speedup:       {}\".format(time_reg.best/time_cuda1.best))\n",
    "print(\"Cuda Kernel   Speedup: {}\".format(time_reg.best/time_cuda2.best))\n",
    "print(\"Cuda Kernel 2 Speedup: {}\".format(time_reg.best/time_cuda3.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"violet\">Recommendations</font>\n",
    "To achieve the best performance:\n",
    "\n",
    "- Find ways to parallelize sequential code\n",
    "- Minimize data transfers between the host and the device\n",
    "- Adjust kernel launch configuration to maximize device utilization\n",
    "- Ensure global memory accesses are coalesced\n",
    "- Minimize redundant accesses to global memory whenever possible\n",
    "- Avoid different execution paths within the same warp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Modify the code below so that it can run on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment_1D_array(inarray, val):\n",
    "    outarray = np.zeros_like(inarray)\n",
    "    for i in range(inarray.size):\n",
    "        outarray[i] = inarray[i] + val\n",
    "    return outarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"violet\" size=4>Click here to access the solution</font> </b></summary>\n",
    "<p>\n",
    "\n",
    "<b>With Numba njit</b>\n",
    "```python\n",
    "    @njit\n",
    "    def increment_1D_array_numba1(inarray, val):\n",
    "        outarray = np.zeros_like(inarray)\n",
    "        for i in range(inarray.size):\n",
    "            outarray[i] = inarray[i] + val\n",
    "        return outarray\n",
    "```\n",
    "\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    \n",
    "<b>With Numba vectorize</b>\n",
    "```python\n",
    "    @vectorize(['float64(float64, float64)'])\n",
    "    def increment_1D_array_numba2(inarray, val):\n",
    "        return inarray + val\n",
    "```\n",
    "    \n",
    "</p>\n",
    "<p>\n",
    "\n",
    "<b>With Numba guvectorize</b>\n",
    "\n",
    "```python\n",
    "    @guvectorize(['(float64[:], float64, float64[:])'], '(n), ()->(n)', target =\"parallel\")\n",
    "    def increment_1D_array_numba3(inarray, val, outarray):\n",
    "        for i in range(inarray.size):\n",
    "            outarray[i] = inarray[i] + val\n",
    "```\n",
    "\n",
    "</p>\n",
    "    \n",
    "    \n",
    "<p>\n",
    "\n",
    "<b>With Numba guvectorize and Cuda</b>\n",
    "```python\n",
    "    @guvectorize(['(float64[:], float64, float64[:])'], '(n), ()->(n)', target =\"cuda\")\n",
    "    def increment_1D_array_cuda1(inarray, val, outarray):\n",
    "        for i in range(inarray.size): \n",
    "            outarray[i] = inarray[i] + val\n",
    "```\n",
    "\n",
    "</p>\n",
    "\n",
    "    \n",
    "<p>\n",
    "\n",
    "<b>With cuda.jit</b>\n",
    "```python\n",
    "    @cuda.jit\n",
    "    def increment_1D_array_cuda2(inarray, val, outarray):  \n",
    "        index = cuda.grid(1)\n",
    "        if index < inarray.size: \n",
    "            outarray[index] = inarray[index] + val\n",
    "```\n",
    "\n",
    "</p>\n",
    "    \n",
    "```python\n",
    "\n",
    "M = 100000000\n",
    "inarray = np.random.rand(M)\n",
    "outarray = np.zeros_like(inarray)\n",
    "val = 2.75\n",
    "\n",
    "time_reg = %timeit -o increment_1D_array(inarray, val)\n",
    "time_numba1 = %timeit -o increment_1D_array_numba1(inarray, val)\n",
    "time_numba2 = %timeit -o increment_1D_array_numba2(inarray, val)\n",
    "time_numba3 = %timeit -o increment_1D_array_numba3(inarray, val, outarray)\n",
    "time_cuda1 = %timeit -o increment_1D_array_cuda1(inarray, val, outarray)\n",
    "\n",
    "threads_per_block = 32\n",
    "blocks_per_grid = (inarray.size + (threads_per_block - 1)) // threads_per_block\n",
    "\n",
    "time_cuda2 = %timeit -o increment_1D_array_cuda2[blocks_per_grid, threads_per_block](inarray, val, outarray)\n",
    "    \n",
    "    \n",
    "print(\"Speedup Regular: {}\".format(time_reg.best/time_reg.best))\n",
    "print(\"Speedup Numba 1: {}\".format(time_reg.best/time_numba1.best))\n",
    "print(\"Speedup Numba 2: {}\".format(time_reg.best/time_numba2.best))\n",
    "print(\"Speedup Numba 3: {}\".format(time_reg.best/time_numba3.best))\n",
    "print(\"Speedup  Cuda 1: {}\".format(time_reg.best/time_cuda1.best))\n",
    "print(\"Speedup  Cuda 2: {}\".format(time_reg.best/time_cuda2.best))\n",
    "```\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Example:</font> Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 256\n",
    "A = np.random.rand(N, N)\n",
    "B = np.random.rand(N, N)\n",
    "C = np.zeros_like(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_multiplication(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            tmp = 0.\n",
    "            for k in range(n):\n",
    "                tmp  += A[i, k]*B[k, j]\n",
    "            C[i, j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tRegMat = %timeit -o matrix_multiplication(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def matrix_multiplication_numba(A, B, C):\n",
    "    \"\"\"\n",
    "        Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    n = len(A[0])\n",
    "    for i in prange(n):\n",
    "        for j in prange(n):\n",
    "            tmp = 0.\n",
    "            for k in prange(n):\n",
    "                tmp += A[i, k]*B[k, j]\n",
    "            C[i,j] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tNumMat = %timeit -o matrix_multiplication_numba(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First GPU Kernel\n",
    "- Each thread reads one row of `A` and one column of `B` and computes the corresponding element of `C`. \n",
    "- For input arrays where `A.shape == (m, n)` and `B.shape == (n, p)` then the result shape will be `C.shape = (m, p)`.\n",
    "\n",
    "![matmult1](https://nyu-cds.github.io/python-numba/fig/05-matmul.png)\n",
    "Image Source: nyu-cds.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matrix_multiplication_cuda(A, B, C):\n",
    "    \"\"\"\n",
    "      Perform square matrix multiplication of C = A * B using loops.\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The host code creates and initiliazes the arrays `A` and `B`, then moves them to the device.\n",
    "- It allocates space on the device for the result array. \n",
    "- Once the kernel has completed, the result array must be copied back to the host so that it can be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockdim = (16, 16)\n",
    "griddim = (8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tCudaMat1 = %timeit -o matrix_multiplication_cuda[griddim, blockdim](A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Regular: {}\".format(tRegMat.best/tRegMat.best))\n",
    "print(\"Speedup Numba:   {}\".format(tRegMat.best/tNumMat.best))\n",
    "print(\"Speedup CUDA 1:  {}\".format(tRegMat.best/tCudaMat1.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second GPU Kernel\n",
    "\n",
    "There can be a faster way for the matrix multiplication with Cuda:\n",
    "\n",
    "- Each thread block is responsible for computing a square sub-matrix of `C` and each thread for computing an element of the sub-matrix. \n",
    "- The sub-matrix is equal to the product of a square sub-matrix of `A` (`sA`) and a square sub-matrix of `B` (`sB`). \n",
    "- In order to fit into the device resources, the two input matrices are divided into as many square sub-matrices of dimension `TPB` as necessary, and the result computed as the sum of the products of these square sub-matrices.\n",
    "- Each product is performed by first loading `sA` and `sB` from global memory to shared memory, with one thread loading each element of each sub-matrix. \n",
    "- Once `sA` and sB have been loaded, each thread accumulates the result into a register (tmp). Once all the products have been calculated, the results are written to the matrix `C` in global memory.\n",
    "- The number of global memory accesses is reduced since `A` is now only read `B.shape[1] / TPB` times and `B` is read `A.shape[0] / TPB` times.\n",
    "\n",
    "\n",
    "\n",
    "![matmult2](https://nyu-cds.github.io/python-numba/fig/05-matmulshared.png)\n",
    "Image Source: nyu-cds.github.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32\n",
    "\n",
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def matrix_multiplication_cuda_fast(A, B, C):\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float32)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    if x >= C.shape[0] and y >= C.shape[1]:\n",
    "        # Quit if (x, y) is outside of valid C boundary\n",
    "        return\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = 0.\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i * TPB]\n",
    "        sB[tx, ty] = B[tx + i * TPB, y]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j] * sB[j, ty]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "\n",
    "    C[x, y] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockdim = (16, 16)\n",
    "griddim = (8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tCudaMat2 = %timeit -o matrix_multiplication_cuda_fast[griddim, blockdim](A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup Regular: {}\".format(tRegMat.best/tRegMat.best))\n",
    "print(\"Speedup Numba:   {}\".format(tRegMat.best/tNumMat.best))\n",
    "print(\"Speedup CUDA 1:  {}\".format(tRegMat.best/tCudaMat1.best))\n",
    "print(\"Speedup CUDA 2:  {}\".format(tRegMat.best/tCudaMat2.best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Example:</font> Mandelbrot Fractal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pure Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color function for point at (x, y)\n",
    "def mandel(x, y, max_iters):\n",
    "    c = complex(x, y)\n",
    "    z = 0.0j\n",
    "    for i in range(max_iters):\n",
    "        z = z*z + c\n",
    "        if z.real*z.real + z.imag*z.imag >= 4:\n",
    "            return i\n",
    "    return max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fractal(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    for x in range(width):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(height):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "create_fractal(xmin, xmax, ymin, ymax, gimage, iters)\n",
    "dt_py = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on CPU in {} s\".format(dt_py))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numba Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandel_numba = jit(nb.uint32(nb.float32, nb.float32, nb.uint32))(mandel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def create_fractal_numba(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    for x in range(width):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(height):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel_numba(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "create_fractal_numba(xmin, xmax, ymin, ymax, gimage, iters)\n",
    "dt_numba = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on CPU in {} s\".format(dt_numba))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mandel_gpu = cuda.jit(restype=nb.uint32, \n",
    "                      argtypes=[nb.float32, nb.float32, nb.uint32], \n",
    "                      device=True)(mandel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(argtypes=[nb.float32, nb.float32, nb.float32, nb.float32, nb.uint8[:,:], nb.uint32])\n",
    "def create_fractal_kernel(xmin, xmax, ymin, ymax, image, iters):\n",
    "    height, width = image.shape\n",
    "\n",
    "    pixel_size_x = (xmax - xmin)/width\n",
    "    pixel_size_y = (ymax - ymin)/height\n",
    "\n",
    "    startX, startY = cuda.grid(2)\n",
    "    gridX = cuda.gridDim.x * cuda.blockDim.x # stride in x\n",
    "    gridY = cuda.gridDim.y * cuda.blockDim.y # stride in y\n",
    "\n",
    "    for x in range(startX, width, gridX):\n",
    "        real = xmin + x*pixel_size_x\n",
    "        for y in range(startY, height, gridY):\n",
    "            imag = ymin + y*pixel_size_y\n",
    "            color = mandel_gpu(real, imag, iters)\n",
    "            image[y, x]  = color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gimage = np.zeros((1024, 1536), dtype=np.uint8)\n",
    "blockdim = (32, 8)\n",
    "griddim = (32, 16)\n",
    "xmin, xmax, ymin, ymax = np.array([-2.0, 1.0, -1.0, 1.0]).astype('float32')\n",
    "iters = 50\n",
    "\n",
    "start = timer()\n",
    "d_image = cuda.to_device(gimage)\n",
    "create_fractal_kernel[griddim, blockdim](xmin, xmax, ymin, ymax, d_image, iters)\n",
    "d_image.to_host()\n",
    "dt_cuda = timer() - start\n",
    "\n",
    "print(\"Mandelbrot created on GPU in {} s\".format(dt_cuda))\n",
    "plt.imshow(gimage);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Speedup CPU:    {}\".format(dt_py/dt_py))\n",
    "print(\"Speedup Numba:  {}\".format(dt_py/dt_numba))\n",
    "print(\"Speedup CUDA:   {}\".format(dt_py/dt_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [cuDF and Dask-cuDF](https://docs.rapids.ai/api/cudf/stable/10min.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install RAPIDS\n",
    "\n",
    "- Install most recent Miniconda release compatible with Google Colab's Python install (3.7.10)\n",
    "- Removes incompatible files\n",
    "- Install RAPIDS' current stable version of its libraries, including:\n",
    "   - cuDF\n",
    "   - cuML\n",
    "   - cuGraph\n",
    "   - cuSpatial\n",
    "   - cuSignal\n",
    "   - xgboost\n",
    "- Set necessary environment variables\n",
    "- Copy RAPIDS .so files into current working directory, a workaround for conda/colab interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
    "!bash rapidsai-csp-utils/colab/rapids-colab.sh stable\n",
    "\n",
    "import sys, os\n",
    "\n",
    "dist_package_index = sys.path.index('/usr/local/lib/python3.7/dist-packages')\n",
    "sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.7/site-packages'] + sys.path[dist_package_index:]\n",
    "sys.path\n",
    "exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import dask_cudf\n",
    "import numpy as np\n",
    "\n",
    "print('Dask cuDF Version:', dask_cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `cudf` to create a dataframe and perform operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 5000000\n",
    "df = cudf.DataFrame({'X':np.random.randint(1000, size=num_rows),\n",
    "                     'Y':np.random.randint(1000, size=num_rows)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_squares(df):\n",
    "    return df.X**2 + df.Y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df['add_squares'] = add_squares(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same as above using `dask_cudf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dask_cudf.from_cudf(df, npartitions=2)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddf['z'] = add_squares(ddf).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Series Data**\n",
    "\n",
    "`DataFrames` supports `datetime` typed columns, which allow users to interact with and filter data based on specific timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "date_df = cudf.DataFrame()\n",
    "date_df['date'] = pd.date_range('01/05/1980', periods=15000, freq='D')\n",
    "date_df['value'] = cp.random.sample(len(date_df))\n",
    "date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_date1 = dt.datetime.strptime('2001-09-11', '%Y-%m-%d')\n",
    "search_date2 = dt.datetime.strptime('2019-11-23', '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_df.query('date >= @search_date1 and date <= @search_date2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ddf = dask_cudf.from_cudf(date_df, npartitions=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "date_ddf.query('date >= @search_date1 and date <= @search_date2', \n",
    "               local_dict={'search_date1':search_date1, \n",
    "                           'search_date2':search_date2}).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> NYC Flights Dataset</font>\n",
    "\n",
    "Data is specific to flights (in 1990's) out of the three airports in the New York City area.\n",
    "\n",
    "\n",
    "Download the remote data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "print(\"\\t Downloading NYC dataset...\", end=\"\\n\", flush=True)\n",
    "\n",
    "url = \"https://storage.googleapis.com/dask-tutorial-data/nycflights.tar.gz\"\n",
    "filename, header = urllib.request.urlretrieve(url, \"nycflights.tar.gz\")\n",
    "\n",
    "print(\"\\t Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the .csv files from the tar file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(filename, mode=\"r:gz\") as flights:\n",
    "     flights.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt data/nycflights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the files at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "df = dask_cudf.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\")) \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The representation of the dataframe object contains no data. \n",
    "- `cudf.read_csv` reads in the entire file before inferring datatypes.\n",
    "- `dask_cudf.dataframe.read_csv` only reads in a sample from the beginning of the file (or first file). These inferred datatypes are then enforced when reading all partitions.\n",
    "\n",
    "We can display the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we display the last few rows, we have a problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an issue with the data types of few columns.\n",
    "- The datatypes inferred in the sample are incorrect.\n",
    "- We can fix it by reading the files again and specify the appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_type = {\n",
    "    'Year': int, 'Month': int, 'DayofMonth': int, 'DayOfWeek': int, \n",
    "    'DepTime': float, 'CRSDepTime': int, 'ArrTime': float , \n",
    "    'CRSArrTime': int, 'UniqueCarrier': str, 'FlightNum': int, \n",
    "    'TailNum': str, 'ActualElapsedTime': float, \n",
    "    'CRSElapsedTime': float, 'AirTime': float, 'ArrDelay': float,\n",
    "    'DepDelay': float, 'Origin': str, 'Dest': str, 'Distance': int, \n",
    "    'TaxiIn': int, 'TaxiOut': int, 'Cancelled': bool, 'Diverted': bool\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask_cudf.read_csv(os.path.join(\"data\", \"nycflights\", \"*.csv\"), \n",
    "                        dtype=col_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Perform Operations as with `CuDF DataFrames`</font>\n",
    "\n",
    "**Maximum value of a column**:\n",
    "\n",
    "- We now want to compute the maximum of the `DepDelay` column.\n",
    "- With `CuDF`, we would loop over each file to find the individual maximums, then find the final maximum over all the individual maximums.\n",
    "- `dask_cudf.dataframe` allows us to write pandas-like code that operates on large than memory datasets in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do the same thing in `CuDF`, we will have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import glob\n",
    "\n",
    "list_files = glob.glob(\"data/nycflights/*csv\")\n",
    "   \n",
    "maxes = list()\n",
    "for file_name in list_files:\n",
    "    pddf = cudf.read_csv(file_name)\n",
    "    maxes.append(pddf.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "\n",
    "print(\"Final Maximum: \", max(maxes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of departures from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Origin\").Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of non-cancelled flights were taken from each airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by destinations and count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\").count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Dest\")[\"ArrDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.ArrDelay+df.DepDelay>30.0].groupby(\"Dest\").Dest.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "- Consider the CuDF code below that computes the mean departure delay per airport. \n",
    "- Parallelize the code using Dask-CuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "sum_delays = list()\n",
    "count_delays = list()\n",
    "\n",
    "for file_name in list_files:\n",
    "    pddf = cudf.read_csv(file_name)\n",
    "    by_origin = pddf.groupby('Origin')\n",
    "    loc_total = by_origin.DepDelay.sum()\n",
    "    loc_count = by_origin.DepDelay.count()\n",
    "    sum_delays.append(loc_total)\n",
    "    count_delays.append(loc_count)\n",
    "\n",
    "total_delays = sum(sum_delays)\n",
    "n_flights = sum(count_delays)\n",
    "mean_delays = total_delays / n_flights\n",
    "print(\"Mean delays: {}\".format(mean_delays))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"violet\" size=4>Click here to access the solution</font> </b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "%%time \n",
    "    \n",
    "df.groupby(\"Origin\").DepDelay.mean().compute()   \n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
