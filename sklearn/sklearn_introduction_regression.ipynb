{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Regression with Scikit-Learn</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "  <head> </head>\n",
    "  <body>\n",
    "<script src=\"https://bot.voiceatlas.mysmce.com/v1/chatlas.js\"></script>\n",
    "<app-chatlas\n",
    "\tatlas-id=\"f759a188-f8bb-46bb-9046-3b1b961bd6aa\"\n",
    "\twidget-background-color=\"#3f51b5ff\"\n",
    "\twidget-text-color=\"#ffffffff\"\n",
    "\twidget-title=\"Chatlas\">\n",
    "</app-chatlas>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Links\n",
    "\n",
    "- <a href=\"https://medium.com/towards-artificial-intelligence/calculating-simple-linear-regression-and-linear-best-fit-an-in-depth-tutorial-with-math-and-python-804a0cb23660\">Calculating Simple Linear Regression and Linear Best Fit an In-depth Tutorial with Math and Python</a>\n",
    "- <a href=\"https://scikit-learn.org/stable/tutorial/index.html\">scikit-learn Tutorials</a>\n",
    "- <a href=\"https://medium.com/@amitg0161/sklearn-linear-regression-tutorial-with-boston-house-dataset-cde74afd460a\">Sklearn Linear Regression Tutorial with Boston House Dataset</a>\n",
    "- <a href=\"https://www.dataquest.io/blog/sci-kit-learn-tutorial/\">Scikit-learn Tutorial: Machine Learning in Python</a>\n",
    "- <a href=\"https://debuggercafe.com/image-classification-with-mnist-dataset/\">Image Classification with MNIST Dataset</a>\n",
    "- <a href=\"https://davidburn.github.io/notebooks/mnist-numbers/MNIST%20Handwrititten%20numbers/\">MNIST handwritten number identification</a>\n",
    "- [K-Fold Cross-Validation in Python Using SKLearn](https://www.askpython.com/python/examples/k-fold-cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Scikit-Learn</font>\n",
    "\n",
    "- Scikit-learn is a free machine learning library for Python. \n",
    "- Provides a selection of efficient tools for machine learning and statistical modeling including: \n",
    "     - **Classification:** Identifying which category an object belongs to. Example: Spam detection\n",
    "     - **Regression:** Predicting a continuous variable based on relevant independent variables. Example: Stock price predictions\n",
    "     - **Clustering:** Automatic grouping of similar objects into different clusters. Example: Customer segmentation \n",
    "     - **Dimensionality Reduction:** Seek to reduce the number of input variables in training data by preserving the salient relationships in the data\n",
    "- Features various algorithms like support vector machine, random forests, and k-neighbours.\n",
    "- Supports Python numerical and scientific libraries like NumPy and SciPy.\n",
    "\n",
    "\n",
    "Some popular groups of models provided by scikit-learn include:\n",
    "\n",
    "- **Clustering:** Group unlabeled data such as KMeans.\n",
    "- **Cross Validation:** Estimate the performance of supervised models on unseen data.\n",
    "- **Datasets:** for test datasets and for generating datasets with specific properties for investigating model behavior.\n",
    "- **Dimensionality Reduction:** Reduce the number of attributes in data for summarization, visualization and feature selection such as Principal component analysis.\n",
    "- **Ensemble Methods:** Combine the predictions of multiple supervised models.\n",
    "- **Feature Extraction:** Define attributes in image and text data.\n",
    "- **Feature Selection:** Identify meaningful attributes from which to create supervised models.\n",
    "- **Parameter Tuning:** Get the most out of supervised models.\n",
    "- **Manifold Learning:** Summarize and depicting complex multi-dimensional data.\n",
    "- **Supervised Models:** A vast array not limited to generalized linear models, discriminate analysis, naive bayes, lazy methods, neural networks, support vector machines and decision trees.\n",
    "- **Unsupervised Learning Algorithms:** âˆ’ They include clustering, factor analysis, PCA (Principal Component Analysis), unsupervised neural networks.\n",
    "\n",
    "\n",
    "![fig_sckl](https://ulhpc-tutorials.readthedocs.io/en/latest/python/advanced/scikit-learn/images/scikit.png)\n",
    "Image Source: ulhpc-tutorials.readthedocs.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Requirements\n",
    "\n",
    "- Numpy\n",
    "- scipy\n",
    "- matplotlib\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy version:        {np.__version__}\")\n",
    "print(f\"Pandas version:       {pd.__version__}\")\n",
    "print(f\"Seaborn version:      {sns.__version__}\")\n",
    "print(f\"Scikit-Learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">Numerical Data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Ames, Iowa Dataset</font>\n",
    "- Contains information about different aspects of residential homes in Ames, Iowa.\n",
    "- There are 1460 observations and 79 feature variables in this dataset.\n",
    "- [Information on the dataset can be done here.](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "We want to predict the value of prices of the house using the given features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df = pd.read_csv('data/housing_data.csv')\n",
    "ames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features of the Dataset and First Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The target is the `SalePrice` represented in the last column\n",
    "- 37 columns have numerical values\n",
    "- 43 columns have `object` as data type. Are we going to use them for our analysis?\n",
    "- There are many missing values. How are we going to treat them?\n",
    "- From the data, the following columns have far fewer quantities and may not not be relevant for the model we want to build:\n",
    "   - `MiscFeature` (54)\n",
    "   - `Fence` (281)\n",
    "   - `PoolQC` (7)\n",
    "   - `Alley` (91) \n",
    "   \n",
    "We can drop the four columns with a lot of missing values. We also drop the `Id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_cols = ['Id', 'MiscFeature', 'Fence', 'PoolQC', 'Alley']\n",
    "ames_df.drop(dropped_cols, axis=1, inplace=True)\n",
    "ames_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To facilitate the analysis, we are only going to consider columns with numerical values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df_num = ames_df.select_dtypes(include=['float64', 'int64'])\n",
    "ames_df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = list(ames_df_num.columns)\n",
    "feature_names.pop(-1)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Exploratory Data Analysis</font>\n",
    "\n",
    "- Important step before training the model. \n",
    "- We use statistical analysis and visualizations to understand the relationship of the target variable with other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtain basic statistics on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df_num.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The average sale price of a house in our dataset is close to $\\$180,921$, with most of the values falling within the $\\$129,975$ to $\\$214,000$ range.\n",
    "- The fact the sale price standard deviation is $\\$79442$ indicates a large spread of the sale price and the exisitence of outliers.\n",
    "- There might be many mixing values in `LotFrontage` (Linear feet of street connected to property). Do we need to keep this column?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Missing Values\n",
    "It is a good practice to see if there are any missing values in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of missing values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df_num.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also determine the perecentage of missing values in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = ames_df_num.isnull().sum().sort_values(ascending=False)\n",
    "percent = (ames_df_num.isnull().sum()/ames_df_num.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, \n",
    "                         keys=['Total', 'Percent'])\n",
    "missing_data.head(ames_df_num.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we going to do with the missing values in?\n",
    "- `LotFrontage` (259): Linear feet of street connected to property\n",
    "- `GarageYrBlt` (81): Year garage was built\n",
    "- `MasVnrArea` (8): Masonry veneer area in square feet\n",
    "\n",
    "**We choose to drop the rows with missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df_nonan = ames_df_num.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_df_nonan.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6));\n",
    "plt.hist(ames_df_nonan['SalePrice']);\n",
    "plt.title('Ames Housing Prices and Count Histogram');\n",
    "plt.xlabel('Sale Price');\n",
    "plt.ylabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6));\n",
    "sns.distplot(ames_df_nonan['SalePrice']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that the values of `SalePrice` are skewed to the left and have some outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap: Two-Dimensional Graphical Representation\n",
    "- Represent the individual values that are contained in a matrix as colors.\n",
    "- Create a correlation matrix that measures the linear relationships between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 11));\n",
    "correlation_matrix = ames_df_nonan.corr().round(1);\n",
    "sns.heatmap(correlation_matrix, cmap=\"YlGnBu\", annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to select only correlations that verify specific conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(22, 11));\n",
    "sns.heatmap(correlation_matrix[(correlation_matrix >= 0.5) | \n",
    "                               (correlation_matrix <= -0.4)], \n",
    "            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n",
    "            annot=True, annot_kws={\"size\": 8}, square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **OverallQual** and **GrLivArea** have a strong positive correlation with **SalePrice** (0.8 and 0.7 respectively).\n",
    "- The features **GrLivArea** & **2ndFlrSF**, **BsmtFullBath** & **BsmtFinSF1** and **TotRmsAbvGrrd** & **BedroomAbvGrd** have a correlation of at least 0.7. These feature pairs are strongly correlated to each other. This can affect the model. \n",
    "- The predictor variables such as **1stFlrSF**, **TotalBsmtSF**, **GarageArea**, **GarageCars**, etc., have good positive correlation with the target. Increase in any of them leads to the increase in the price of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_name in feature_names:\n",
    "    plt.figure(figsize=(5, 4));\n",
    "    plt.scatter(ames_df_nonan[feature_name], ames_df_nonan['SalePrice']);\n",
    "    plt.ylabel('Sale Price', size=12);\n",
    "    plt.xlabel(feature_name, size=12);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sale prices increase as the value of GrLivArea increases linearly. - There are few outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above observations we will plot an `lmplot` between **GrLivArea** and **SalePrice** to see the relationship between the two more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x = 'GrLivArea', y = 'SalePrice', data = ames_df_nonan);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Model Selection Process</font>\n",
    "\n",
    "![fig_skl](https://miro.medium.com/max/1400/1*LixatBxkewppAhv1Mm5H2w.jpeg)\n",
    "Image Source: Christophe Bourguignat\n",
    "\n",
    "- A Machine Learning algorithm needs to be trained on a set of data to learn the relationships between different features and how these features affect the target variable. \n",
    "- We need to divide the entire data set into two sets:\n",
    "    + Training set on which we are going to train our algorithm to build a model. \n",
    "    + Testing set on which we will test our model to see how accurate its predictions are.\n",
    "    \n",
    "Before we create the two sets, we need to identify the algorithm we will use for our model.\n",
    "We can use the `machine_learning_map` map (shown at the top of this page) as a cheat sheet to shortlist the algorithms that we can try out to build our prediction model. Using the checklist letâ€™s see under which category our current dataset falls into:\n",
    "- We have **1121** samples: >50? (**Yes**)\n",
    "- Are we predicting a category? (**No**)\n",
    "- Are we predicting a quantity? (**Yes**)\n",
    "\n",
    "Based on the checklist that we prepared above and going by the `machine_learning_map` we can try out **regression methods** such as:\n",
    "\n",
    "- Linear Regression \n",
    "- Lasso\n",
    "- ElasticNet Regression\n",
    "- Ridge Regression: \n",
    "- K Neighbors Regressor\n",
    "- Decision Tree Regressor\n",
    "- Simple Vector Regression (SVR)\n",
    "- Ada Boost Regressor\n",
    "- Gradient Boosting Regressor\n",
    "- Random Forest Regression\n",
    "- Extra Trees Regressor\n",
    "\n",
    "Check the following documents on regresssion: \n",
    "<a href=\"https://scikit-learn.org/stable/supervised_learning.html\">Supervised learning--scikit-learn</a>,\n",
    "<a href=\"https://developer.ibm.com/technologies/data-science/tutorials/learn-regression-algorithms-using-python-and-scikit-learn/\">Learn regression algorithms using Python and scikit-learn</a>,\n",
    "<a href=\"https://www.pluralsight.com/guides/non-linear-\">Non-Linear Regression Trees with scikit-learn</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Simple Linear Model</font>\n",
    "- It is difficult to visualize the multiple features.\n",
    "- We want to predict the house price with just one variable and then move to the regression with all features.\n",
    "- Because **GrLivArea** shows positive correlation with **SalePrice**, we will use **GrLivArea** for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_garage = ames_df_nonan.GrLivArea\n",
    "y_price = ames_df_nonan.SalePrice\n",
    "\n",
    "\n",
    "X_garage = np.array(X_garage).reshape(-1,1)\n",
    "y_price = np.array(y_price).reshape(-1,1)\n",
    "\n",
    "print(X_garage.shape)\n",
    "print(y_price.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and testing sets\n",
    "- We split the data into training and testing sets. \n",
    "- We train the model with 80% of the samples and test with the remaining 20%. \n",
    "- We do this to assess the modelâ€™s performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, Y_train_1, Y_test_1 = \\\n",
    "             train_test_split(X_garage, y_price, \n",
    "                              test_size = 0.2, random_state=5)\n",
    "\n",
    "print(X_train_1.shape)\n",
    "print(Y_train_1.shape)\n",
    "print(X_test_1.shape)\n",
    "print(Y_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and testing the model\n",
    "- We use scikit-learnâ€™s LinearRegression to train our model on both the training and check it on the test sets.\n",
    "- We check the model performance on the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_1 = LinearRegression()\n",
    "reg_1.fit(X_train_1, Y_train_1)\n",
    "\n",
    "y_train_predict_1 = reg_1.predict(X_train_1)\n",
    "rmse = (np.sqrt(metrics.mean_squared_error(Y_train_1, y_train_predict_1)))\n",
    "r2 = round(reg_1.score(X_train_1, Y_train_1),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f'RMSE is {rmse}')\n",
    "print(f'R2 score is {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = reg_1.predict(X_test_1)\n",
    "rmse = (np.sqrt(metrics.mean_squared_error(Y_test_1, y_pred_1)))\n",
    "r2 = round(reg_1.score(X_test_1, Y_test_1),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination: 1 is perfect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Coefficient of determination: {metrics.r2_score(Y_test_1, y_pred_1) :.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45-Degree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5));\n",
    "plt.scatter(Y_test_1, y_pred_1);\n",
    "plt.plot(y_price, y_price, '--k');\n",
    "plt.axis('tight');\n",
    "plt.xlabel(\"Actual Sale Prices\");\n",
    "plt.ylabel(\"Predicted House Prices\");\n",
    "#plt.xticks(range(0, int(max(y_test)),2));\n",
    "#plt.yticks(range(0, int(max(y_test)),2));\n",
    "plt.title(\"Actual Prices vs Predicted prices\");\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Linear Regression Model with All Variables</font>\n",
    "- We want to create a model considering all the features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ames_df_nonan.drop('SalePrice', axis = 1)\n",
    "y = ames_df_nonan['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the `train_test_split` to split the data into random train and test subsets.\n",
    "- Everytime you run it without specifying `random_state`, you will get a different result.\n",
    "- If you use `random_state=some_number`, then you can guarantee the split will be always the same.\n",
    "- It doesn't matter what the value of `random_state` is:  42, 0, 21, ...\n",
    "- This is useful if you want reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_all = LinearRegression()\n",
    "reg_all.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = reg_all.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(metrics.mean_squared_error(y_train, y_train_predict)))\n",
    "r2 = round(reg_all.score(X_train, y_train),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f'RMSE is {rmse}')\n",
    "print(f'R2 score is {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg_all.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "r2 = round(reg_all.score(X_test, y_test),2)\n",
    "\n",
    "print(f\"The model performance for training set\")\n",
    "print(f\"--------------------------------------\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R^2: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of determination: 1 is perfect prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Coefficient of determination: {metrics.r2_score(y_test, y_pred) :.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_test - y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45-Degree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5));\n",
    "plt.scatter(y_test, y_pred);\n",
    "plt.plot(y, y, '--k');\n",
    "plt.axis('tight');\n",
    "plt.xlabel(\"Actual House Prices ($1000)\");\n",
    "plt.ylabel(\"Predicted House Prices: ($1000)\");\n",
    "#plt.xticks(range(0, int(max(y_test)),2));\n",
    "#plt.yticks(range(0, int(max(y_test)),2));\n",
    "plt.title(\"Actual Prices vs Predicted prices\");\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMS: %r \" % np.sqrt(np.mean((y_test - y_pred) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "df2 = df1.head(10)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Choosing the Best Model:</font> k-Fold Cross-Validation\n",
    "\n",
    "- Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "- It is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data.\n",
    "- We use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n",
    "- The biggest advantage of this method is that every data point is used for validation exactly once and for training `k-1` times.\n",
    "- To choose the final model to use, **we select the one that has the lowest validation error**.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into `k` groups\n",
    "3. For each unique group:\n",
    "       3.1 Take the group as a hold out or test data set\n",
    "       3.2 Take the remaining k-1 groups as a training data set\n",
    "       3.3 Fit a model on the training set and evaluate it on the test set\n",
    "       3.4 Retain the evaluation score and discard the model  \n",
    "4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "How to choose **k**?\n",
    "- A poorly chosen value for **k** may result in a mis-representative idea of the skill of the model, such as a score with a high variance, or a high bias.\n",
    "- The choice of **k** is usually 5 or 10, but there is no formal rule. As **k** gets larger, the difference in size between the training set and the resampling subsets gets smaller. As this difference decreases, the bias of the technique becomes smaller.\n",
    "- A value of **k=10** is very common in the field of applied machine learning, and is recommend if you are struggling to choose a value for your dataset.\n",
    "\n",
    "Below is the visualization of a k-fold validation when k=5.\n",
    "![FIG_kFold](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "Image Source: https://scikit-learn.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# user variables to tune\n",
    "seed    = 9\n",
    "folds   = 10\n",
    "metric  = \"neg_mean_squared_error\"\n",
    "\n",
    "# hold different regression models in a single dictionary\n",
    "models = dict()\n",
    "models[\"Linear\"]        = LinearRegression()\n",
    "models[\"Lasso\"]         = Lasso()\n",
    "models[\"ElasticNet\"]    = ElasticNet()\n",
    "models[\"Ridge\"]         = Ridge()\n",
    "models[\"BayesianRidge\"] = BayesianRidge()\n",
    "models[\"KNN\"]           = KNeighborsRegressor()\n",
    "models[\"DecisionTree\"]  = DecisionTreeRegressor()\n",
    "models[\"SVR\"]           = SVR()\n",
    "models[\"AdaBoost\"]      = AdaBoostRegressor()\n",
    "models[\"GradientBoost\"] = GradientBoostingRegressor()\n",
    "models[\"RandomForest\"]  = RandomForestRegressor()\n",
    "\n",
    "# 10-fold cross validation for each model\n",
    "model_results = list()\n",
    "model_names   = list()\n",
    "for model_name in models:\n",
    "    model   = models[model_name]\n",
    "    k_fold  = KFold(n_splits=folds, random_state=seed, shuffle=True)\n",
    "    results = cross_val_score(model, X_train, y_train, cv=k_fold, scoring=metric)\n",
    "    \n",
    "    model_results.append(results)\n",
    "    model_names.append(model_name)\n",
    "    print(\"{:>20}: {:16.2f}, {:15.2f}\".format(model_name, round(results.mean(), 3), \n",
    "                                  round(results.std(), 3)))\n",
    "\n",
    "# box-whisker plot to compare regression models\n",
    "figure = plt.figure(figsize=(12, 9));\n",
    "figure.suptitle('Regression models comparison');\n",
    "ax = figure.add_subplot(111);\n",
    "plt.boxplot(model_results);\n",
    "ax.set_xticklabels(model_names, rotation = 45, ha=\"right\");\n",
    "ax.set_ylabel(\"Mean Squared Error (MSE)\");\n",
    "plt.margins(0.05, 0.1);\n",
    "#plt.savefig(\"model_mse_scores.png\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the above comparison, we can see that `Gradient Boosting Regression` model outperforms all the other regression models:** it has the smallest mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Model with Gradient Boosting Regression</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "gbr_predicted = gbr.predict(X_test)\n",
    "gbr_expected = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Root Mean Square Error:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMS: %r \" % np.sqrt(np.mean((gbr_predicted - gbr_expected) ** 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The coefficient of determination**: (1 is perfect prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coeff of determination: {:.4f}'.format(metrics.r2_score(gbr_expected, gbr_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(gbr_expected - gbr_predicted);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45-Degree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5));\n",
    "plt.scatter(gbr_expected, gbr_predicted)\n",
    "plt.plot(y, y, '--k');\n",
    "plt.axis('tight');\n",
    "plt.xlabel('True price ($1000s)');\n",
    "plt.ylabel('Predicted price ($1000s)');\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zoom in:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Actual': gbr_expected, 'Predicted': gbr_predicted})\n",
    "df2 = df1.head(10)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "- Once we have a trained model, we can understand feature importance (or variable importance) of the dataset which tells us how important each feature is, to predict the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 11));\n",
    "\n",
    "feature_importance = gbr.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos        = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "np_feature_names = np.array(feature_names)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center');\n",
    "plt.yticks(pos, np_feature_names[sorted_idx]);\n",
    "plt.xlabel('Relative Importance');\n",
    "plt.title('Variable Importance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot training deviance:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((n_estimators,), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(gbr.staged_predict(X_test)):\n",
    "    test_score[i] = gbr.loss_(gbr_expected, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6));\n",
    "plt.subplot(1, 1, 1);\n",
    "plt.title('Deviance');\n",
    "plt.plot(np.arange(n_estimators) + 1, \n",
    "         gbr.train_score_, 'b-',\n",
    "         label='Training Set Deviance');\n",
    "plt.plot(np.arange(n_estimators) + 1, \n",
    "         test_score, 'r-',\n",
    "         label='Test Set Deviance');\n",
    "plt.legend(loc='upper right');\n",
    "plt.xlabel('Boosting Iterations');\n",
    "plt.ylabel('Deviance');"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
