{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "    <h1><font color=\"red\">Overview of Neural Networks</font></h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Reference\n",
    "\n",
    "\n",
    "- [What is Neural Network: Overview, Applications, and Advantages](https://www.simplilearn.com/tutorials/deep-learning-tutorial/what-is-neural-network)\n",
    "- [Introduction to Neural Networks](https://intellipaat.com/blog/tutorial/machine-learning-tutorial/neural-network-tutorial/)\n",
    "- [Neural Networks Tutorial – A Pathway to Deep Learning](https://adventuresinmachinelearning.com/neural-networks-tutorial/)\n",
    "- [A Beginner's Guide to Neural Networks and Deep Learning](https://wiki.pathmind.com/neural-network)\n",
    "- [Neural Networks: What they are & why they matter](https://www.sas.com/en_us/insights/analytics/neural-networks.html)\n",
    "- [The limits and challenges of deep learning](https://bdtechtalks.com/2018/02/27/limits-challenges-deep-learning-gary-marcus/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> What are Neural Networks? </font>\n",
    "\n",
    "\n",
    "- Biological neural networks have interconnected `neurons` with `dendrites` that receive inputs, then based on these inputs they produce an output signal through an `axon` to another `neuron`.\n",
    "- Artificial Neural Networks (ANN) are computing systems with interconnected nodes that work much like neurons in the human brain.\n",
    "- They are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. \n",
    "- The task of every neuron is to process the information received and then transmit it to the neurons in the next layer.\n",
    "- If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\"> Layers in Neural Networks</font>\n",
    "\n",
    "There are three main layers in neural networks:\n",
    "\n",
    "- **Input layer:** The input layer is where the network starts. The number of units in this layer is fixed and corresponds exactly to the number of input features.\n",
    "- **Hidden layers:** The number of hidden layer and units per layer are the free parameters that one has to fix. There is no rule to decide these two parameters but it depends strongly on the problem.\n",
    "- **Output layer:** The output layer is where the network ends and the predictions are given. In the case of a classification problem, the number of units in the output layer corresponds exactly to the number of classes.\n",
    "\n",
    "<font color=\"blue\">There is not any precise rule, in general the number of hidden layers depends strongly on the problem, at odds to the input and output layers</font>. \n",
    "\n",
    "![nn](https://www.xenonstack.com/hubfs/xenonstack-artificial-neural-network-architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>How Do Neural Network Works?</font>\n",
    "\n",
    "The three layers form the base for all new networks regardless of complexity.\n",
    "Once the architecture of a neural network has been set, we can proceed to the training of the neural network.\n",
    "\n",
    "- The input features are fed to the input layer. \n",
    "- The network goes through all the hidden layer(s) until the output layer, where the predictions are produced.\n",
    "- Each layer has an __activation function__ which activates a `neuron` and can have several expressions, but in most the cases it is either a Rectified Linear Unit (ReLU) or a logistic function.\n",
    "- A `neuron` is connected with another neuron through a connection link. Each connection link is associated with a __weight__ that has information about the input signal.\n",
    "- A `neuron` has one or more inputs, a __bias__ (a constant weight outside of the inputs that allows to achieve better fit for the predictive models), an activation function, and a single output. The perceptron receives inputs, multiplies them by some weight, and then passes them into an activation function to produce an output. \n",
    "\n",
    "![fig_neuron](https://d2r55xnwy6nx47.cloudfront.net/uploads/2021/02/Simulating-a-Neuron.svg)\n",
    "Image Source: [Samuel Velasco/Quanta Magazine](https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/)\n",
    "\n",
    "\n",
    "- Between two adjacent layers there is always a weight matrix which is responsible to transmit the information. \n",
    "   - For an `N`-layer neural network, we have `N-1` weight matrices and the `j`-th matrix, i.e. the matrix of the `j`-th layer, will be a function of all the `j-1` matrices in the previous layers.\n",
    "- The output layer delivers a prediction, which must be compared to the real values. \n",
    "   - This comparison is an estimate of the error. We want to minimize the error, hence optimize the parameters of the problems, i.e. the weight matrices. \n",
    "   - One of the best known optimizing algorithms is the gradient descent.\n",
    "\n",
    "\n",
    "#### Processes\n",
    "\n",
    "<font color=\"red\">Neural networks need to be trained with large sets of labelled data.<font>\n",
    "\n",
    "- We initially assign random values to the weights and thresholds (for activation functions).\n",
    "- Training data are fed to the input layer.\n",
    "- Data passes through the succeeding layers, getting multiplied and added together in complex ways, until it finally arrives, radically transformed, at the output layer. \n",
    "- During training, the weights and thresholds are continually adjusted until training data with the same labels consistently yield similar outputs.\n",
    "\n",
    "- Most training methods consist of proposing an error function that measures the current performance of the network as a function of the  weights. \n",
    "- The goal of the method is to find the set of weights that minimise (or maximise) the function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Things to Consider</font>\n",
    "\n",
    "When designing a neural network, it is import to have the following in mind:\n",
    "\n",
    "- The input data should be representive of the available dataset and should be selected to avoid overfitting.\n",
    "- Properly select the **number of neurons** and **layers**:\n",
    "   - too few: the process becomes general. \n",
    "   - too many: the network may not perform well on data that are not part of the training set.\n",
    "- To determine the correct number of neurons to use in the hidden layers, consider as starting point, the three rules:\n",
    "   - The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "   - The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "   - The number of hidden neurons should be less than twice the size of the input layer.\n",
    "- For most problems, one could start with the following configuration:\n",
    "   1. The number of hidden layers equals one; and \n",
    "   2. The number of neurons in that layer is the mean of the neurons in the input and output layers.\n",
    "- **Activation Function**:  \n",
    "    - Determines the active state of the neuron. It decides whether the information received by the neuron is relevant or not.\n",
    "    - It transforms the input signal into the non-linear form and sent to the next layer as an input\n",
    "    - Start with the simplest and the use more complex ones if they improve the performance.\n",
    "- **Optimizer**:\n",
    "    - A mathematical algorithm that helps our loss function reach its convergence point with minimum delay.\n",
    "    -  The difference in predicted value and the real outcome is known as error. This error is used to update network weights and internal model parameters.\n",
    "    - Optimization is a process which tried to reduce the network error. \n",
    "    - Examples include, Stochastic Gradient Descent, Nesterov accelerated gradient, Adagrad, RMSProp, AdaDelta, Adam, etc.\n",
    "- **Loss Function**: \n",
    "    - Loss functions are quantitative measures of how satisfactory the model predictions are (i.e., how “good” the NN parameters are).\n",
    "the model parameters are).\n",
    "- <font color=\"red\">Keep track of your results with different network designs to see which configurations work better for your problem domain.</font>\n",
    "\n",
    "![fig_design](https://d2r55xnwy6nx47.cloudfront.net/uploads/2019/01/NeuralNetwok_560_rev.jpg)\n",
    "Image Source: Lucy Reading-Ikkanda/Quanta Magazine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Where are Neural Network Used?</font>\n",
    "\n",
    "- Neural networks are also ideally suited to help people solve complex problems in real-life situations. \n",
    "- Neural networks: \n",
    "   - Can learn and model the relationships between inputs and outputs that are nonlinear and complex.\n",
    "   - Make generalizations and inferences.\n",
    "   - Reveal hidden relationships, patterns and predictions; and \n",
    "   - Model highly volatile data (such as time series data) and variances needed to predict rare events (such as fraud detection).\n",
    "\n",
    "\n",
    "### Few Applications\n",
    "\n",
    "- [New Deep Learning Method Adds 301 Planets to Kepler's Total Count](https://www.nasa.gov/feature/ames/new-deep-learning-method-adds-301-planets-to-keplers-total-count)\n",
    "- [Artificial Intelligence Helps Improve NASA’s Eyes on the Sun](https://www.nasa.gov/feature/goddard/2021/artificial-intelligence-helps-improve-nasa-s-eyes-on-the-sun)\n",
    "- [Multi-Objective Reinforcement Learning–based\n",
    "Deep Neural Networks for Cognitive Space\n",
    "Communications](https://ntrs.nasa.gov/api/citations/20170007958/downloads/20170007958.pdf)\n",
    "- [Neural networks meet space](https://www.symmetrymagazine.org/article/neural-networks-meet-space)\n",
    "\n",
    "- [AI in space exploration](https://www.aiacceleratorinstitute.com/ai-in-space-exploration/)\n",
    "- [Ten Ways to Apply Machine Learning in Earth and Space Sciences](https://eos.org/opinions/ten-ways-to-apply-machine-learning-in-earth-and-space-sciences)\n",
    "- [Artificial Intelligence and Machine Learning for Earth Science](https://ntrs.nasa.gov/api/citations/20210019474/downloads/2021-07-31_AIST-AI-ML_for_ES-Short_Revised.pdf)\n",
    "- [MACHINE LEARNING PROTOTYPING: ML and AI Research](https://impact.earthdata.nasa.gov/project/ml.html)\n",
    "- [Advancing AI for Earth Science: A Data Systems Perspective](https://eos.org/science-updates/advancing-ai-for-earth-science-a-data-systems-perspective)\n",
    "- [Carbon Nanotube Gas Sensor Using Neural Networks](https://www.nas.nasa.gov/hecc/assets/pdf/papers/HECC_Data_Science_CNT_Neural_Network.pdf)\n",
    "- [Predicting Composition of Photo Voltaic Cells Using Neural Networks](https://www.nas.nasa.gov/hecc/assets/pdf/papers/HECC_Data_Science_PV_Cells.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Limitations of  Neural Network</font>\n",
    "\n",
    "#### Need lots of data: \n",
    "- Unlike the human brain, which can learn to do things with very few examples, neural networks require a huge ampunt of data, at least thousands if not millions of labeled samples.\n",
    "- The efficiency of any neural network is directly proportional to the amount of data it receives to process. \n",
    "\n",
    "#### Not good for generalizing \n",
    "- A neural network will perform accurately at a task it has been trained for, but very poorly at anything else, even if it’s similar to the original problem.  \n",
    "- Unlike humans, neural networks don’t develop knowledge in terms of symbols (ears, eyes, whiskers, tail)—they process values. \n",
    "- They will not be able to learn about new objects in terms of high-level features and they need to be retrained from scratch.\n",
    "\n",
    "#### Network Structure\n",
    "- There is no specific rule for determining the structure of artificial neural networks.\n",
    "- The appropriate network structure is achieved through experience and trial and error.\n",
    "\n",
    "#### Computational Resources\n",
    "- Require significant computational power (parallel processing) to be trained.\n",
    "\n",
    "#### Seen as `Black Boxes`\n",
    "- Since neural networks express their behavior in terms of neuron weights and activations, it is very hard to determine the logic behind their decisions. \n",
    "- It is hard to find out if they’re making decisions based on the wrong factors. They do not provide a clue as to why and how a solution was obtained.\n",
    "- While neural networks can, in principle, perform accurate predictions, it’s unlikely that we’ll obtain insights on the structure of a dataset through them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
