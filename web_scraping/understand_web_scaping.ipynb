{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1> <font color=\"red\" size=\"+3\">\n",
    "    Web Scraping with Python</font>\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_json](https://daveberesford.co.uk/wp-content/uploads/2019/02/data-scraping-960x594.png)\n",
    "Image Source: daveberesford.co.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "\n",
    "- [Web Scraping: What It Is and How to Use It](https://scrape-it.cloud/blog/web-scraping-what-it-is-and-how-to-use-it)\n",
    "- [What is web scraping](https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/)\n",
    "- [Python Requests Tutorial](http://zetcode.com/python/requests/)\n",
    "- [Python’s Requests Library (Guide](https://realpython.com/python-requests/)\n",
    "- [Download Files with Python](https://stackabuse.com/download-files-with-python/)\n",
    "- [Building a Web Scraper from start to finish](https://hackernoon.com/building-a-web-scraper-from-start-to-finish-bb6b95388184)\n",
    "- [Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup](https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/)\n",
    "- [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
    "- [HTTP status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Objectives</font>\n",
    "\n",
    "In this course, we want to describe web scraping and show how it can be accomplished with Python. We present the basic steps of web scraping and run examples on accessing HTTP servers, grabbing the content of web pages (in JSON and HTML formats), parsing the content to extract useful information and performing analyses.\n",
    "\n",
    "The following topics will be covered:\n",
    "\n",
    "+ What is web scraping?\n",
    "+ Components of a web page\n",
    "+ Accessing Web Pages with `requests`\n",
    "+ Web Scraping with `Json`\n",
    "+ Web Scraping with `Beautiful Soup`\n",
    "\n",
    "We expect that at end of this presentation, participants will be able to write Python scripts that automatically perform web scraping to extract specific data from webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Web Scraping</font>\n",
    "\n",
    "> Web scraping is a mechanism of collecting large amounts of data from a webpage and store the data into any required format which further helps us to perform analysis on the extracted data.\n",
    " \n",
    "- It can be an invaluable process for acquiring volumes of data from multiple sources, manipulating them and arranging them to be stored.\n",
    "- It is performed using a “**web-scraper**” (or a “bot” or a “web spider” or “web crawler”). \n",
    "    - A web-scraper is a program that goes to web pages, downloads the contents, extracts data out of the contents and then saves the data to a file or a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping involves a three-step process:\n",
    "\n",
    "1. **Step 1**: Send an HTTP request to the webpage\n",
    "   - The server responds to the request by returning the (JSON, HTML, etc.) content of the target webpage.\n",
    "2. **Step 2**: Parse the webpage content\n",
    "   - A parser is needed to create a nested structure of the data. \n",
    "3. **Step 3**: Pull out useful data out\n",
    "   - We use Python packages such as Json and Beautiful Soup to pull out data and store them.\n",
    "   \n",
    "![fig_scap](https://scrape-it.cloud/assets/cache_image/assets/blog_img/web-scraping-process2_760x0_073.webp)\n",
    "Image Source: [scrape-it.cloud](https://scrape-it.cloud/blog/web-scraping-what-it-is-and-how-to-use-it)\n",
    "     \n",
    "\n",
    "__Web Scrapers crawl websites, extracts data from it, transforms to a usable structured format and load it to a file or database for subsequent use.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Web Scraping Rules</font>\n",
    "\n",
    "Web scraping in itself is completely legal, though websites can set specific rules regarding the practice on its domain.\n",
    "\n",
    "![fig_ethics](https://hackernoon.com/hn-images/0*MPt2rectMhwklT63.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As reference, check: <a href=\"https://info.scrapinghub.com/web-scraping-guide/web-scraping-best-practices\">The Web Scraping Best Practices Guide</a> or watch the video <a href=\"https://www.youtube.com/watch?v=i7DEy-ZB_Lk\">Is Web Scraping Legal?</a>\n",
    "\n",
    "- Check a website’s Terms and Conditions before you scrape it.\n",
    "- Do not request data from the website too aggressively with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). \n",
    "  - One request for one webpage per second is good practice.\n",
    "  - Never scrape more frequently than you need to.\n",
    "  - Consider caching the content you scrape so that it’s only downloaded once.\n",
    "  - Build pauses into your code using functions like `time.sleep()` to keep from overwhelming servers with too many requests too quickly.\n",
    "- **<font color=\"red\">The layout of a website may change from time to time, so make sure to revisit the site and rewrite your web scraping code as needed.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Required Packages</font>\n",
    "We will need the three main Python packages:\n",
    "\n",
    "- `requests`: for accessing servers and getting the contents of web pages.\n",
    "- `json`: for manipulating JSON documents.\n",
    "- `BeautifupSoup`: for parsing the content of a HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Requests version:  {reqs.__version__}\")\n",
    "print(f\"JSON version:      {json.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Python `requests` Module</font>\n",
    "\n",
    "* Requests is a built-in Python module.\n",
    "* Requests is a simple and elegant Python HTTP (Hypertext Transfer Protocol) library. \n",
    "* It provides methods for accessing Web resources via HTTP. \n",
    "* The HTTP request returns a Response Object with all the response data (content, encoding, status, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading a Web Page**\n",
    "- We use the function `get()` to grab the content of a web page into an object.\n",
    "- We extract from the object the HTML content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get all information from the `resp` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the module `re` to strip all the HTML markups from the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = resp.text\n",
    "\n",
    "stripped_content = re.sub('<[^<]+?>', '', content)\n",
    "print(stripped_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you issue a request, `Requests` makes educated guesses about the encoding of the response based on the HTTP headers. \n",
    "- The text encoding guessed by `Requests` is used when you access `resp.text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Status of a Web Page\n",
    "- We perform an HTTP request with the `get()` method and check for the returned status code.\n",
    "- A status code informs you of the status of the request: if the request was successfull or not.\n",
    "- `200` is a standard response for a successful HTTP request and `404` tells that the requested resource could not be found.\n",
    "- By accessing `.status_code`, you can see the status code that the server returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = \"http://www.webcode.me/news\"\n",
    "resp = reqs.get(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert resp.status_code == 200, f\"Failed to fetch {my_url}, got {resp.status_code}\"\n",
    "#print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check first if a webpage is accessible\n",
    "\n",
    "We write a function to check first that a website is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_website(url):\n",
    "    try:\n",
    "        resp = reqs.get(url)\n",
    "        resp.raise_for_status()\n",
    "        print(f\"Valid url {url}\")\n",
    "    except reqs.exceptions.RequestException as errex:\n",
    "        print(f\"Invalid url: {url}\")\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\"\n",
    "resp = access_website(url)\n",
    "print(f\"Status Code: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me/news\"\n",
    "resp = access_website(url)\n",
    "print(f\"Status Code: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\"\n",
    "resp = access_website(url)\n",
    "if resp.status_code == 200:\n",
    "    print(f\"\\t URL:      {resp.url}\")\n",
    "    print(f\"\\t Encoding: {resp.encoding}\")\n",
    "    print(f\"\\t Time:     {resp.elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server Headers\n",
    "\n",
    "We can access the headers the server sent back to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = resp.headers\n",
    "for key in headers:\n",
    "    print(f\"{key:20} --> {headers[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the `get()` Method\n",
    "- The `get()` method issues a GET request to the server. \n",
    "- The GET method requests a representation of the specified resource.\n",
    "\n",
    "```python\n",
    "requests.get(url, params={key: value}, args)\n",
    "```\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | :--- |\n",
    "|`url` | (required) The url of the request |\n",
    "| `params` | (optional) A dictionary, list of tuples or bytes to send as a query string. |\n",
    "| `allow_redirects` | (optional) A Boolean to enable/disable redirection. |\n",
    "| `auth` | (optional) A tuple to enable a certain HTTP authentication. |\n",
    "| `cert` | (optional) A String or Tuple specifying a cert file or key. |\n",
    "| `cookies` | (optional) A dictionary of cookies to send to the specified url. |\n",
    "| `headers` | (optional) A dictionary of HTTP headers to send to the specified url. |\n",
    "| `proxies` | (optional) A dictionary of the protocol to the proxy url. |\n",
    "| `stream` | (optional) A Boolean indication if the response should be immediately downloaded (False) or streamed (True). |\n",
    "| `timeout` | (optional) A number, or a tuple, indicating how many seconds to wait for the client to make a connection and/or send a response. |\n",
    "| `verify` | (optional) A Boolean or a String indication to verify the servers TLS certificate or not. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sending Parmeters in URL\n",
    "\n",
    "- We often ant to send some sort of data in the URL’s query string.\n",
    "- The `get()` method takes a `params` keyword argument where we can specify the query parameters.\n",
    "     - The beginning of the query parameters is denoted by a question mark (`?`).\n",
    "     - The pieces of information constituting one query parameter are encoded in key-value pairs, where related keys and values are joined together by an equals sign (`key=value`).\n",
    "     - Every URL can have multiple query parameters, which are separated from each other by an ampersand (`&`)\n",
    "\n",
    "If:\n",
    "\n",
    "```python\n",
    "   {'key1': value1, 'key2': value2, 'key3': value3}\n",
    "```\n",
    "is the dictionary of the parameters, and `https://MyOwnWebsite.com/` is the base url, then the final url to access will be:\n",
    "\n",
    "```\n",
    "    https://MyOwnWebsite.com/?key1=value1&key2=value2&key3=value3\n",
    "```\n",
    "\n",
    "The code to reach the webpage is:\n",
    "\n",
    "```Python\n",
    "payload = {'key1': value1, 'key2': value2, 'key2': value3}\n",
    "resp = reqs.get(\"https://MyOwnWebsite.com\", params=payload)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script sends a variable with a value to the `httpbin.org` server. The variable is specified directly in the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"https://httpbin.org/get?name=Peter\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We send a `get()` request to the web site and pass the data, which is specified in the `params` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': 'Peter'}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`payload` is a dictionary of pairs of keys/values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': 'Peter', 'age': 23}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass a list of items as a value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': ['Peter', 'Johns'], 'age': 23}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Type\n",
    "\n",
    "- It is part of the HTTP header.\n",
    "   - A string used to indicate the media type of the resource you want to access.  \n",
    "   - It tells the browser the type of content it has to load on the machine. \n",
    "- Here are some values of `content-type`:\n",
    "\n",
    "```html\n",
    "   text/html\n",
    "   text/csv\n",
    "   application/json\n",
    "   application/javascript\n",
    "   audio/ogg\n",
    "   image/png\n",
    "```\n",
    "\n",
    "- While scraping a web page, it is important to determine the content type first before to choosing the right tool needed to parse the content of a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content Type: \\n\\t {resp.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisiting the function to accesss a web page\n",
    "\n",
    "We want to pass the `payload` and `timeout` as arguments of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_website(url: str, payload: dict=None, timeout: int=10):\n",
    "    \"\"\"\n",
    "    Attempt to access a server. If the attempt is successful,\n",
    "    return the response object, otherwise return an error message.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "       HTTP address of the web page we want to access\n",
    "    payload : dict\n",
    "       Parameters needed to construct the target url.\n",
    "    timeout: int\n",
    "       Maximum time (in seconds) to access a website and grab its content.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    resp : object\n",
    "       Object which has infomation on the web page of interest.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if payload:\n",
    "            resp = reqs.get(url, params=payload, timeout=timeout)\n",
    "        else:\n",
    "            resp = reqs.get(url, timeout=timeout)\n",
    "        resp.raise_for_status()\n",
    "    except reqs.exceptions.HTTPError as errh:\n",
    "        print(f\"Http Error: {errh}\")\n",
    "    except reqs.exceptions.ConnectionError as errc:\n",
    "        print(f\"Error Connecting: {errc}\")\n",
    "    except reqs.exceptions.Timeout as errt:\n",
    "        print(f\"Timeout Error: {errt}\")\n",
    "    except reqs.exceptions.RequestException as err:\n",
    "        print(f\"General Error: {err}\")\n",
    "    else:\n",
    "        print(f\"Successfully accessed the site: \\n\\t {resp.url}\")\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Web Scraping with JSON</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">NYC - Citi Bike System Data </font>\n",
    "\n",
    "- Citi Bike is the nation's largest bikeshare program, with 25,000 bikes and over 1,500 stations across Manhattan, Brooklyn, Queens, the Bronx, Jersey City, and Hoboken.\n",
    "- Citi Bike bikes can be unlocked from one station and returned to any other station in the system.\n",
    "- Citi Bike maintains a [database of bike trip histories](https://citibikenyc.com/system-data):\n",
    "   - Where do Citi Bikers ride?\n",
    "   - When do they ride? How far do they go?\n",
    "   - Which stations are most popular?\n",
    "   - What days of the week are most rides taken on? \n",
    "- Here we read data from the bike sharing system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get JSON string data from CitiBike NYC__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_url = \"https://gbfs.citibikenyc.com/gbfs/en/station_status.json\"\n",
    "bike_url = \"https://gbfs.citibikenyc.com/gbfs/2.3/gbfs.json\"\n",
    "bike_url = \"https://gbfs.lyft.com/gbfs/2.3/bkn/en/station_status.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = access_website(bike_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Verify the status code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check the content type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content Type: \\n\\t {json_response.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check type of json_response object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(json_response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using the `loads()` function, convert the JSON object into a Python object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dict = json.loads(json_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspect the Python object__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(bike_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bike_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dict['last_updated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dict['ttl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dict['version']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Record data in a Pandas DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bike_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bike_dict['data'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bike_dict['data']['stations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bike_dict['data']['stations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_dict['data']['stations'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = list(bike_dict['data']['stations'][0].keys())\n",
    "list_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=list_columns)\n",
    "for item in bike_dict['data']['stations']:\n",
    "    df.loc[len(df)] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Number of stations with at least one available dock__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df[df['num_docks_available'] > 0])} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Number of stations with at least one available ebike__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(df[df['num_ebikes_available'] > 0])} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Total number of disabled ebikes at stations__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_bikes_disabled'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Further analysis__\n",
    "\n",
    "To do a more comprehensive analysis, you may want to start with the JSON document:\n",
    "\n",
    "[https://gbfs.citibikenyc.com/gbfs/2.3/gbfs.json](https://gbfs.citibikenyc.com/gbfs/2.3/gbfs.json)\n",
    "\n",
    "It contains links to other JSON documents that can be combined to learn how the bike share system is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Scraping the NASA Astronomy Picture Of the Day (APOD) Webpage </font>\n",
    "\n",
    "- We want to be able to obtain from the webpage <a href=\"https://api.nasa.gov/planetary/apod\"> https://api.nasa.gov/planetary/apod</a>,  the Astronomy picture of the day for a given day and plot the image.\n",
    "- We access the webpage (using a set of parameters) and retrieve the content of the page as a JSON object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Query Parameters**\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "| --- | --- | --- | --- |\n",
    "|`date` | YYYY-MM-DD | today | Date of the APOD image to retrieve |\n",
    "|`start_date` | YYYY-MM-DD | none | The start of a date range, when requesting date for a range of dates. Cannot be used with `date`. |\n",
    "|`end_date` | YYYY-MM-DD | today | The end of the date range, when used with `start_date`. |\n",
    "| `count` |\tint\t| none\t| If this is specified then count randomly chosen images will be returned. Cannot be used with `date` or `start_date` and `end_date`. |\n",
    "| `hd` | bool | False | Retrieve the URL for the high resolution image |\n",
    "| `api_key` | string | DEMO_KEY | <a href=\"https://api.nasa.gov/\">[https://api.nasa.gov/</a> key for expanded usage |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get today's date__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.datetime.today()\n",
    "date = today.strftime(\"%Y-%m-%d\")\n",
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Access the webpage__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.nasa.gov/planetary/apod\"\n",
    "payload = {'api_key': \"DEMO_KEY\",\n",
    "          'date': date,\n",
    "          'hd': True}\n",
    "\n",
    "page_content = access_website(url, payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get the url__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"URL: \\n\\t {page_content.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Determine the content type__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content type: \\n\\t {page_content.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Process the data with JSON__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_page = json.loads(page_content.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APOD variable is a dictionary of various keys and values. Let’s take a look at the keys of this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the keys and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(f\"{x} --> {json_page[x]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot images__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if json_page[\"media_type\"] == \"image\":\n",
    "    io.imshow(io.imread(json_page[\"url\"]))\n",
    "    plt.title(json_page[\"title\"])\n",
    "    io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">If you want to download the file on your local system:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_name = json_page[\"url\"]\n",
    "loc_file_name = os.path.basename(url_name)\n",
    "\n",
    "urllib.request.urlretrieve(url_name, loc_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to view the image through a browser, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "def window_open(url):\n",
    "    display(Javascript('window.open(\"{url}\");'.format(url=url)))\n",
    "    \n",
    "window_open(json_page['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Obtaining Mars Rover Photos</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rover_url = 'https://api.nasa.gov/mars-photos/api/v1/rovers/curiosity/photos'\n",
    "\n",
    "payload = {'api_key': \"DEMO_KEY\",\n",
    "           'sol': 1000}\n",
    "\n",
    "response = access_website(rover_url, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"URL: \\n\\t {response.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content type: \\n\\t {response.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"KEYS: \\n\\t {response_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = response_dict['photos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(photos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(photos)} photos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(photos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extract the URL of each photo__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_photos = list()\n",
    "for photo in photos:\n",
    "    url_photos.append(photo['img_src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(url_photos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(url_photos) == len(photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Randomly select 9 photos__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pictures = random.sample(url_photos, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Display the 9 photos__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(20, 20))\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(9):\n",
    "    ax[i].imshow(io.imread(url_pictures[i]))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"purple\">Breakout 1</font>\n",
    "\n",
    "Use the following code to list all the images in the provided range of years:\n",
    "\n",
    "```python\n",
    "url = \"https://images-api.nasa.gov/search\"\n",
    "\n",
    "payload = {\n",
    "        \"q\": \"apollo\",\n",
    "        \"page\": \"1\",\n",
    "        \"media_type\": \"image\",\n",
    "        \"year_start\": \"2020\",\n",
    "        \"year_end\": \"2024\"}\n",
    "\n",
    "response = reqs.get(url, params=payload)\n",
    "images = response.json()[\"collection\"][\"items\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "import requests as reqs\n",
    "\n",
    "url = \"https://images-api.nasa.gov/search\"\n",
    "\n",
    "params = {\n",
    "    \"q\": \"apollo\",\n",
    "    \"page\": \"1\",\n",
    "    \"media_type\": \"image\",\n",
    "    \"year_start\": \"2020\",\n",
    "    \"year_end\": \"2024\"\n",
    "}\n",
    "\n",
    "response = reqs.get(url, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "images = response.json()[\"collection\"][\"items\"]\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "for image in images:\n",
    "    thumbnail_url = image[\"links\"][0][\"href\"]\n",
    "    image_url = thumbnail_url[:thumbnail_url.rfind(\"~\")] + \"~orig.jpg\"\n",
    "    print(image_url)\n",
    "``` \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Web Scraping with Beautiful Soup</font>\n",
    "\n",
    "- Web scraping allows you to download the HTML of a website and extract the data that you need.\n",
    "- Beautiful Soup is a Python library for scraping data from websites.\n",
    "- Beautiful Soup creates a parse tree from parsed HTML and XML documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Parsing a HTML document</font>\n",
    "\n",
    "HTML is made up of <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element\">tags</a>. It stores all of it’s data in them, and in the midst of all that clutter lies the data we need. \n",
    "\n",
    "Some of the tags are:\n",
    "\n",
    "* `head` - contains machine-readable information (metadata) about the document, like its title, scripts, and style sheets.\n",
    "* `body` - represents the content of an HTML document. There can be only one `<body>` element in a document.\n",
    "* `title` - defines the document's title that is shown in a Browser's title bar or a page's tab. \n",
    "* `p` - for paragraph\n",
    "* `div` — indicates a division, or area, of the page.\n",
    "* `b` — bolds any text inside.\n",
    "* `i` — italicizes any text inside.\n",
    "* `table` — creates a table.\n",
    "* `form` — creates an input form.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\"\n",
    "source = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content type: \\n\\t {source.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a beautiful soup object by using the html parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the the HTML content of the page using the `prettify` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the title section of the page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get attribute name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get attribute values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beginning navigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting specific tags**\n",
    "- The `find` method searches for the first tag with the needed name.\n",
    "- The `find_all` method searches for all tags with the needed tag name and returns them as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we want to find paragraph tags `<p>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mysoup.find('p')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mysoup.find_all('p')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the last paragraph only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paragraph in enumerate(mysoup.find_all('p'), start=1):\n",
    "    print(f\"Paragraph Text {i}: {paragraph.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = mysoup.find_all('body')\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Type body:       {type(body)}\")\n",
    "print(f\"Type inner body: {type(body[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(body[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('body')[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab the text**\n",
    "\n",
    "- Use the method `get_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching for tags by `class` and `id`**\n",
    "\n",
    "<a href=\"https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors\">CSS (Cascading Style Sheets)</a> is a declarative language that controls how webpages look in the browser. \n",
    "- The browser applies CSS style declarations to selected elements to display them properly. \n",
    "- A style declaration contains the properties and their values, which determine how a webpage looks.\n",
    "- Classes and ids are used by CSS to determine which HTML elements to apply certain styles to. \n",
    "- We can also use them when scraping to select specific elements we want to scrape.\n",
    "\n",
    "\n",
    "We can use the `find_all` method to search for items by `class` or by `id`. \n",
    "\n",
    "```python\n",
    "mysoup.find_all(\"html_tag\", class_=\"your_class_name\")\n",
    "mysoup.find_all(class_=\"your_class_name\")\n",
    "\n",
    "mysoup.find_all(\"html_tag\", id=\"your_id_name\")\n",
    "mysoup.find_all(id=\"your_id_name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\"\n",
    "source = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, we’ll search for any `p` tag that has the class `outer-text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for any tag that has the class `outer-text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")[-1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")[-1].get_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for elements by id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(id=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(id=\"first\")[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using CSS Selectors**\n",
    "\n",
    "CSS selectors</a> are how the CSS language allows developers to specify HTML tags to style. \n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "- `p a` — finds all `a` tags inside of a `p` tag.\n",
    "- `body p a` — finds all `a` tags inside of a `p` tag inside of a `body` tag.\n",
    "- `html body` — finds all `body` tags inside of an `html` tag.\n",
    "- `p.outer-text` — finds all `p` tags with a class of `outer-text`.\n",
    "- `p#first` — finds all `p` tags with an id of `first`.\n",
    "- `body p.outer-text` — finds any `p` tags with a class of `outer-text` inside of a `body` tag.\n",
    "\n",
    "We can use the CSS selectors to search items inside webpages. `BeautifulSoup` objects support searching a page via CSS selectors using the `select` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `p` tags in our page that are inside of a `body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"body p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `b` tags in our page that are inside of a `p`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"p b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `b` tags inside of a `p` tag inside of a `body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"body p b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `p` tags with an id of `first`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select('p#first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> Example: Extract the web link of the Astronomy Picture of the Day</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://apod.nasa.gov/apod/astropix.html\"\n",
    "source = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print basic information of the Image of the Day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('p').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_comments = mysoup.find_all('a')\n",
    "for a in href_comments:\n",
    "    print(a.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">__The `Picture of the Day` can either be a picture or a video.__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_day = \"picture\"\n",
    "if mysoup.iframe:\n",
    "    print(\"We have a video.\")\n",
    "    picture_day = \"video\"\n",
    "else:\n",
    "    print(\"We have a picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"video\":\n",
    "    HTML(str(mysoup.iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"video\":\n",
    "    mysoup.iframe['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"video\":\n",
    "    src_list = [a['src'] for a in mysoup.select('iframe[src]')]\n",
    "    print(src_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `src` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tags = mysoup.find_all(src=True)\n",
    "src_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `href` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_tags = mysoup.find_all(href=True)\n",
    "href_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all valid urls in `a` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_with_text = [a['href'] for a in mysoup.find_all('a', href=True) if a.text]\n",
    "links_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list1 = [a['href'] for a in mysoup.find_all('a', href=True)]\n",
    "link_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list2 = [l.get('href') for l in mysoup.find_all('a')]\n",
    "link_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list3 = [a['href'] for a in mysoup.select('a[href]')]\n",
    "link_list3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Picture of the Day is an image instead (not a video), the following can help us view the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"picture\":\n",
    "    url_image = \"\".join([\"https://apod.nasa.gov/apod/\", link_list3[1]])\n",
    "    fig, axes = plt.subplots(figsize=(10, 8))\n",
    "    axes.imshow(io.imread(url_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"purple\">Breakout 2</font>\n",
    "\n",
    "Go to the webpage:\n",
    "\n",
    "[https://astg606.github.io/py_courses/summer_2022/](https://astg606.github.io/py_courses/summer_2022/)\n",
    "\n",
    "and extract the `Course Evaluation` web link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import requests as reqs\n",
    "\n",
    "from bs4 import BeautifulSoup as bso\n",
    "\n",
    "URL = \"https://astg606.github.io/py_courses/summer_2022/\"\n",
    "\n",
    "source = reqs.get(URL)\n",
    "if source.status_code == 200:\n",
    "    mysoup = bso(source.content, 'html.parser')\n",
    "    href_tags = mysoup.find_all(href=True)\n",
    "    for tag in href_tags:\n",
    "        if tag.get_text() == \"Course Evaluation\":\n",
    "            print(tag[\"href\"])\n",
    "else:\n",
    "    print(\"URL not accessible.\")\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"purple\">Breakout 3</font>\n",
    "\n",
    "Go to the webpage:\n",
    "\n",
    "[https://mars.nasa.gov/news/](https://mars.nasa.gov/news/)\n",
    "\n",
    "and extract the latest `News Title` and associated `Paragraph Text`.\n",
    "\n",
    "This exercise was inspired from: [Mars Information Scraper](https://pyligent.github.io/2019-01-26-Mars-Information-Scraper/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "mars_news_url = 'https://mars.nasa.gov/news/'\n",
    "mars_news_source = access_website(mars_news_url)\n",
    "mars_news_bs = bso(mars_news_source.text, 'html.parser')\n",
    "\n",
    "#print(mars_news_bs.prettify())\n",
    "\n",
    "mars_news_titles = [a['title'] for a in mars_news_bs.find_all('a', title=True) if not a.text]\n",
    "mars_news_paras = [a.text for a in mars_news_bs.find_all(class_=\"margin-top-0 margin-bottom-1\")]\n",
    "\n",
    "assert len(mars_news_titles) == len(mars_news_paras)\n",
    "\n",
    "for i in range(len(mars_news_titles)):\n",
    "    pprint.pprint(mars_news_titles[i])\n",
    "    pprint.pprint(mars_news_paras[i])\n",
    "    print()\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Parsing a XML document</font>\n",
    "\n",
    "- Extensible Markup Language (XML) is a markup language similar to HTML, but it is primarily used to store, transport, and exchange data rather than solely for creating web pages.\n",
    "- XML provides the flexibility to define custom tags to describe data in a specific manner.\n",
    "- XML uses markup symbols (tags) to provide more information about any data.\n",
    "- Tags are crucial in defining data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing a XML document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample XML Document__\n",
    "\n",
    "- The elements in an XML document form a document tree. The tree starts at the root and branches to the lowest level of the tree.\n",
    "- The first line describes the root element of the document: `<nasa_centers>`\n",
    "- The next set of lines describes two child elements (`<center>` `</center>`) of the root, and each of the elements has 3 subelements (`name`, `state`, `location`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```xml\n",
    "<nasa_centers>\n",
    "    <center>\n",
    "        <name>Goddard Space Flight Center</name>\n",
    "        <state>Maryland</state>\n",
    "        <location>8800 Greenbelt Road, Greenbelt</location>\n",
    "    </center>\n",
    "    <center>\n",
    "        <name>Stennis Space Center</name>\n",
    "        <state>Mississipi</state>\n",
    "        <location>John C. Stennis Space Center</location>\n",
    "    </center>\n",
    "</nasa_centers>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parsing with BeautifulSoup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = \"\"\" \n",
    "<nasa_centers>\n",
    "    <center>\n",
    "        <name>Goddard Space Flight Center</name>\n",
    "        <state>Maryland</state>\n",
    "        <location>8800 Greenbelt Road, Greenbelt</location>\n",
    "    </center>\n",
    "    <center>\n",
    "        <name>Stennis Space Center</name>\n",
    "        <state>Mississipi</state>\n",
    "        <location>John C. Stennis Space Center</location>\n",
    "    </center>\n",
    "</nasa_centers>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc_bs = bso(xmldoc, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xmldoc_bs.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the root element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc_root = xmldoc_bs.find_all('nasa_centers')\n",
    "xmldoc_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc_root[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data from subelements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc_centers = xmldoc_bs.find_all('center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for center in xmldoc_centers:\n",
    "    for tag in center.find_all():\n",
    "        print(f\"{tag.name}: {tag.text}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\"> NASA held and pending patents </font>\n",
    "\n",
    "- We want to grab the content of a dataset that has information pertaining to NASA held and pending patents.\n",
    "- We will dat save the content in a Pandas DataFRame for analyis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_url = \"https://data.nasa.gov/api/views/gquh-watm/rows.xml?accessType=DOWNLOAD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_source = access_website(patent_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_source.headers['Content-Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_bs = bso(patent_source.text, 'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patent_bs.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get all the rows in the XML document__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_rows = patent_bs.find_all('row')\n",
    "num_rows = len(patent_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get the column names__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'center',\n",
    "    'status',\n",
    "    'case_number',\n",
    "    'patent_number',\n",
    "    'application_sn',\n",
    "    'title',\n",
    "    'patent_expiration_date'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns = [tag.name for tag in patent_rows[2].find_all()]\n",
    "#columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Loop over the rows to extract elements and populate a Pandas DataFrame__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for row in patent_rows:\n",
    "    item = dict()\n",
    "    for key in columns:\n",
    "        try:\n",
    "            item[key] = row.find(key).text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    df.loc[len(df)] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for row in patent_rows:\n",
    "    item = dict()\n",
    "    for tag in row.find_all():\n",
    "        item[tag.name] = tag.text\n",
    "\n",
    "    df.loc[len(df)] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Group data by centers__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_center = df.groupby('center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_center.ngroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_groups = list(df_center.groups.keys())\n",
    "list_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_center.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_center.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.pie(df_center.count(), values=\"status\", names=list_groups)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_center.get_group('NASA Goddard Space Flight Center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
