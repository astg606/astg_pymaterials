{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/nasa-logo.svg\" width=\"100\"/> </td>\n",
    "     <td><img src=\"https://portal.nccs.nasa.gov/datashare/astg/training/python/logos/ASTG_logo.png?raw=true\" width=\"80\"/> </td>\n",
    "     <td> <img src=\"https://www.nccs.nasa.gov/sites/default/files/NCCS_Logo_0.png\" width=\"130\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "</center>\n",
    "\n",
    "        \n",
    "<center>\n",
    "<h1><font color= \"blue\" size=\"+3\">ASTG Python Courses</font></h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "<CENTER>\n",
    "<H1> <font color=\"red\" size=\"+3\">\n",
    "    Web Scraping with Python</font>\n",
    "</H1>\n",
    "</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Reference Documents</font>\n",
    "\n",
    "- [Web Scraping: What It Is and How to Use It](https://scrape-it.cloud/blog/web-scraping-what-it-is-and-how-to-use-it)\n",
    "- [What is web scraping](https://www.scrapehero.com/a-beginners-guide-to-web-scraping-part-1-the-basics/)\n",
    "- [Python Requests Tutorial](http://zetcode.com/python/requests/)\n",
    "- [Python’s Requests Library (Guide](https://realpython.com/python-requests/)\n",
    "- [Download Files with Python](https://stackabuse.com/download-files-with-python/)\n",
    "- [Building a Web Scraper from start to finish](https://hackernoon.com/building-a-web-scraper-from-start-to-finish-bb6b95388184)\n",
    "- [Ultimate Guide to Web Scraping with Python Part 1: Requests and BeautifulSoup](https://www.learndatasci.com/tutorials/ultimate-guide-web-scraping-w-python-requests-and-beautifulsoup/)\n",
    "- [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Objectives</font>\n",
    "\n",
    "In this course, we want to describe web scraping and show how it can be accomplished with Python. We present the basic steps of web scraping and run examples on accessing HTTP servers, grabbing the content of web pages (in JSON and HTML formats), parsing the content to extract useful information and performing analyses.\n",
    "\n",
    "The following topics will be covered:\n",
    "\n",
    "+ What is web scraping?\n",
    "+ Components of a web page\n",
    "+ Accessing Web Pages with `requests`\n",
    "+ Web Scraping with `Json`\n",
    "+ Web Scraping with `Beautiful Soup`\n",
    "\n",
    "We expect that at end of this presentation, participants will be able to write Python scripts that automatically perform web scraping to extract specific data from webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Web Scraping</font>\n",
    "\n",
    "> Web scraping is a mechanism of collecting large amounts of data from a webpage and store the data into any required format which further helps us to perform analysis on the extracted data.\n",
    "\n",
    "\n",
    "- It is used to extract or “scrape” data from any web page on the Internet.\n",
    "- It is performed using a “**web-scraper**” (or a “bot” or a “web spider” or “web crawler”). \n",
    "    - A web-scraper is a program that goes to web pages, downloads the contents, extracts data out of the contents and then saves the data to a file or a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_json](https://daveberesford.co.uk/wp-content/uploads/2019/02/data-scraping-960x594.png)\n",
    "Image Source: daveberesford.co.uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web scraping involves a three-step process:\n",
    "\n",
    "1. **Step 1**: Send an HTTP request to the webpage\n",
    "   - The server responds to the request by returning the (JSON, HTML, etc.) content of the target webpage.\n",
    "2. **Step 2**: Parse the webpage content\n",
    "   - A parser is needed to create a nested structure of the data. \n",
    "3. **Step 3**: Pull out useful data out\n",
    "   - We use Python packages such as Json and Beautiful Soup to pull out data and store them.\n",
    "   \n",
    "![fig_scap](https://scrape-it.cloud/assets/cache_image/assets/blog_img/web-scraping-process2_760x0_073.webp)\n",
    "Image Source: [scrape-it.cloud](https://scrape-it.cloud/blog/web-scraping-what-it-is-and-how-to-use-it)\n",
    "     \n",
    "\n",
    "__Web Scrapers crawl websites, extracts data from it, transforms to a usable structured format and load it to a file or database for subsequent use.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_scrap](https://miro.medium.com/max/1400/1*4BnBQE9Bu-EQ-gGz25x8pg.png)\n",
    "Image Source: gurutechnolabs.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Web Scraping Rules</font>\n",
    "\n",
    "![fig_ethics](https://hackernoon.com/hn-images/0*MPt2rectMhwklT63.jpg)\n",
    "\n",
    "As reference, check: <a href=\"https://info.scrapinghub.com/web-scraping-guide/web-scraping-best-practices\">The Web Scraping Best Practices Guide</a> or watch the video <a href=\"https://www.youtube.com/watch?v=i7DEy-ZB_Lk\">Is Web Scraping Legal?</a>\n",
    "\n",
    "- Check a website’s Terms and Conditions before you scrape it.\n",
    "- Do not request data from the website too aggressively with your program (also known as spamming), as this may break the website. Make sure your program behaves in a reasonable manner (i.e. acts like a human). \n",
    "  - One request for one webpage per second is good practice.\n",
    "  - Never scrape more frequently than you need to.\n",
    "  - Consider caching the content you scrape so that it’s only downloaded once.\n",
    "  - Build pauses into your code using functions like `time.sleep()` to keep from overwhelming servers with too many requests too quickly.\n",
    "- **<font color=\"red\">The layout of a website may change from time to time, so make sure to revisit the site and rewrite your web scraping code as needed.</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Components of a Web Page</font>\n",
    "\n",
    "A web page typically has the following components:\n",
    "\n",
    "- [HTML (HyperText Markup Language)](https://en.wikipedia.org/wiki/HTML): The main content of the page.\n",
    "   - Is the language that web pages are created in.\n",
    "   - It’s a markup language that tells a browser how to display content.\n",
    "- [CSS (Casdading Style Sheets)](https://skillcrush.com/blog/css/): Used to add styling to make the page look nicer and modern.\n",
    "- [JS (Javascript)](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/First_steps/What_is_JavaScript): Adds interactivity to web pages.\n",
    "- Images: image formats, such as JPG and PNG, allow web pages to show pictures.\n",
    "\n",
    "![webpage](https://wp.brytdesigns.com/wp-content/uploads/2019/12/html_css_javascript_infographic.png)\n",
    "Image Source: [brytdesigns.com](https://brytdesigns.com/html-css-javascript-whats-the-difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the content of a [sample](https://www.freecodecamp.org/news/html-css-and-javascript-explained-for-beginners/) web page:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "  <meta charset=\"UTF-8\">\n",
    "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "  <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n",
    "  <link rel=\"stylesheet\" href=\"./styles.css\">\n",
    "  <title>Document</title>\n",
    "</head>\n",
    "<body>\n",
    "  <h1>This is a first level heading in HTML. With CSS, I will turn this into red color</h1>\n",
    "  <h2>This is a second level heading in HTML. With CSS, I will turn this into blue color</h2>\n",
    "  <h3>This is a third level heading in HTML. With CSS, I will turn this into green color</h3>\n",
    "  <p>This is a <em>paragragh</em> As you can see, I placed an empahisis on the word \"paragraph\". Now, I will change also\n",
    "    the background color of the word \"paragraph\" to black, and its text color  to green, all with just CSS.</p>\n",
    "  <p>The main essence of this tutorial is to:</p>\n",
    "    <ul>\n",
    "       <li>Show you how to format a web document with HTML</li>\n",
    "       <li>Show you how to design a web page with CSS</li>\n",
    "       <li>Show you how to program a web document with JavaScript</li>\n",
    "    </ul>\n",
    "\n",
    "  <p>Next, I am going to add the following two numbers and display the result, all with JavaScript<p/>\n",
    "    <p>First number:<span id= \"firstNum\">2</span> <br></p>\n",
    "    <p>Second number: <span id= \"secondNum\">7</span> </p>\n",
    "    <p>Therefore, the sum of the two of those numbers is: <span id= \"answer\">(placeholder for the answer)</span></p>\n",
    "    <input type=\"button\" id=\"sumButton\" value=\"Click to add!\">\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Required Packages</font>\n",
    "We will need the three main Python packages:\n",
    "\n",
    "- `requests`: for accessing servers and getting the contents of web pages.\n",
    "- `json`: for manipulating JSON documents.\n",
    "- `BeautifupSoup`: for parsing the content of a HTML document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Requests version:  {reqs.__version__}\")\n",
    "print(f\"JSON version:      {json.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'>Python `requests` Module</font>\n",
    "\n",
    "* Requests is a built-in Python module.\n",
    "* Requests is a simple and elegant Python HTTP (Hypertext Transfer Protocol) library. \n",
    "* It provides methods for accessing Web resources via HTTP. \n",
    "* The HTTP request returns a Response Object with all the response data (content, encoding, status, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading a Web Page**\n",
    "- We use the function `get()` to grab the content of a web page into an object.\n",
    "- We extract from the object the HTML content of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get all information from the `resp` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the module `re` to strip all the HTML markups from the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "\n",
    "content = resp.text\n",
    "\n",
    "stripped = re.sub('<[^<]+?>', '', content)\n",
    "print(stripped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you issue a request, `Requests` makes educated guesses about the encoding of the response based on the HTTP headers. \n",
    "- The text encoding guessed by `Requests` is used when you access `resp.text`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can find out what encoding `Requests` is using, and change it.\n",
    "- If you change the encoding, `Requests` will use the new value of `resp.encoding` whenever you call `resp.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the Status of a Web Page\n",
    "- We perform an HTTP request with the `get()` method and check for the returned status code.\n",
    "- A status code informs you of the status of the request: if the request was successfull or not.\n",
    "- `200` is a standard response for a successful HTTP request and `404` tells that the requested resource could not be found.\n",
    "- By accessing `.status_code`, you can see the status code that the server returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"http://www.webcode.me/news\")\n",
    "print(resp.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for checking if a website is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_website(url):\n",
    "    \"\"\"\n",
    "    Attempt to access a server. If the attempt is successful,\n",
    "    return the response object, otherwise return an error message.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "       HTTP address of the web page we want to access\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    resp : object\n",
    "       Object which has infomation on the web page of interest.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = reqs.get(url, timeout=3)\n",
    "        resp.raise_for_status()\n",
    "    except reqs.exceptions.HTTPError as errh:\n",
    "        print(f\"Http Error: {errh}\")\n",
    "    except reqs.exceptions.ConnectionError as errc:\n",
    "        print(f\"Error Connecting: {errc}\")\n",
    "    except reqs.exceptions.Timeout as errt:\n",
    "        print(f\"Timeout Error: {errt}\")\n",
    "    except reqs.exceptions.RequestException as err:\n",
    "        print(f\"General Error: {err}\")\n",
    "    else:\n",
    "        print(\"Successfully accessed the site!\")\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me/news\"\n",
    "resp = access_website(url)\n",
    "print(f\"Type of returned value: {type(resp)}\")\n",
    "if not isinstance(resp, str):\n",
    "    print(f\"Status Code: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\"\n",
    "resp = access_website(url)\n",
    "print(f\"Type of returned value: {type(resp)}\")\n",
    "if not isinstance(resp, str):\n",
    "    print(f\"Status Code: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(resp, str):\n",
    "    print(f\"Status Code: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\"\n",
    "resp = access_website(url)\n",
    "if resp.status_code == 200:\n",
    "    print(f\"\\t URL:      {resp.url}\")\n",
    "    print(f\"\\t Encoding: {resp.encoding}\")\n",
    "    print(f\"\\t Time:     {resp.elapsed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Server Headers\n",
    "\n",
    "We can access the headers the server sent back to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = resp.headers\n",
    "for key in headers:\n",
    "    print(f\"{key:20} --> {headers[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More on the `get()` Method\n",
    "- The `get()` method issues a GET request to the server. \n",
    "- The GET method requests a representation of the specified resource.\n",
    "\n",
    "```python\n",
    "requests.get(url, params={key: value}, args)\n",
    "```\n",
    "\n",
    "| Parameter | Description |\n",
    "| --- | :--- |\n",
    "|`url` | (required) The url of the request |\n",
    "| `params` | (optional) A dictionary, list of tuples or bytes to send as a query string. |\n",
    "| `allow_redirects` | (optional) A Boolean to enable/disable redirection. |\n",
    "| `auth` | (optional) A tuple to enable a certain HTTP authentication. |\n",
    "| `cert` | (optional) A String or Tuple specifying a cert file or key. |\n",
    "| `cookies` | (optional) A dictionary of cookies to send to the specified url. |\n",
    "| `headers` | (optional) A dictionary of HTTP headers to send to the specified url. |\n",
    "| `proxies` | (optional) A dictionary of the protocol to the proxy url. |\n",
    "| `stream` | (optional) A Boolean indication if the response should be immediately downloaded (False) or streamed (True). |\n",
    "| `timeout` | (optional) A number, or a tuple, indicating how many seconds to wait for the client to make a connection and/or send a response. |\n",
    "| `verify` | (optional) A Boolean or a String indication to verify the servers TLS certificate or not. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sending Parmeters in URL\n",
    "\n",
    "- We often ant to send some sort of data in the URL’s query string.\n",
    "- The `get()` method takes a `params` keyword argument where we can specify the query parameters.\n",
    "     - The beginning of the query parameters is denoted by a question mark (`?`).\n",
    "     - The pieces of information constituting one query parameter are encoded in key-value pairs, where related keys and values are joined together by an equals sign (`key=value`).\n",
    "     - Every URL can have multiple query parameters, which are separated from each other by an ampersand (`&`)\n",
    "\n",
    "If:\n",
    "```python\n",
    "   {'key1': value1, 'key2': value2, 'key2': value3}\n",
    "```\n",
    "is the dictionary of the parameters, and `https://MyOwnWebsite.com/` is the url, then the final url to access will be:\n",
    "```\n",
    "    https://MyOwnWebsite.com/?key1=value1&key2=value2&key3=value3\n",
    "```\n",
    "\n",
    "The code to reach the webpage is:\n",
    "```Python\n",
    "payload = {'key1': value1, 'key2': value2, 'key2': value3}\n",
    "resp = reqs.get(\"https://MyOwnWebsite.com\", params=payload)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script sends a variable with a value to the `httpbin.org` server. The variable is specified directly in the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = reqs.get(\"https://httpbin.org/get?name=Peter\")\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We send a `get()` request to the web site and pass the data, which is specified in the `params` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': 'Peter'}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`payload` is a dictionary of pairs of keys/values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': 'Peter', 'age': 23}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass a list of items as a value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'name': ['Peter', 'Johns'], 'age': 23}\n",
    "resp = reqs.get(\"https://httpbin.org/get\", params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Methods**\n",
    "\n",
    "```python\n",
    "requests.post('https://httpbin.org/post', data={'key':'value'})\n",
    "requests.put('https://httpbin.org/put', data={'key':'value'})\n",
    "requests.delete('https://httpbin.org/delete')\n",
    "requests.patch('https://httpbin.org/patch', data={'key':'value'})\n",
    "requests.options('https://httpbin.org/get')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of `requests` Methods \n",
    "\n",
    "| Method\t| Description |\n",
    "| :--- | :--- |\n",
    "| delete(url, args)\t| Sends a DELETE request to the specified url | \n",
    "| get(url, params, args)\t| Sends a GET request to the specified url | \n",
    "| head(url, args)\t| Sends a HEAD request to the specified url | \n",
    "| patch(url, data, args)\t| Sends a PATCH request to the specified url | \n",
    "| post(url, data, json, args)\t| Sends a POST request to the specified url | \n",
    "| put(url, data, args)\t| Sends a PUT request to the specified url | \n",
    "| request(method, url, args)\t| Sends a request of the specified method to the specified url| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-Type\n",
    "\n",
    "- It is part of the HTTP header.\n",
    "   - A string used to indicate the media type of the resource you want to access.  \n",
    "   - It tells the browser the type of content it has to load on the machine. \n",
    "- Here are some values of `content-type`:\n",
    "\n",
    "```html\n",
    "   text/html\n",
    "   text/csv\n",
    "   application/json\n",
    "   application/javascript\n",
    "   audio/ogg\n",
    "   image/png\n",
    "```\n",
    "\n",
    "- While scraping a web page, it is used to determine which tool is needed to parse the content of a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content Type: \\n\\t {resp.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revisiting the function to accesss a web page\n",
    "We want to pass the `payload` as argument of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_website(url, payload=None):\n",
    "    \"\"\"\n",
    "    Attempt to access a server. If the attempt is successful,\n",
    "    return the response object, otherwise return an error message.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "       HTTP address of the web page we want to access\n",
    "    payload : dict\n",
    "       Parameters needed to construct the target url.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    resp : object\n",
    "       Object which has infomation on the web page of interest.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if payload:\n",
    "            resp = reqs.get(url, params=payload, timeout=3)\n",
    "        else:\n",
    "            resp = reqs.get(url, timeout=3)\n",
    "        resp.raise_for_status()\n",
    "    except reqs.exceptions.HTTPError as errh:\n",
    "        print(f\"Http Error: {errh}\")\n",
    "    except reqs.exceptions.ConnectionError as errc:\n",
    "        print(f\"Error Connecting: {errc}\")\n",
    "    except reqs.exceptions.Timeout as errt:\n",
    "        print(f\"Timeout Error: {errt}\")\n",
    "    except reqs.exceptions.RequestException as err:\n",
    "        print(f\"General Error: {err}\")\n",
    "    else:\n",
    "        print(\"Successfully accessed the site!\")\n",
    "    \n",
    "    return resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Web Scraping with JSON</font>\n",
    "\n",
    "### <font color=\"blue\"> What is JSON?</font>\n",
    "\n",
    "* JSON (JavaScript Object Notation) is a popular data format used for representing structured data. \n",
    "* It is a text format that is language independent and can be used in Python, Perl among other languages. \n",
    "* JSON format is used for data communications between servers and web applications.\n",
    "* It is built on two structures:\n",
    "\n",
    "     - A collection of name/value pairs. This is realized as an object, record, dictionary, hash table, keyed list, or associative array.\n",
    "     - An ordered list of values. This is realized as an array, vector, list, or sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main functions of `JSON` are:\n",
    "\n",
    "* `dump()`: encoded string writing on file.\n",
    "* `load()`: Decode while JSON file read.\n",
    "* `dumps()`: encoding to JSON objects\n",
    "* `loads()`: Decode the JSON string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of JSON Data**\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"stations\": [\n",
    "        {\n",
    "            \"acronym\": “BLD”, \n",
    "            \"name\": \"Boulder Colorado\",\n",
    "            \"latitude”: 40.00,\n",
    "            \"longitude”: -105.25\n",
    "        }, \n",
    "        {\n",
    "            \"acronym”: “BHD”, \n",
    "            \"name\": \"Baring Head Wellington New Zealand\",\n",
    "            \"latitude\": -41.28,\n",
    "            \"longitude\": 174.87\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Example of JSON Data**\n",
    "\n",
    "We consider an online database, <a href=\"IP-API.com\">IP-API.com</a>, that returns GeoIP data in JSON format. Simply opening <a href=\"http://ip-api.com/json/54.148.84.95\">http://ip-api.com/json/54.148.84.95</a> will return the following JSON result:\n",
    "\n",
    "\n",
    "```python\n",
    "{\n",
    "  \"as\": \"AS16509 Amazon.com, Inc.\",\n",
    "  \"city\": \"Boardman\",\n",
    "  \"country\": \"United States\",\n",
    "  \"countryCode\": \"US\",\n",
    "  \"isp\": \"Amazon\",\n",
    "  \"lat\": 45.8696,\n",
    "  \"lon\": -119.688,\n",
    "  \"org\": \"Amazon\",\n",
    "  \"query\": \"54.148.84.95\",\n",
    "  \"region\": \"OR\",\n",
    "  \"regionName\": \"Oregon\",\n",
    "  \"status\": \"success\",\n",
    "  \"timezone\": \"America\\/Los_Angeles\",\n",
    "  \"zip\": \"97818\"\n",
    "}\n",
    "```\n",
    "\n",
    "To see your own Geolocation data in JSON format, just open <a href=\"http://ip-api.com/json/\">http://ip-api.com/json/</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://ip-api.com/json/\"\n",
    "resp = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resp.status_code == 200:\n",
    "    print(f\"Content type: \\t {resp.headers['content-type']}\")\n",
    "    print(f\"Web page content: \\n\\t {resp.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=\"red\">Note that we have a JSON page.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_page = json.loads(resp.text)\n",
    "pprint.pprint(json_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">`JSON` Main Functions</font>\n",
    "`JSON' has two sets of functions:\n",
    "\n",
    "- Set 1 - for serialization (process of transforming objects or data structures into byte streams or strings)\n",
    "   - `dumps()`: Returns a string representing a JSON object from a Python object.\n",
    "   - `dump()`: Store a file (`.json`) the JSON representation of a Python object.\n",
    "- Set 2 - for deserialization (conversion of JSON object into their respective Python objects)\n",
    "   - `loads()`: Returns a Python object from a string representing a JSON object.\n",
    "   - `load()`: Retrieve from a `.json` (with a JSON object) the Python object.\n",
    "\n",
    "\n",
    "![fig_json](https://www.bogotobogo.com/python/images/json_load_dump/python-json-load-loads-dump-dumps.png)\n",
    "Image Source: [www.bogotobogo.com](https://www.bogotobogo.com/python/python-json-dumps-loads-file-read-write.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_sd](https://miro.medium.com/max/1150/1*9zJJ65xk8agiQXlqd7nYUw.jpeg)\n",
    "Image Source: Phonlawat Khunphet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serialization**\n",
    "\n",
    "We use the `dump()` that takes two arguments: \n",
    "* The data object to be serialized.\n",
    "* The file object to which it will be written (Byte format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Sample.json\"\n",
    "with open(file_name, \"w\") as fid: \n",
    "     json.dump(json_page, fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Sample.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deserializing JSON**\n",
    "\n",
    "* The Deserialization is opposite of Serialization, i.e. conversion of JSON object into their respective Python objects. \n",
    "* We use the `load()` function which is usually used to load from string, otherwise the root object is in list or dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name, \"r\") as fid: \n",
    "     json_obj = json.load(fid)\n",
    "\n",
    "print(json_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in json_obj:\n",
    "    print(\"{:>12}: {}\".format(key, json_obj[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Scraping the NASA Astronomy Picture Of the Day (APOD) Webpage </font>\n",
    "\n",
    "- We want to be able to obtain from the webpage <a href=\"https://api.nasa.gov/planetary/apod\"> https://api.nasa.gov/planetary/apod</a>,  the Astronomy picture of the day for a given day and plot the image.\n",
    "- We access the webpage (using a set of parameters) and retrieve the content of the page as a JSON object.\n",
    "\n",
    "**Query Parameters**\n",
    "\n",
    "| Parameter | Type | Default | Description |\n",
    "| --- | --- | --- | --- |\n",
    "|`date` | YYYY-MM-DD | today | Date of the APOD image to retrieve |\n",
    "|`start_date` | YYYY-MM-DD | none | The start of a date range, when requesting date for a range of dates. Cannot be used with `date`. |\n",
    "|`end_date` | YYYY-MM-DD | today | The end of the date range, when used with `start_date`. |\n",
    "| `count` |\tint\t| none\t| If this is specified then count randomly chosen images will be returned. Cannot be used with `date` or `start_date` and `end_date`. |\n",
    "| `hd` | bool | False | Retrieve the URL for the high resolution image |\n",
    "| `api_key` | string | DEMO_KEY | <a href=\"https://api.nasa.gov/\">[https://api.nasa.gov/</a> key for expanded usage |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.nasa.gov/planetary/apod\"\n",
    "date = \"2022-07-04\"\n",
    "payload = {'api_key': \"DEMO_KEY\",\n",
    "          'date': date,\n",
    "          'hd': True}\n",
    "\n",
    "page_content = access_website(url, payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"URL: \\n\\t {page_content.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the content type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content type: \\n\\t {page_content.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data with JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_page = json.loads(page_content.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APOD variable is a dictionary of various keys and values. Let’s take a look at the keys of this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the keys and values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(f\"{x} --> {json_page[x]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if json_page[\"media_type\"] == \"image\":\n",
    "    io.imshow(io.imread(json_page[\"url\"]))\n",
    "    plt.title(json_page[\"title\"])\n",
    "    io.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">If you want to download the file on your local system:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "url_name = json_page[\"url\"]\n",
    "loc_file_name = os.path.basename(url_name)\n",
    "\n",
    "urllib.request.urlretrieve(url_name, loc_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to view the image through a browser, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "def window_open(url):\n",
    "    display(Javascript('window.open(\"{url}\");'.format(url=url)))\n",
    "    \n",
    "window_open(json_page['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Obtaining Mars Rover Photos</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rover_url = 'https://api.nasa.gov/mars-photos/api/v1/rovers/curiosity/photos'\n",
    "\n",
    "payload = {'api_key': \"DEMO_KEY\",\n",
    "           'sol': 1000}\n",
    "\n",
    "response = access_website(rover_url, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"URL: \\n\\t {response.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content type: \\n\\t {response.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"KEYS: \\n\\t {response_dict.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = response_dict['photos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(photos))\n",
    "print(len(photos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(photos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the URL of each photo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_photos = list()\n",
    "for photo in photos:\n",
    "    url_photos.append(photo['img_src'])\n",
    "\n",
    "print(url_photos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly select 20 pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "url_pictures = random.sample(url_photos, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the 20 photos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 5, figsize=(20, 20))\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(20):\n",
    "    ax[i].imshow(io.imread(url_pictures[i]))\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"purple\">Breakout 1</font>\n",
    "\n",
    "Use the following code to list all the images in the provided range of years:\n",
    "\n",
    "```python\n",
    "url = \"https://images-api.nasa.gov/search\"\n",
    "\n",
    "payload = {\n",
    "        \"q\": \"apollo\",\n",
    "        \"page\": \"1\",\n",
    "        \"media_type\": \"image\",\n",
    "        \"year_start\": \"2020\",\n",
    "        \"year_end\": \"2022\"}\n",
    "\n",
    "response = reqs.get(url, params=payload)\n",
    "images = response.json()[\"collection\"][\"items\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "import requests as reqs\n",
    "\n",
    "url = \"https://images-api.nasa.gov/search\"\n",
    "\n",
    "params = {\n",
    "    \"q\": \"apollo\",\n",
    "    \"page\": \"1\",\n",
    "    \"media_type\": \"image\",\n",
    "    \"year_start\": \"2020\",\n",
    "    \"year_end\": \"2022\"\n",
    "}\n",
    "\n",
    "response = reqs.get(url, params=params)\n",
    "response.raise_for_status()\n",
    "\n",
    "images = response.json()[\"collection\"][\"items\"]\n",
    "print(f\"Number of images: {len(images)}\")\n",
    "for image in images:\n",
    "    thumbnail_url = image[\"links\"][0][\"href\"]\n",
    "    image_url = thumbnail_url[:thumbnail_url.rfind(\"~\")] + \"~orig.jpg\"\n",
    "    print(image_url)\n",
    "``` \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">Scraping the Earth Observatory Natural Event Tracker (EONET) Webpage </font>\n",
    "\n",
    "- We want to be able to browse the webpage <a href=\"https://eonet.gsfc.nasa.gov/api/v2.1/events\"> https://eonet.gsfc.nasa.gov/api/v2.1/events</a>,  to identify natural events on Earth.\n",
    "\n",
    "**Query Parameters**\n",
    "\n",
    "| Parameter | Value(s) |  Description |\n",
    "| --- | --- | --- |\n",
    "|`source` | Source ID | Filter the returned events by the <a href=\"https://eonet.gsfc.nasa.gov/api/v2.1/sources\">Source</a>. Multiple sources can be included in the parameter: comma separated, operates as a boolean OR. |\n",
    "|`status` | open or closed | Events that have ended are assigned a closed date and the existence of that date will allow you to filter for only-open or only-closed events. Omitting the status parameter will return only the currently open events. |\n",
    "| `limit` | int | Limits the number of events returned |\n",
    "| `days ` | int | Limit the number of prior days (including today) from which events will be returned. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://eonet.gsfc.nasa.gov/api/v2.1/events\"\n",
    "payload = {'source': \"EO,JTWC\",\n",
    "          'status': \"open\",\n",
    "          'limit': 6,\n",
    "          'days': 30}\n",
    "\n",
    "page_content = access_website(url, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content type: \\n\\t {page_content.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_page = json.loads(page_content.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in json_page:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_page['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_page['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(json_page['events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_page['events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in json_page['events']:\n",
    "    print(event['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze one event\n",
    "\n",
    "We use `Pandas`, `GeoPandas` and `MovingPandas` to track the movement of an event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import movingpandas as mpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = json_page['events'][1]\n",
    "print(event['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['latitude', 'longitude', 't']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for geom in event['geometries']:\n",
    "    lat = geom['coordinates'][1]\n",
    "    lon = geom['coordinates'][0]\n",
    "    date = geom['date']\n",
    "    row = dict(latitude=lat, longitude=lon, t=date)\n",
    "    df.loc[len(df)] = row\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t'] = pd.to_datetime(df['t'], format = '%Y-%m-%dT%H:%M:%SZ')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geometry'] = [Point(xy) for xy in zip(df.longitude, df.latitude)] \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a GeoPandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a MovingPandas Trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf = mpd.Trajectory(gdf, 1)\n",
    "mdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the distance and the speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.add_distance(overwrite=True, name=\"distance\", units=\"mi\")\n",
    "mdf.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.add_speed(overwrite=True, \n",
    "                      name=\"speed\", units=(\"mi\", \"h\"))\n",
    "\n",
    "mdf.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the trajectory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.hvplot(tiles=\"ESRI\",\n",
    "           c=\"speed\",\n",
    "           title=event['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Web Scraping with Beautiful Soup</font>\n",
    "\n",
    "- Web scraping allows you to download the HTML of a website and extract the data that you need.\n",
    "- Beautiful Soup is a Python library for scraping data from websites.\n",
    "- Beautiful Soup creates a parse tree from parsed HTML and XML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.webcode.me\"\n",
    "source = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Content type: \\n\\t {source.headers['content-type']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a beautiful soup object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the the HTML content of the page using the `prettify` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain the title section of the page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get attribute name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get attribute values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beginning navigation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.title.parent.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting specific tags**\n",
    "- HTML is made up of <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element\">tags</a>. It stores all of it’s data in them, and in the midst of all that clutter lies the data we need. Some of the tags are:\n",
    "     * `head` - contains machine-readable information (metadata) about the document, like its title, scripts, and style sheets.\n",
    "     * `body` - represents the content of an HTML document. There can be only one `<body>` element in a document.\n",
    "     * `title` - defines the document's title that is shown in a Browser's title bar or a page's tab. \n",
    "     * `p` - for paragraph\n",
    "     * `div` — indicates a division, or area, of the page.\n",
    "     * `b` — bolds any text inside.\n",
    "     * `i` — italicizes any text inside.\n",
    "     * `table` — creates a table.\n",
    "     * `form` — creates an input form.\n",
    "- The `find` method searches for the first tag with the needed name.\n",
    "- The `find_all` method searches for all tags with the needed tag name and returns them as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we want to find paragraph tags `<p>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mysoup.find('p')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mysoup.find_all('p')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the last paragraph only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('p')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, paragraph in enumerate(mysoup.find_all('p'), start=1):\n",
    "    print(f\"Paragraph Text {i}: {paragraph.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = mysoup.find_all('body')\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Type body:       \", type(body))\n",
    "print(\"Type inner body: \", type(body[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(body[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find_all('body')[0].find_all('p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grab the text**\n",
    "\n",
    "- Use the method `get_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Searching for tags by `class` and `id`**\n",
    "\n",
    "- Classes and ids are used by CSS to determine which HTML elements to apply certain styles to. \n",
    "- We can also use them when scraping to select specific elements we want to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\"\n",
    "source = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `find_all` method to search for items by `class` or by `id`. \n",
    "\n",
    "```python\n",
    "mysoup.find_all(\"html_tag\", class_=\"your_class_name\")\n",
    "mysoup.find_all(class_=\"your_class_name\")\n",
    "\n",
    "mysoup.find_all(\"html_tag\", id=\"your_id_name\")\n",
    "mysoup.find_all(id=\"your_id_name\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, we’ll search for any `p` tag that has the class `outer-text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for any tag that has the class `outer-text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")[-1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(class_=\"outer-text\")[-1].get_text().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also search for elements by id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(id=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.find_all(id=\"first\")[0].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using CSS Selectors**\n",
    "\n",
    "- <a href=\"https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors\">CSS (Cascading Style Sheets)</a> is a declarative language that controls how webpages look in the browser. \n",
    "- The browser applies CSS style declarations to selected elements to display them properly. \n",
    "- A style declaration contains the properties and their values, which determine how a webpage looks.\n",
    "- CSS selectors</a> are how the CSS language allows developers to specify HTML tags to style. \n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "- `p a` — finds all `a` tags inside of a `p` tag.\n",
    "- `body p a` — finds all `a` tags inside of a `p` tag inside of a `body` tag.\n",
    "- `html body` — finds all `body` tags inside of an `html` tag.\n",
    "- `p.outer-text` — finds all `p` tags with a class of `outer-text`.\n",
    "- `p#first` — finds all `p` tags with an id of `first`.\n",
    "- `body p.outer-text` — finds any `p` tags with a class of `outer-text` inside of a `body` tag.\n",
    "\n",
    "We can use the CSS selectors to search items inside webpages. `BeautifulSoup` objects support searching a page via CSS selectors using the `select` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `p` tags in our page that are inside of a `body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"body p\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `b` tags in our page that are inside of a `p`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"p b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `b` tags inside of a `p` tag inside of a `body`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select(\"body p b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `p` tags with an id of `first`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup.select('p#first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Example: Extract the web link of the Astronomy Picture of the Day</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://apod.nasa.gov/apod/astropix.html\"\n",
    "source = access_website(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print basic information of the Image of the Day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mysoup.find('p').get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_comments = mysoup.find_all('a')\n",
    "for a in href_comments:\n",
    "    print(a.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">__The `Picture of the Day` can either be a picture or a video.__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picture_day = \"picture\"\n",
    "if mysoup.iframe:\n",
    "    print(\"We have a video.\")\n",
    "    picture_day = \"video\"\n",
    "else:\n",
    "    print(\"We have a picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"video\":\n",
    "    HTML(str(mysoup.iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"video\":\n",
    "    mysoup.iframe['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"video\":\n",
    "    src_list = [a['src'] for a in mysoup.select('iframe[src]')]\n",
    "    src_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the `src` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tags = mysoup.find_all(src=True)\n",
    "src_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all `href` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_tags = mysoup.find_all(href=True)\n",
    "href_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all valud urls in `a` tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_with_text = [a['href'] for a in mysoup.find_all('a', href=True) if a.text]\n",
    "links_with_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list1 = [a['href'] for a in mysoup.find_all('a', href=True)]\n",
    "link_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list2 = [l.get('href') for l in mysoup.find_all('a')]\n",
    "link_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list3 = [a['href'] for a in mysoup.select('a[href]')]\n",
    "link_list3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Picture of the Day is an image instead (not a video), the following can help us view the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if picture_day == \"picture\":\n",
    "    url_image = \"\".join([\"https://apod.nasa.gov/apod/\", link_list3[1]])\n",
    "    fig, axes = plt.subplots(figsize=(10, 8))\n",
    "    axes.imshow(io.imread(url_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"purple\">Breakout 2</font>\n",
    "\n",
    "Go to the webpage:\n",
    "\n",
    "[https://astg606.github.io/py_courses/summer_2022/](https://astg606.github.io/py_courses/summer_2022/)\n",
    "\n",
    "and extract the `Course Evaluation` web link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import requests as reqs\n",
    "\n",
    "from bs4 import BeautifulSoup as bso\n",
    "\n",
    "URL = \"https://astg606.github.io/py_courses/summer_2022/\"\n",
    "\n",
    "source = reqs.get(URL)\n",
    "if source.status_code == 200:\n",
    "    mysoup = bso(source.content, 'html.parser')\n",
    "    href_tags = mysoup.find_all(href=True)\n",
    "    for tag in href_tags:\n",
    "        if tag.get_text() == \"Course Evaluation\":\n",
    "            print(tag[\"href\"])\n",
    "else:\n",
    "    print(\"URL not accessible.\")\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Example: Weather Data for Greenbelt, Maryland</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://forecast.weather.gov/MapClick.php\"\n",
    "params = {'lat': 39.00079000000005,\n",
    "          'lon': -76.88055999999995}\n",
    "\n",
    "source = access_website(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"URL: \\n\\t {source.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Tonight's Forecast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day = mysoup.find(id=\"seven-day-forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in forecast_items:\n",
    "    if item.find(class_=\"period-name\").get_text() == \"Tonight\":\n",
    "        tonight = item\n",
    "        break\n",
    "\n",
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = tonight.find(class_=\"period-name\").get_text()\n",
    "print(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
    "print(short_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tonight.find(class_=\"temp\").get_text()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tonight.find(\"img\")\n",
    "desc = img['title']\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting all Data**\n",
    "\n",
    "We use CSS selectors to extract everything at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select all items with the class `period-name` inside an item with the class `tombstone-container` in `seven_day`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "periods = [pt.get_text() for pt in period_tags]\n",
    "print(periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same technique to get the other fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_descs = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
    "print(short_descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")]\n",
    "print(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
    "print(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(temps) < len(descs):\n",
    "    temps = [\" \"] + temps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the data into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "forecast_dict = dict(Period=periods, \n",
    "                     Temperature=temps,\n",
    "                     Short_Description=short_descs,  \n",
    "                     Description=descs)\n",
    "df_weather = pd.DataFrame(forecast_dict)\n",
    "df_weather = df_weather.set_index(\"Period\")\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detailed Forecast**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_forecast = mysoup.find(id=\"detailed-forecast-body\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_labels = det_forecast.find_all(class_=\"col-sm-2 forecast-label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_texts = det_forecast.find_all(class_=\"col-sm-10 forecast-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in zip(forecast_labels, forecast_texts):\n",
    "    print(f\"\\033[1m {a.get_text():>15}: \\033[0m {b.get_text():<}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"purple\">Breakout 3</font>\n",
    "\n",
    "- Go to the sitethe website `https://eonet.gsfc.nasa.gov/api/v2.1/events`\n",
    "- Select a date range and the number of events you want to retrieve.\n",
    "- Creade a Pandas DataFrame that contains as columns the event type, date, latitude and longitude.\n",
    "\n",
    "```python\n",
    "url = \"https://eonet.gsfc.nasa.gov/api/v2.1/events\"\n",
    "payload = {'source': \"EO\",\n",
    "          'status': \"open\",\n",
    "          'limit': 6,\n",
    "          'days': 100}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b><font color=\"green\">Click here to access the solution</font></b></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "import json\n",
    "\n",
    "url = \"https://eonet.gsfc.nasa.gov/api/v2.1/events\"\n",
    "payload = {'source': \"EO\",\n",
    "          'status': \"open\",\n",
    "          'limit': 6,\n",
    "          'days': 100}\n",
    "\n",
    "page_content = reqs.get(url, params=payload)\n",
    "\n",
    "if page_content.status_code == 200:\n",
    "    json_page = json.loads(page_content.text)\n",
    "\n",
    "for x in json_page:\n",
    "    print(x)\n",
    "\n",
    "list_events = json_page['events']\n",
    "\n",
    "print(f\"Number of events: {len(list_events)}\")\n",
    "print(f\"List of events: \\n {list_events}\")\n",
    "\n",
    "event_types = [evt['categories'][0]['title'] for evt in list_events]\n",
    "event_dates = [evt['geometries'][0]['date'] for evt in list_events]\n",
    "event_lons = [evt['geometries'][0]['coordinates'][0] for evt in list_events]\n",
    "event_lats = [evt['geometries'][0]['coordinates'][1] for evt in list_events]\n",
    "\n",
    "print()\n",
    "\n",
    "import pandas as pd\n",
    "df_events = pd.DataFrame({\n",
    "    \"Type\": event_types,\n",
    "    \"Dates\": event_dates,\n",
    "    #\"Latitudes\": event_lats,\n",
    "    \"Longitudes\":event_lons\n",
    "})\n",
    "df_events\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\"> Example: MODIS Aerosol Optical Thickness</font>\n",
    "\n",
    "- Scientists use measurements from the MODIS sensor aboard NASA's Terra and Aqua satellites to map the amount of aerosol that is in the air all over the world. Because aerosols reflect visible and near-infrared light back to space, scientists can use satellites to make maps of where there are high concentrations of these particles.\n",
    "- Scientists call this measurement aerosol optical thickness (AOT). \n",
    "- It is a measure of how much light the airborne particles prevent from traveling through the atmosphere. \n",
    "- Aerosols absorb and scatter incoming sunlight, thus reducing visibility and increasing optical thickness. An optical thickness of less than 0.1 indicates a crystal clear sky with maximum visibility, whereas a value of 1 indicates the presence of aerosols so dense that people would have difficulty seeing the Sun, even at mid-day!\n",
    "\n",
    "\n",
    "In this example, we want to access the <a href=\"https://neo.gsfc.nasa.gov/\">NASA Earth Observations (NEO)</a> website to obtain the AOT measurements for a given day or a range of days (from 2000 to present)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the day range of interest:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_date = '2019-12-30'\n",
    "end_date = '2019-12-31'\n",
    "\n",
    "pd_series = pd.date_range(start=beg_date, end=end_date, freq='D')\n",
    "dates = [dt.strftime('%Y-%m-%d') for dt in pd_series]\n",
    "\n",
    "url_base = \"https://neo.gsfc.nasa.gov/view.php?datasetId=MODAL2_M_AER_OD&year=\"\n",
    "\n",
    "urls = [url_base+dt for dt in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access the webpage for the first day:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = reqs.get(urls[0])\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse the webpage and print its content:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysoup = bso(source.text, 'html.parser')\n",
    "print(mysoup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather all the lines with `href` tag:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "href_tags = mysoup.find_all(href=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the `http` address that has the word `CSV`. That will give us the remote location of the file we want to read.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in href_tags:\n",
    "    loc_url = tag[\"href\"]\n",
    "    if \"CSV\" in loc_url:\n",
    "        csv_url = loc_url\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(csv_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `Pandas` to read the remote file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = access_website(csv_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "file_object = io.StringIO(resp.content.decode('utf-8'))\n",
    "pd.read_csv(file_object, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that `99999.0` corresponds to a missing value. Let us replace it with `NaN`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_object = io.StringIO(resp.content.decode('utf-8'))\n",
    "df = pd.read_csv(file_object, index_col=0, na_values=99999.0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use `Xarray` to quickly visualize the data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "da = xr.DataArray(df.values,\n",
    "                  coords=[[float(lat) for lat in df.index], \n",
    "                          [float(lon) for lon in df.columns]],\n",
    "                  dims=['latitude', 'longitude'])\n",
    "\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Application</font>\n",
    "\n",
    "- We want to get all book names on historic New York Time Best Sellers (Business section)\n",
    "- The purpose is to:\n",
    "     1. Help to compile my reading list in 2020\n",
    "     2. Serve as reference to use Python for simple web analytics\n",
    "- We use the Python packages: `Pandas`, `Requests` and `Baeutiful Soup`\n",
    "- We save data in `pickle` and `csv` formats.\n",
    "\n",
    "The example was taken from: <a href=\"https://towardsdatascience.com/building-my-2020-reading-list-with-a-simple-python-script-b610c7f2c223\">Building my 2020 reading list with a simple Python script</a> by Pan Wu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty Pandas dataframe\n",
    "nylist = pd.DataFrame()\n",
    "\n",
    "beg_year = 2016\n",
    "end_year = 2021\n",
    "for the_year in range(beg_year, end_year):\n",
    "    for the_month in range(1, 13):\n",
    "        cur_month = str(the_month).zfill(2) # month in two digits\n",
    "        # one need to get the URL pattern first, and then use Requests package to get the URL content\n",
    "        url = 'https://www.nytimes.com/books/best-sellers/{0}/{1}/01/business-books/'.format(the_year, cur_month)\n",
    "        page = reqs.get(url)\n",
    "        print(\" --  try: {0}, {1} -- \".format(the_year, cur_month))\n",
    "        \n",
    "        # Ensure proper result is returned\n",
    "        if page.status_code != 200:\n",
    "            print(\"      Missing data for Year {} and Month {}\".format(the_year, cur_month))\n",
    "            continue\n",
    "        \n",
    "        # one may want to use BeautifulSoup to parse the right elements out\n",
    "        soup = bso(page.text, 'html.parser')\n",
    "        \n",
    "        # the specific class names are unique for this URL and they don't change across all URLs\n",
    "        top_list = soup.findAll(\"ol\", {\"class\": \"css-12yzwg4\"})[0].findAll(\"div\", {\"class\": \"css-xe4cfy\"})\n",
    "        print(\"Year: {} - Month: {} - How many in the top list: {}\".format(the_year, the_month, len(top_list)))\n",
    "        \n",
    "        # loop through the Best Seller list in each Year-Month, and append the information into a pandas DataFrame\n",
    "        for i in range(len(top_list)):\n",
    "            book   = top_list[i].contents[0]\n",
    "            title  = book.findAll(\"h3\", {\"class\": \"css-5pe77f\"})[0].text\n",
    "            author = book.findAll(\"p\",  {\"class\": \"css-hjukut\"})[0].text\n",
    "            review = book.get(\"href\")\n",
    "            # print(\"{0}, {1}; review: {2}\".format(title, author, review))\n",
    "            one_item = pd.Series([the_year, the_month, title, author, i+1, review], \n",
    "                                 index=['year', 'month', 'title', 'author', 'rank', 'review'])\n",
    "            nylist = nylist._append(one_item, ignore_index=True, sort=False)\n",
    "\n",
    "# write out the result to a pickle file for easy analysis later.\n",
    "nylist.to_pickle(\"nylist.pkl\")\n",
    "nylist.to_csv(\"nylist.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nylist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"purple\">Breakout 4</font>\n",
    "\n",
    "- Write a Python script that reads the pickle file `nylist.pkl` or the csv file `nylist.csv` and prints its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
